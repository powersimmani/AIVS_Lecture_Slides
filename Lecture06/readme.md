# Lecture 6: Supervised Learning Evaluation

## ğŸ“‹ Overview

**Instructor:** Ho-min Park  
**Email:** homin.park@ghent.ac.kr | powersimmani@gmail.com

This lecture covers all aspects of model evaluation: data splitting, metrics, cross-validation, and hyperparameter tuning.

---

## ğŸ¯ Learning Objectives

1. Design data splitting strategies (Train/Val/Test)
2. Select and interpret appropriate metrics for regression and classification
3. Evaluate model performance with cross-validation
4. Apply hyperparameter tuning techniques
5. Diagnose and resolve overfitting/underfitting

---

## ğŸ“š Key Topics

**Data splitting**: Train/Val/Test, Stratified Split, Time Series Split
**Regression metrics**: MSE, RMSE, MAE, RÂ², MAPE
**Classification metrics**: Accuracy, Precision, Recall, F1, AUC-ROC, Confusion Matrix
**Cross-validation**: K-Fold, Stratified K-Fold, Leave-One-Out
**Tuning**: Grid Search, Random Search, Bayesian Optimization

---

## ğŸ’¡ Key Concepts

- Test set for final evaluation only, Val set for model selection
- Use F1, AUC for imbalanced data
- Cross-validation for robust performance estimation
- Early stopping prevents overfitting
- Perform hyperparameter tuning systematically

---

## ğŸ› ï¸ Prerequisites

- Basic Python programming
- Understanding of previous lecture content
- Basic machine learning concepts

---

## ğŸ“– Additional Resources

For detailed code examples, practice materials, and slides, please refer to the original lecture files.
Lecture materials: HTML-based interactive slides provided
