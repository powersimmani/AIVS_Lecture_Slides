# Lecture 11: Sequence Models

## ğŸ“‹ Overview

**Instructor:** Ho-min Park  
**Email:** homin.park@ghent.ac.kr | powersimmani@gmail.com

This lecture covers sequence data modeling from traditional time series analysis to RNN and LSTM.

---

## ğŸ¯ Learning Objectives

1. Understand characteristics of sequence data
2. Apply traditional time series techniques (ARIMA)
3. Implement RNN architectures
4. Handle long-term dependencies with LSTM/GRU
5. Design Seq2Seq models

---

## ğŸ“š Key Topics

**Time series basics**: MA, AR, ARMA, ARIMA
**RNN**: Recurrent structure, Hidden state, Backpropagation Through Time
**LSTM**: Cell state, Gates (Forget, Input, Output)
**GRU**: Simplified structure, Reset/Update gates
**Seq2Seq**: Encoder-Decoder, machine translation

---

## ğŸ’¡ Key Concepts

- RNN preserves sequential information
- LSTM solves long-term dependency problem
- GRU more efficient than LSTM
- Vanishing gradient is main RNN problem
- Seq2Seq handles variable-length input/output

---

## ğŸ› ï¸ Prerequisites

- Basic Python programming
- Understanding of previous lecture content
- Basic machine learning concepts

---

## ğŸ“– Additional Resources

For detailed code examples, practice materials, and slides, please refer to the original lecture files.
Lecture materials: HTML-based interactive slides provided
