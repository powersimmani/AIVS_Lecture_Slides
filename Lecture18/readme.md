# Lecture 18: Advanced Unsupervised Learning

## ğŸ“‹ Overview

**Instructor:** Ho-min Park  
**Email:** homin.park@ghent.ac.kr | powersimmani@gmail.com

This lecture covers advanced unsupervised learning techniques including Self-Supervised Learning and Contrastive Learning.

---

## ğŸ¯ Learning Objectives

1. Understand Self-Supervised Learning paradigm
2. Implement Contrastive Learning (SimCLR, MoCo)
3. Time series clustering (DTW, K-Shape)
4. Understand Autoencoder variants
5. Learn practical applications

---

## ğŸ“š Key Topics

**Self-Supervised**: Pretext tasks, Representation learning
**Contrastive**: SimCLR, MoCo, BYOL, positive/negative pairs
**Time series clustering**: DTW, K-Shape, Subsequence clustering
**Autoencoder**: VAE, latent representations
**Semi-Supervised**: Pseudo-labeling, Consistency regularization

---

## ğŸ’¡ Key Concepts

- Self-Supervised learns without labels
- Contrastive learns similarity
- DTW measures time series distance
- VAE for generative modeling
- Semi-Supervised leverages small labeled data

---

## ğŸ› ï¸ Prerequisites

- Basic Python programming
- Understanding of previous lecture content
- Basic machine learning concepts

---

## ğŸ“– Additional Resources

For detailed code examples, practice materials, and slides, please refer to the original lecture files.
Lecture materials: HTML-based interactive slides provided
