# Lecture 8: Loss, Optimization and Scheduling

## ğŸ“‹ Overview

**Instructor:** Ho-min Park  
**Email:** homin.park@ghent.ac.kr | powersimmani@gmail.com

This lecture covers theory and practice of loss function selection, optimization algorithms, and learning rate scheduling.

---

## ğŸ¯ Learning Objectives

1. Design appropriate loss functions for different tasks
2. Compare and implement optimization algorithms
3. Apply learning rate scheduling strategies
4. Understand Momentum and Adaptive learning rates
5. Understand convergence characteristics and tradeoffs

---

## ğŸ“š Key Topics

**Loss functions**: MSE, MAE, Cross-Entropy, Focal Loss, Contrastive Loss
**Optimization**: SGD, Momentum, NAG, Adagrad, RMSprop, Adam, AdamW
**Learning rate scheduling**: Step Decay, Exponential Decay, Cosine Annealing, Warm-up
**Momentum**: Acceleration, oscillation reduction
**Adaptive learning rates**: Per-parameter learning rate adjustment

---

## ğŸ’¡ Key Concepts

- Cross-Entropy for classification, MSE for regression
- Adam generally good starting point
- AdamW properly implements weight decay
- Learning rate scheduling improves convergence
- Warm-up stabilizes early training

---

## ğŸ› ï¸ Prerequisites

- Basic Python programming
- Understanding of previous lecture content
- Basic machine learning concepts

---

## ğŸ“– Additional Resources

For detailed code examples, practice materials, and slides, please refer to the original lecture files.
Lecture materials: HTML-based interactive slides provided
