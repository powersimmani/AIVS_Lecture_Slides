# Lecture 4: From Linear to Logistic Regression

## ğŸ“‹ Overview

**Instructor:** Ho-min Park  
**Email:** homin.park@ghent.ac.kr | powersimmani@gmail.com

This lecture covers the transition from regression to classification, mathematical derivation of logistic regression, and regularization techniques.

---

## ğŸ¯ Learning Objectives

1. Apply polynomial regression, Ridge, Lasso, Elastic Net regularization techniques
2. Understand differences between regression and classification problems
3. Map probabilities with sigmoid function, derive logistic regression
4. Implement cross-entropy loss and gradient descent
5. Extend to multi-class classification with Softmax

---

## ğŸ“š Key Topics

**Advanced regression**: Polynomial regression, Ridge (L2), Lasso (L1), Elastic Net
**Classification basics**: Why linear regression fails for classification
**Logistic regression**: Sigmoid, Odds, Log-odds, probability interpretation
**Loss function**: Binary Cross-Entropy, gradient descent
**Multi-class**: Softmax, One-vs-Rest, One-vs-One

---

## ğŸ’¡ Key Concepts

- Regularization prevents overfitting, L1 for feature selection, L2 for weight shrinkage
- Sigmoid maps real numbers to (0,1) probabilities
- Cross-Entropy is suitable loss function for classification
- Softmax generates multi-class probability distribution
- Decision boundary is linear but probability output is nonlinear

---

## ğŸ› ï¸ Prerequisites

- Basic Python programming
- Understanding of previous lecture content
- Basic machine learning concepts

---

## ğŸ“– Additional Resources

For detailed code examples, practice materials, and slides, please refer to the original lecture files.
Lecture materials: HTML-based interactive slides provided
