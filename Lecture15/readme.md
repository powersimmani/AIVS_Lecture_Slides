# Lecture 15: Generative Models - GAN

## ğŸ“‹ Overview

**Instructor:** Ho-min Park  
**Email:** homin.park@ghent.ac.kr | powersimmani@gmail.com

This lecture covers theoretical foundations, architectural variants, and training stabilization techniques of GANs.

---

## ğŸ¯ Learning Objectives

1. Understand GAN theory (Minimax, Nash Equilibrium)
2. Implement Generator-Discriminator structure
3. Solve Mode collapse and training instability
4. Understand variants like DCGAN, StyleGAN
5. Implement Conditional generation (Conditional GAN)

---

## ğŸ“š Key Topics

**GAN basics**: Minimax game, Value function
**Architecture**: Generator (upsampling), Discriminator (classification)
**Training stabilization**: Label smoothing, Feature matching, Spectral norm
**Variants**: DCGAN, WGAN, StyleGAN, CycleGAN
**Conditional GAN**: Class labels, Text-to-Image

---

## ğŸ’¡ Key Concepts

- GAN generates through adversarial training
- Mode collapse is major problem
- WGAN stabilizes training
- StyleGAN generates high-quality images
- Conditional GAN enables controlled generation

---

## ğŸ› ï¸ Prerequisites

- Basic Python programming
- Understanding of previous lecture content
- Basic machine learning concepts

---

## ğŸ“– Additional Resources

For detailed code examples, practice materials, and slides, please refer to the original lecture files.
Lecture materials: HTML-based interactive slides provided
