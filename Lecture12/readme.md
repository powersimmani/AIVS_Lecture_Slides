# Lecture 12: Advanced Sequence Models

## ğŸ“‹ Overview

**Instructor:** Ho-min Park  
**Email:** homin.park@ghent.ac.kr | powersimmani@gmail.com

This lecture covers Attention mechanisms and advanced sequence modeling techniques that overcome RNN limitations.

---

## ğŸ¯ Learning Objectives

1. Analyze RNN limitations (vanishing gradient, bottleneck)
2. Understand and implement Attention mechanism
3. Utilize Bidirectional RNN
4. Understand Attention variants (Self, Multi-head)
5. Implement Seq2Seq + Attention

---

## ğŸ“š Key Topics

**RNN limitations**: Vanishing gradient, information bottleneck, sequential processing
**Attention**: Query-Key-Value, Alignment scores
**Bidirectional RNN**: Bidirectional context
**Self-Attention**: Model relationships between inputs
**Seq2Seq + Attention**: Revolutionary for translation, summarization

---

## ğŸ’¡ Key Concepts

- Attention solves fixed-length vector bottleneck
- Query-Key-Value framework
- Alignment scores measure relevance
- Self-Attention captures full sequence relationships
- Attention is foundation of Transformers

---

## ğŸ› ï¸ Prerequisites

- Basic Python programming
- Understanding of previous lecture content
- Basic machine learning concepts

---

## ğŸ“– Additional Resources

For detailed code examples, practice materials, and slides, please refer to the original lecture files.
Lecture materials: HTML-based interactive slides provided
