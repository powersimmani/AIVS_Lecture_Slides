{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 📚 Lecture 17: Clustering and Unsupervised Learning Fundamentals\n",
        "\n",
        "## Comprehensive Hands-on Practice Notebook\n",
        "\n",
        "**Author:** Ho-min Park  \n",
        "**Email:** homin.park@ghent.ac.kr | powersimmani@gmail.com\n",
        "\n",
        "---\n",
        "\n",
        "### 🎯 Learning Objectives\n",
        "\n",
        "By the end of this notebook, you will be able to:\n",
        "\n",
        "1. ✅ **Understand** the fundamental concepts of unsupervised learning\n",
        "2. ✅ **Apply** multiple clustering algorithms (K-Means, DBSCAN, Hierarchical, GMM)\n",
        "3. ✅ **Evaluate** clustering quality using internal and external metrics\n",
        "4. ✅ **Reduce** dimensionality using PCA, t-SNE, and UMAP\n",
        "5. ✅ **Detect** anomalies using statistical and ML-based methods\n",
        "6. ✅ **Build** complete unsupervised learning pipelines\n",
        "7. ✅ **Select** appropriate algorithms for different scenarios\n",
        "8. ✅ **Interpret** and communicate results effectively\n",
        "\n",
        "---\n",
        "\n",
        "### 📋 Table of Contents\n",
        "\n",
        "**Part 0:** Setup and Introduction  \n",
        "**Part 1:** Clustering Fundamentals (K-Means, Hierarchical, DBSCAN)  \n",
        "**Part 2:** Cluster Evaluation Metrics  \n",
        "**Part 3:** Dimensionality Reduction (PCA, t-SNE, UMAP)  \n",
        "**Part 4:** Anomaly Detection  \n",
        "**Part 5:** Integrated Real-World Applications  \n",
        "**Part 6:** Summary and Key Takeaways  \n",
        "\n",
        "---\n",
        "\n",
        "### ⚡ Quick Start\n",
        "\n",
        "Run all cells in order, or jump to specific sections using the table of contents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 0: Setup and Configuration\n",
        "\n",
        "Let's import all necessary libraries and set up our environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "\n",
        "# Scikit-learn: Clustering\n",
        "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, MeanShift\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "# Scikit-learn: Dimensionality Reduction\n",
        "from sklearn.decomposition import PCA, KernelPCA\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Scikit-learn: Anomaly Detection\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.covariance import EllipticEnvelope\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "\n",
        "# Scikit-learn: Metrics and Utilities\n",
        "from sklearn.metrics import (\n",
        "    silhouette_score, silhouette_samples,\n",
        "    davies_bouldin_score, calinski_harabasz_score,\n",
        "    adjusted_rand_score, normalized_mutual_info_score,\n",
        "    confusion_matrix, classification_report,\n",
        "    roc_curve, auc, precision_recall_curve\n",
        ")\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.datasets import make_blobs, make_moons, make_circles, load_iris\n",
        "\n",
        "# Scipy\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "from scipy.spatial.distance import cdist\n",
        "from scipy.stats import zscore\n",
        "\n",
        "# Plotly for Interactive Visualizations\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# Configuration\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "np.random.seed(42)\n",
        "\n",
        "# Display settings\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "\n",
        "print(\"✅ All libraries imported successfully!\")\n",
        "print(f\"📦 NumPy version: {np.__version__}\")\n",
        "print(f\"📦 Pandas version: {pd.__version__}\")\n",
        "print(f\"📦 Scikit-learn imported successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Helper Functions\n",
        "\n",
        "Let's define some utility functions for visualization and analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_clusters(X, labels, centers=None, title=\"Cluster Visualization\", figsize=(10, 6)):\n",
        "    \"\"\"\n",
        "    Plot 2D clusters with optional centroids\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    X : array-like, shape (n_samples, 2)\n",
        "        Data points (only first 2 features used)\n",
        "    labels : array-like, shape (n_samples,)\n",
        "        Cluster labels\n",
        "    centers : array-like, optional\n",
        "        Cluster centers\n",
        "    title : str\n",
        "        Plot title\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=figsize)\n",
        "    \n",
        "    # Plot points\n",
        "    scatter = plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', \n",
        "                         alpha=0.6, edgecolors='k', s=50)\n",
        "    \n",
        "    # Plot centers if provided\n",
        "    if centers is not None:\n",
        "        plt.scatter(centers[:, 0], centers[:, 1], \n",
        "                   c='red', marker='X', s=200, \n",
        "                   edgecolors='black', linewidths=2,\n",
        "                   label='Centroids')\n",
        "        plt.legend()\n",
        "    \n",
        "    plt.colorbar(scatter, label='Cluster')\n",
        "    plt.xlabel('Feature 1')\n",
        "    plt.ylabel('Feature 2')\n",
        "    plt.title(title)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_silhouette(X, labels, metric='euclidean'):\n",
        "    \"\"\"\n",
        "    Create silhouette plot for cluster analysis\n",
        "    \"\"\"\n",
        "    from matplotlib import cm\n",
        "    \n",
        "    n_clusters = len(np.unique(labels[labels >= 0]))\n",
        "    silhouette_avg = silhouette_score(X, labels, metric=metric)\n",
        "    sample_silhouette_values = silhouette_samples(X, labels, metric=metric)\n",
        "    \n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    y_lower = 10\n",
        "    \n",
        "    for i in range(n_clusters):\n",
        "        ith_cluster_silhouette_values = sample_silhouette_values[labels == i]\n",
        "        ith_cluster_silhouette_values.sort()\n",
        "        \n",
        "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
        "        y_upper = y_lower + size_cluster_i\n",
        "        \n",
        "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
        "        ax.fill_betweenx(np.arange(y_lower, y_upper),\n",
        "                        0, ith_cluster_silhouette_values,\n",
        "                        facecolor=color, edgecolor=color, alpha=0.7)\n",
        "        \n",
        "        ax.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
        "        y_lower = y_upper + 10\n",
        "    \n",
        "    ax.set_title(f'Silhouette Plot (Average Score: {silhouette_avg:.3f})')\n",
        "    ax.set_xlabel('Silhouette Coefficient')\n",
        "    ax.set_ylabel('Cluster')\n",
        "    ax.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\", label='Average')\n",
        "    ax.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return silhouette_avg\n",
        "\n",
        "\n",
        "def evaluate_clustering(X, labels, name=\"Algorithm\"):\n",
        "    \"\"\"\n",
        "    Calculate and display clustering evaluation metrics\n",
        "    \"\"\"\n",
        "    # Filter out noise points (label = -1) for metrics that don't support them\n",
        "    mask = labels >= 0\n",
        "    X_filtered = X[mask]\n",
        "    labels_filtered = labels[mask]\n",
        "    \n",
        "    if len(np.unique(labels_filtered)) > 1:\n",
        "        silhouette = silhouette_score(X_filtered, labels_filtered)\n",
        "        davies_bouldin = davies_bouldin_score(X_filtered, labels_filtered)\n",
        "        calinski = calinski_harabasz_score(X_filtered, labels_filtered)\n",
        "        \n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"📊 {name} - Evaluation Metrics\")\n",
        "        print(f\"{'='*50}\")\n",
        "        print(f\"Silhouette Score:        {silhouette:.4f} (higher is better, range: [-1, 1])\")\n",
        "        print(f\"Davies-Bouldin Index:    {davies_bouldin:.4f} (lower is better)\")\n",
        "        print(f\"Calinski-Harabasz Score: {calinski:.2f} (higher is better)\")\n",
        "        print(f\"Number of Clusters:      {len(np.unique(labels_filtered))}\")\n",
        "        if -1 in labels:\n",
        "            print(f\"Number of Noise Points:  {np.sum(labels == -1)}\")\n",
        "        print(f\"{'='*50}\\n\")\n",
        "        \n",
        "        return {\n",
        "            'silhouette': silhouette,\n",
        "            'davies_bouldin': davies_bouldin,\n",
        "            'calinski_harabasz': calinski\n",
        "        }\n",
        "    else:\n",
        "        print(f\"⚠️ {name}: Not enough clusters for evaluation\")\n",
        "        return None\n",
        "\n",
        "\n",
        "print(\"✅ Helper functions defined successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load Sample Datasets\n",
        "\n",
        "We'll use multiple datasets throughout this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Iris dataset\n",
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()\n",
        "iris_df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "iris_df['species'] = iris.target\n",
        "iris_df['species_name'] = iris_df['species'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})\n",
        "\n",
        "print(\"🌸 Iris Dataset Loaded\")\n",
        "print(f\"   Shape: {iris_df.shape}\")\n",
        "print(f\"   Features: {list(iris.feature_names)}\")\n",
        "print(f\"   Classes: {list(iris.target_names)}\")\n",
        "\n",
        "# Create synthetic datasets\n",
        "# Dataset 1: Well-separated blobs\n",
        "X_blobs, y_blobs = make_blobs(n_samples=300, centers=4, n_features=2, \n",
        "                               cluster_std=0.6, random_state=42)\n",
        "\n",
        "# Dataset 2: Non-linear patterns (moons)\n",
        "X_moons, y_moons = make_moons(n_samples=300, noise=0.1, random_state=42)\n",
        "\n",
        "# Dataset 3: Circles\n",
        "X_circles, y_circles = make_circles(n_samples=300, noise=0.05, factor=0.5, random_state=42)\n",
        "\n",
        "print(\"\\n🎲 Synthetic Datasets Created\")\n",
        "print(f\"   Blobs: {X_blobs.shape}\")\n",
        "print(f\"   Moons: {X_moons.shape}\")\n",
        "print(f\"   Circles: {X_circles.shape}\")\n",
        "\n",
        "# Visualize datasets\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "axes[0].scatter(X_blobs[:, 0], X_blobs[:, 1], c=y_blobs, cmap='viridis', alpha=0.6, edgecolors='k')\n",
        "axes[0].set_title('Blobs Dataset')\n",
        "axes[0].set_xlabel('Feature 1')\n",
        "axes[0].set_ylabel('Feature 2')\n",
        "\n",
        "axes[1].scatter(X_moons[:, 0], X_moons[:, 1], c=y_moons, cmap='viridis', alpha=0.6, edgecolors='k')\n",
        "axes[1].set_title('Moons Dataset')\n",
        "axes[1].set_xlabel('Feature 1')\n",
        "axes[1].set_ylabel('Feature 2')\n",
        "\n",
        "axes[2].scatter(X_circles[:, 0], X_circles[:, 1], c=y_circles, cmap='viridis', alpha=0.6, edgecolors='k')\n",
        "axes[2].set_title('Circles Dataset')\n",
        "axes[2].set_xlabel('Feature 1')\n",
        "axes[2].set_ylabel('Feature 2')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n✅ All datasets loaded and ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 1: Clustering Fundamentals\n",
        "\n",
        "## What is Clustering?\n",
        "\n",
        "**Clustering** is an unsupervised learning technique that groups similar data points together based on their features. Unlike supervised learning, clustering doesn't require labeled data.\n",
        "\n",
        "### Key Concepts:\n",
        "\n",
        "- **Goal**: Partition *n* observations into *k* groups\n",
        "- **Objective**: Maximize intra-cluster similarity, minimize inter-cluster similarity\n",
        "- **Distance Metrics**: Euclidean, Manhattan, Cosine, Mahalanobis\n",
        "\n",
        "### Types of Clustering:\n",
        "\n",
        "1. **Hard Clustering**: Each point belongs to exactly one cluster (e.g., K-Means)\n",
        "2. **Soft Clustering**: Points have probability distribution over clusters (e.g., GMM)\n",
        "3. **Hierarchical**: Nested clusters forming tree structure (e.g., Agglomerative)\n",
        "4. **Density-Based**: Clusters as high-density regions (e.g., DBSCAN)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 1.1: K-Means Clustering\n",
        "\n",
        "### 📖 Theory\n",
        "\n",
        "**K-Means** is one of the most popular clustering algorithms. It partitions data into K clusters by:\n",
        "\n",
        "1. **Initialize**: Randomly select K centroids\n",
        "2. **Assignment**: Assign each point to nearest centroid\n",
        "3. **Update**: Recompute centroids as cluster means\n",
        "4. **Repeat**: Steps 2-3 until convergence\n",
        "\n",
        "**Strengths:**\n",
        "- ✅ Simple and fast: O(n·k·i·d)\n",
        "- ✅ Scales to large datasets\n",
        "- ✅ Easy to interpret\n",
        "\n",
        "**Weaknesses:**\n",
        "- ❌ Assumes spherical clusters\n",
        "- ❌ Sensitive to initialization\n",
        "- ❌ Requires pre-specified K\n",
        "\n",
        "**Key Parameter:**\n",
        "- `n_clusters` (k): Number of clusters to form\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === K-Means Implementation ===\n",
        "\n",
        "# Use blobs dataset (well-separated clusters)\n",
        "X = X_blobs.copy()\n",
        "\n",
        "# Apply K-Means with k=4\n",
        "kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)\n",
        "labels_kmeans = kmeans.fit_predict(X)\n",
        "centers = kmeans.cluster_centers_\n",
        "\n",
        "# Visualize results\n",
        "plot_clusters(X, labels_kmeans, centers, title=\"K-Means Clustering (k=4)\")\n",
        "\n",
        "# Evaluate\n",
        "metrics = evaluate_clustering(X, labels_kmeans, name=\"K-Means (k=4)\")\n",
        "\n",
        "print(\"\\n💡 Key Observations:\")\n",
        "print(\"   • K-Means successfully identified 4 well-separated clusters\")\n",
        "print(\"   • High silhouette score indicates good cluster separation\")\n",
        "print(\"   • Centroids (red X) represent the mean of each cluster\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 🔍 Finding Optimal K: The Elbow Method\n",
        "\n",
        "How do we choose the right number of clusters? The **Elbow Method** helps us find optimal K by plotting inertia (within-cluster sum of squares) against K.\n",
        "\n",
        "**Inertia**: Sum of squared distances of samples to their closest cluster center"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Elbow Method ===\n",
        "\n",
        "# Test different values of K\n",
        "K_range = range(2, 11)\n",
        "inertias = []\n",
        "silhouette_scores = []\n",
        "\n",
        "for k in K_range:\n",
        "    kmeans_temp = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    labels_temp = kmeans_temp.fit_predict(X)\n",
        "    \n",
        "    inertias.append(kmeans_temp.inertia_)\n",
        "    silhouette_scores.append(silhouette_score(X, labels_temp))\n",
        "\n",
        "# Plot Elbow Curve\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Inertia plot\n",
        "ax1.plot(K_range, inertias, marker='o', linewidth=2, markersize=8)\n",
        "ax1.set_xlabel('Number of Clusters (K)', fontsize=12)\n",
        "ax1.set_ylabel('Inertia (WCSS)', fontsize=12)\n",
        "ax1.set_title('Elbow Method: Inertia vs K', fontsize=14, fontweight='bold')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.axvline(x=4, color='red', linestyle='--', label='Optimal K=4')\n",
        "ax1.legend()\n",
        "\n",
        "# Silhouette score plot\n",
        "ax2.plot(K_range, silhouette_scores, marker='s', linewidth=2, markersize=8, color='green')\n",
        "ax2.set_xlabel('Number of Clusters (K)', fontsize=12)\n",
        "ax2.set_ylabel('Silhouette Score', fontsize=12)\n",
        "ax2.set_title('Silhouette Score vs K', fontsize=14, fontweight='bold')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "ax2.axvline(x=4, color='red', linestyle='--', label='Optimal K=4')\n",
        "ax2.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Find optimal K\n",
        "optimal_k_silhouette = K_range[np.argmax(silhouette_scores)]\n",
        "\n",
        "print(f\"\\n📊 Analysis Results:\")\n",
        "print(f\"   • Elbow point appears around K=4 (inertia starts decreasing slowly)\")\n",
        "print(f\"   • Highest silhouette score at K={optimal_k_silhouette}\")\n",
        "print(f\"   • Recommended K: 4 (both metrics agree)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 💪 Your Turn - Practice Task 1.1\n",
        "\n",
        "**Task:** Apply K-Means to the Iris dataset\n",
        "\n",
        "1. Load the first 2 features of the Iris dataset (sepal length and width)\n",
        "2. Apply K-Means with k=3 (we know there are 3 species)\n",
        "3. Visualize the clusters\n",
        "4. Calculate evaluation metrics\n",
        "5. Compare clustering results with true species labels using confusion matrix\n",
        "\n",
        "**Hint:** Use `X_iris = iris.data[:, :2]` and `y_true = iris.target`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === YOUR CODE HERE ===\n",
        "\n",
        "# Step 1: Prepare data\n",
        "X_iris = iris.data[:, :2]  # First 2 features\n",
        "y_true = iris.target\n",
        "\n",
        "# Step 2: Apply K-Means\n",
        "# TODO: Create KMeans model with n_clusters=3\n",
        "\n",
        "\n",
        "# Step 3: Visualize\n",
        "# TODO: Use plot_clusters() function\n",
        "\n",
        "\n",
        "# Step 4: Evaluate\n",
        "# TODO: Use evaluate_clustering() function\n",
        "\n",
        "\n",
        "# Step 5: Compare with true labels\n",
        "# TODO: Create confusion matrix\n",
        "\n",
        "\n",
        "# === END OF YOUR CODE ===\n",
        "\n",
        "print(\"\\n💡 Reflection Questions:\")\n",
        "print(\"   1. How well did K-Means identify the true species?\")\n",
        "print(\"   2. Which species was easiest/hardest to cluster?\")\n",
        "print(\"   3. Why might clustering not perfectly match species labels?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === SOLUTION (Uncomment to see) ===\n",
        "\n",
        "# # Step 1: Prepare data\n",
        "# X_iris = iris.data[:, :2]\n",
        "# y_true = iris.target\n",
        "\n",
        "# # Step 2: Apply K-Means\n",
        "# kmeans_iris = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
        "# y_pred = kmeans_iris.fit_predict(X_iris)\n",
        "\n",
        "# # Step 3: Visualize\n",
        "# plot_clusters(X_iris, y_pred, kmeans_iris.cluster_centers_, \n",
        "#               title=\"K-Means on Iris (k=3)\")\n",
        "\n",
        "# # Step 4: Evaluate\n",
        "# evaluate_clustering(X_iris, y_pred, name=\"K-Means on Iris\")\n",
        "\n",
        "# # Step 5: Compare with true labels\n",
        "# from sklearn.metrics import confusion_matrix, adjusted_rand_score\n",
        "# cm = confusion_matrix(y_true, y_pred)\n",
        "# ari = adjusted_rand_score(y_true, y_pred)\n",
        "\n",
        "# plt.figure(figsize=(8, 6))\n",
        "# sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "#             xticklabels=['Cluster 0', 'Cluster 1', 'Cluster 2'],\n",
        "#             yticklabels=iris.target_names)\n",
        "# plt.title(f'Confusion Matrix (ARI: {ari:.3f})')\n",
        "# plt.ylabel('True Species')\n",
        "# plt.xlabel('Predicted Cluster')\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n",
        "\n",
        "# print(f\"\\n📊 Adjusted Rand Index: {ari:.3f}\")\n",
        "# print(\"   (1.0 = perfect match, 0.0 = random labeling)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Exercise 1.2: K-Means++ Initialization\n",
        "\n",
        "### 📖 Theory\n",
        "\n",
        "**K-Means++** is an improved initialization strategy that addresses K-Means' sensitivity to initial centroid placement.\n",
        "\n",
        "**Algorithm:**\n",
        "1. Choose first centroid randomly from data points\n",
        "2. For each subsequent centroid:\n",
        "   - Calculate distance D(x) to nearest existing centroid\n",
        "   - Choose next centroid with probability ∝ D(x)²\n",
        "3. Spreads initial centroids far apart\n",
        "\n",
        "**Benefits:**\n",
        "- ✅ Faster convergence (fewer iterations)\n",
        "- ✅ Better quality clusters\n",
        "- ✅ O(log k) approximation guarantee\n",
        "- ✅ Default in scikit-learn\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Compare Random vs K-Means++ Initialization ===\n",
        "\n",
        "# Random initialization\n",
        "kmeans_random = KMeans(n_clusters=4, init='random', n_init=1, max_iter=300, random_state=42)\n",
        "kmeans_random.fit(X)\n",
        "\n",
        "# K-Means++ initialization\n",
        "kmeans_plusplus = KMeans(n_clusters=4, init='k-means++', n_init=1, max_iter=300, random_state=42)\n",
        "kmeans_plusplus.fit(X)\n",
        "\n",
        "# Compare results\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Random init\n",
        "axes[0].scatter(X[:, 0], X[:, 1], c=kmeans_random.labels_, cmap='viridis', alpha=0.6, edgecolors='k')\n",
        "axes[0].scatter(kmeans_random.cluster_centers_[:, 0], \n",
        "                kmeans_random.cluster_centers_[:, 1],\n",
        "                c='red', marker='X', s=200, edgecolors='black', linewidths=2)\n",
        "axes[0].set_title(f'Random Init\\nInertia: {kmeans_random.inertia_:.2f} | Iterations: {kmeans_random.n_iter_}')\n",
        "axes[0].set_xlabel('Feature 1')\n",
        "axes[0].set_ylabel('Feature 2')\n",
        "\n",
        "# K-Means++ init\n",
        "axes[1].scatter(X[:, 0], X[:, 1], c=kmeans_plusplus.labels_, cmap='viridis', alpha=0.6, edgecolors='k')\n",
        "axes[1].scatter(kmeans_plusplus.cluster_centers_[:, 0], \n",
        "                kmeans_plusplus.cluster_centers_[:, 1],\n",
        "                c='red', marker='X', s=200, edgecolors='black', linewidths=2)\n",
        "axes[1].set_title(f'K-Means++ Init\\nInertia: {kmeans_plusplus.inertia_:.2f} | Iterations: {kmeans_plusplus.n_iter_}')\n",
        "axes[1].set_xlabel('Feature 1')\n",
        "axes[1].set_ylabel('Feature 2')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n📊 Comparison Results:\")\n",
        "print(f\"\\n{'Method':<20} {'Inertia':<15} {'Iterations':<15} {'Silhouette'}\")\n",
        "print(\"-\" * 65)\n",
        "print(f\"{'Random Init':<20} {kmeans_random.inertia_:<15.2f} {kmeans_random.n_iter_:<15} \"\n",
        "      f\"{silhouette_score(X, kmeans_random.labels_):.4f}\")\n",
        "print(f\"{'K-Means++':<20} {kmeans_plusplus.inertia_:<15.2f} {kmeans_plusplus.n_iter_:<15} \"\n",
        "      f\"{silhouette_score(X, kmeans_plusplus.labels_):.4f}\")\n",
        "\n",
        "print(\"\\n💡 Key Observations:\")\n",
        "print(\"   • K-Means++ typically converges faster (fewer iterations)\")\n",
        "print(\"   • K-Means++ often achieves lower inertia (better clusters)\")\n",
        "print(\"   • Small initialization overhead → Large quality gains\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Exercise 1.3: Hierarchical Clustering\n",
        "\n",
        "### 📖 Theory\n",
        "\n",
        "**Hierarchical Clustering** builds a hierarchy of clusters, represented as a **dendrogram** (tree diagram).\n",
        "\n",
        "**Two Approaches:**\n",
        "\n",
        "1. **Agglomerative (Bottom-Up)**: ⬆️\n",
        "   - Start: Each point is its own cluster\n",
        "   - Repeatedly merge closest clusters\n",
        "   - End: All points in one cluster\n",
        "\n",
        "2. **Divisive (Top-Down)**: ⬇️\n",
        "   - Start: All points in one cluster\n",
        "   - Repeatedly split clusters\n",
        "   - End: Each point is its own cluster\n",
        "\n",
        "**Linkage Criteria** (how to measure cluster distance):\n",
        "- **Single**: Minimum distance between points\n",
        "- **Complete**: Maximum distance between points\n",
        "- **Average**: Average distance between all pairs\n",
        "- **Ward** ⭐: Minimizes within-cluster variance (most common)\n",
        "\n",
        "**Advantages:**\n",
        "- ✅ No need to specify K upfront\n",
        "- ✅ Dendrogram provides insights\n",
        "- ✅ Can create any number of clusters by cutting tree\n",
        "\n",
        "**Disadvantages:**\n",
        "- ❌ O(n³) complexity - not scalable\n",
        "- ❌ Sensitive to noise and outliers\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Hierarchical Clustering with Dendrogram ===\n",
        "\n",
        "# Use a smaller sample for better visualization\n",
        "np.random.seed(42)\n",
        "sample_indices = np.random.choice(len(X), size=100, replace=False)\n",
        "X_sample = X[sample_indices]\n",
        "\n",
        "# Compute linkage matrix\n",
        "linkage_matrix = linkage(X_sample, method='ward')\n",
        "\n",
        "# Create dendrogram\n",
        "plt.figure(figsize=(14, 6))\n",
        "dendrogram(linkage_matrix, \n",
        "           truncate_mode='lastp',  # Show only last p merged clusters\n",
        "           p=12,\n",
        "           leaf_rotation=90,\n",
        "           leaf_font_size=10,\n",
        "           show_contracted=True)\n",
        "\n",
        "plt.axhline(y=10, color='r', linestyle='--', label='Cut at height=10 → 4 clusters')\n",
        "plt.title('Hierarchical Clustering Dendrogram (Ward Linkage)', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Sample Index or (Cluster Size)', fontsize=12)\n",
        "plt.ylabel('Distance (Ward)', fontsize=12)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n📊 Dendrogram Interpretation:\")\n",
        "print(\"   • Height: Distance at which clusters merge\")\n",
        "print(\"   • Vertical lines: Represent clusters being merged\")\n",
        "print(\"   • Horizontal cut: Determines number of clusters\")\n",
        "print(\"   • Red dashed line: Cutting here gives 4 clusters\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Apply Agglomerative Clustering ===\n",
        "\n",
        "# Fit hierarchical clustering with n_clusters=4\n",
        "hierarchical = AgglomerativeClustering(n_clusters=4, linkage='ward')\n",
        "labels_hier = hierarchical.fit_predict(X)\n",
        "\n",
        "# Visualize results\n",
        "plot_clusters(X, labels_hier, title=\"Hierarchical Clustering (Ward, k=4)\")\n",
        "\n",
        "# Evaluate\n",
        "evaluate_clustering(X, labels_hier, name=\"Hierarchical (Ward)\")\n",
        "\n",
        "print(\"\\n💡 Key Observations:\")\n",
        "print(\"   • Ward linkage minimizes within-cluster variance\")\n",
        "print(\"   • Results similar to K-Means for well-separated clusters\")\n",
        "print(\"   • Dendrogram helps visualize cluster formation process\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 💪 Your Turn - Practice Task 1.3\n",
        "\n",
        "**Task:** Compare different linkage methods\n",
        "\n",
        "1. Apply hierarchical clustering with three linkage methods: 'single', 'complete', 'average'\n",
        "2. Use n_clusters=4 for all methods\n",
        "3. Visualize results side-by-side\n",
        "4. Calculate silhouette scores for each method\n",
        "5. Which linkage method works best for this data?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === YOUR CODE HERE ===\n",
        "\n",
        "# TODO: Apply hierarchical clustering with different linkage methods\n",
        "# linkages = ['single', 'complete', 'average']\n",
        "\n",
        "\n",
        "# TODO: Visualize and compare\n",
        "\n",
        "\n",
        "# TODO: Compare silhouette scores\n",
        "\n",
        "\n",
        "# === END OF YOUR CODE ==="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Exercise 1.4: DBSCAN - Density-Based Clustering\n",
        "\n",
        "### 📖 Theory\n",
        "\n",
        "**DBSCAN** (Density-Based Spatial Clustering of Applications with Noise) discovers clusters of arbitrary shape and identifies outliers.\n",
        "\n",
        "**Key Principle:** Clusters are high-density regions separated by low-density regions.\n",
        "\n",
        "**Parameters:**\n",
        "- **ε (epsilon)**: Neighborhood radius\n",
        "- **minPts**: Minimum points to form dense region\n",
        "\n",
        "**Point Types:**\n",
        "1. **Core Points** 🟢: ≥ minPts neighbors within ε\n",
        "2. **Border Points** 🟡: In neighborhood of core, but not core itself\n",
        "3. **Noise Points** 🔴: Neither core nor border (outliers)\n",
        "\n",
        "**Advantages:**\n",
        "- ✅ Discovers arbitrary-shaped clusters\n",
        "- ✅ Automatically detects outliers\n",
        "- ✅ No need to specify number of clusters\n",
        "- ✅ Robust to outliers\n",
        "\n",
        "**Disadvantages:**\n",
        "- ❌ Sensitive to ε and minPts parameters\n",
        "- ❌ Struggles with varying densities\n",
        "- ❌ High-dimensional data challenges\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === DBSCAN on Non-linear Data ===\n",
        "\n",
        "# Use moons dataset (non-linear clusters)\n",
        "X_moons_scaled = StandardScaler().fit_transform(X_moons)\n",
        "\n",
        "# Apply DBSCAN\n",
        "dbscan = DBSCAN(eps=0.3, min_samples=5)\n",
        "labels_dbscan = dbscan.fit_predict(X_moons_scaled)\n",
        "\n",
        "# Count noise points\n",
        "n_clusters = len(set(labels_dbscan)) - (1 if -1 in labels_dbscan else 0)\n",
        "n_noise = list(labels_dbscan).count(-1)\n",
        "\n",
        "# Visualize\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Original data\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(X_moons[:, 0], X_moons[:, 1], c=y_moons, cmap='viridis', \n",
        "           alpha=0.6, edgecolors='k', s=50)\n",
        "plt.title('True Labels (Moons Dataset)')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "\n",
        "# DBSCAN results\n",
        "plt.subplot(1, 2, 2)\n",
        "unique_labels = set(labels_dbscan)\n",
        "colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))\n",
        "\n",
        "for k, col in zip(unique_labels, colors):\n",
        "    if k == -1:\n",
        "        # Noise points in black\n",
        "        col = 'black'\n",
        "        marker = 'x'\n",
        "        label = 'Noise'\n",
        "    else:\n",
        "        marker = 'o'\n",
        "        label = f'Cluster {k}'\n",
        "    \n",
        "    class_member_mask = (labels_dbscan == k)\n",
        "    xy = X_moons_scaled[class_member_mask]\n",
        "    plt.scatter(xy[:, 0], xy[:, 1], c=[col], marker=marker, \n",
        "               alpha=0.6, edgecolors='k', s=50, label=label)\n",
        "\n",
        "plt.title(f'DBSCAN Results\\n{n_clusters} clusters, {n_noise} noise points')\n",
        "plt.xlabel('Feature 1 (scaled)')\n",
        "plt.ylabel('Feature 2 (scaled)')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate (excluding noise points)\n",
        "if n_clusters > 1:\n",
        "    evaluate_clustering(X_moons_scaled, labels_dbscan, name=\"DBSCAN\")\n",
        "\n",
        "print(\"\\n💡 Key Observations:\")\n",
        "print(\"   • DBSCAN successfully identified non-linear (moon-shaped) clusters\")\n",
        "print(\"   • K-Means would fail on this data (assumes spherical clusters)\")\n",
        "print(\"   • Noise points (black X) automatically detected\")\n",
        "print(\"   • No need to specify number of clusters in advance\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 🔧 Parameter Sensitivity Analysis\n",
        "\n",
        "Let's see how ε (epsilon) affects clustering results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === DBSCAN Parameter Sensitivity ===\n",
        "\n",
        "eps_values = [0.2, 0.3, 0.4, 0.5]\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for idx, eps in enumerate(eps_values):\n",
        "    dbscan_temp = DBSCAN(eps=eps, min_samples=5)\n",
        "    labels_temp = dbscan_temp.fit_predict(X_moons_scaled)\n",
        "    \n",
        "    n_clusters_temp = len(set(labels_temp)) - (1 if -1 in labels_temp else 0)\n",
        "    n_noise_temp = list(labels_temp).count(-1)\n",
        "    \n",
        "    # Plot\n",
        "    unique_labels = set(labels_temp)\n",
        "    colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))\n",
        "    \n",
        "    for k, col in zip(unique_labels, colors):\n",
        "        if k == -1:\n",
        "            col = 'black'\n",
        "            marker = 'x'\n",
        "        else:\n",
        "            marker = 'o'\n",
        "        \n",
        "        class_member_mask = (labels_temp == k)\n",
        "        xy = X_moons_scaled[class_member_mask]\n",
        "        axes[idx].scatter(xy[:, 0], xy[:, 1], c=[col], marker=marker, \n",
        "                         alpha=0.6, edgecolors='k', s=40)\n",
        "    \n",
        "    axes[idx].set_title(f'ε = {eps}\\nClusters: {n_clusters_temp}, Noise: {n_noise_temp}')\n",
        "    axes[idx].set_xlabel('Feature 1')\n",
        "    axes[idx].set_ylabel('Feature 2')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n📊 Parameter Sensitivity:\")\n",
        "print(\"   • Small ε: More clusters, more noise points\")\n",
        "print(\"   • Large ε: Fewer clusters, points merge together\")\n",
        "print(\"   • Optimal ε: Balance between separation and connectivity\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 💪 Your Turn - Practice Task 1.4\n",
        "\n",
        "**Task:** Apply DBSCAN to the circles dataset\n",
        "\n",
        "1. Scale the `X_circles` dataset using StandardScaler\n",
        "2. Apply DBSCAN with different epsilon values: [0.1, 0.2, 0.3, 0.4]\n",
        "3. For each epsilon, count the number of clusters and noise points\n",
        "4. Visualize the best result\n",
        "5. Compare with K-Means results on the same data\n",
        "\n",
        "**Question:** Why does DBSCAN work better than K-Means for this dataset?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === YOUR CODE HERE ===\n",
        "\n",
        "# TODO: Scale the circles dataset\n",
        "\n",
        "\n",
        "# TODO: Try different epsilon values\n",
        "\n",
        "\n",
        "# TODO: Apply K-Means for comparison\n",
        "\n",
        "\n",
        "# TODO: Visualize and compare\n",
        "\n",
        "\n",
        "# === END OF YOUR CODE ==="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 2: Cluster Evaluation Metrics\n",
        "\n",
        "## The Challenge: No Ground Truth\n",
        "\n",
        "Unlike supervised learning, clustering has no \"correct\" answers. So how do we evaluate cluster quality?\n",
        "\n",
        "### Two Types of Metrics:\n",
        "\n",
        "1. **Internal Metrics** 📊: Use only the data itself\n",
        "   - Silhouette Score\n",
        "   - Davies-Bouldin Index  \n",
        "   - Calinski-Harabasz Index\n",
        "   - Inertia (WCSS)\n",
        "\n",
        "2. **External Metrics** 🏷️: Compare with ground truth labels (when available)\n",
        "   - Adjusted Rand Index (ARI)\n",
        "   - Normalized Mutual Information (NMI)\n",
        "   - Fowlkes-Mallows Index\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 2.1: Internal Metrics\n",
        "\n",
        "### 📖 Silhouette Score\n",
        "\n",
        "**Definition:** Measures how similar an object is to its own cluster compared to other clusters.\n",
        "\n",
        "**Formula for point i:**\n",
        "```\n",
        "s(i) = (b(i) - a(i)) / max(a(i), b(i))\n",
        "```\n",
        "\n",
        "Where:\n",
        "- **a(i)**: Mean distance to other points in same cluster (cohesion)\n",
        "- **b(i)**: Mean distance to points in nearest cluster (separation)\n",
        "\n",
        "**Range:** [-1, 1]\n",
        "- +1: Perfect clustering (far from other clusters)\n",
        "- 0: On decision boundary\n",
        "- -1: Likely in wrong cluster\n",
        "\n",
        "**Interpretation:**\n",
        "- > 0.7: Strong structure\n",
        "- 0.5 - 0.7: Reasonable structure\n",
        "- 0.25 - 0.5: Weak structure\n",
        "- < 0.25: No substantial structure\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Comprehensive Silhouette Analysis ===\n",
        "\n",
        "# Apply K-Means with different K values\n",
        "K_values = [2, 3, 4, 5, 6]\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "axes = axes.ravel()\n",
        "\n",
        "metrics_comparison = []\n",
        "\n",
        "for idx, k in enumerate(K_values):\n",
        "    # Fit K-Means\n",
        "    kmeans_temp = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    labels_temp = kmeans_temp.fit_predict(X)\n",
        "    \n",
        "    # Calculate metrics\n",
        "    silhouette_avg = silhouette_score(X, labels_temp)\n",
        "    sample_silhouette_values = silhouette_samples(X, labels_temp)\n",
        "    \n",
        "    metrics_comparison.append({\n",
        "        'K': k,\n",
        "        'Silhouette': silhouette_avg,\n",
        "        'Davies-Bouldin': davies_bouldin_score(X, labels_temp),\n",
        "        'Calinski-Harabasz': calinski_harabasz_score(X, labels_temp),\n",
        "        'Inertia': kmeans_temp.inertia_\n",
        "    })\n",
        "    \n",
        "    # Plot silhouette\n",
        "    ax = axes[idx]\n",
        "    y_lower = 10\n",
        "    \n",
        "    for i in range(k):\n",
        "        ith_cluster_silhouette_values = sample_silhouette_values[labels_temp == i]\n",
        "        ith_cluster_silhouette_values.sort()\n",
        "        \n",
        "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
        "        y_upper = y_lower + size_cluster_i\n",
        "        \n",
        "        color = plt.cm.nipy_spectral(float(i) / k)\n",
        "        ax.fill_betweenx(np.arange(y_lower, y_upper),\n",
        "                        0, ith_cluster_silhouette_values,\n",
        "                        facecolor=color, edgecolor=color, alpha=0.7)\n",
        "        \n",
        "        ax.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
        "        y_lower = y_upper + 10\n",
        "    \n",
        "    ax.set_title(f'K={k} (Silhouette: {silhouette_avg:.3f})')\n",
        "    ax.set_xlabel('Silhouette Coefficient')\n",
        "    ax.set_ylabel('Cluster')\n",
        "    ax.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
        "    ax.set_xlim([-0.2, 1])\n",
        "\n",
        "# Hide last subplot\n",
        "axes[-1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Display comparison table\n",
        "metrics_df = pd.DataFrame(metrics_comparison)\n",
        "print(\"\\n📊 Clustering Metrics Comparison:\\n\")\n",
        "print(metrics_df.to_string(index=False))\n",
        "print(\"\\n💡 Best K based on Silhouette Score:\", \n",
        "      metrics_df.loc[metrics_df['Silhouette'].idxmax(), 'K'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 6: Summary and Key Takeaways\n",
        "\n",
        "## 🎓 What We've Learned\n",
        "\n",
        "### Clustering Algorithms\n",
        "\n",
        "| Algorithm | Best For | Pros | Cons |\n",
        "|-----------|----------|------|------|\n",
        "| **K-Means** | Spherical, well-separated clusters | Fast, scalable | Needs K, assumes spherical |\n",
        "| **Hierarchical** | Small datasets, dendrogram needed | No K needed, intuitive | O(n³), not scalable |\n",
        "| **DBSCAN** | Arbitrary shapes, noise detection | Finds outliers, no K | Parameter sensitive |\n",
        "| **GMM** | Elliptical clusters, soft assignment | Probabilistic, flexible | Assumes Gaussian |\n",
        "\n",
        "### Dimensionality Reduction\n",
        "\n",
        "| Method | Purpose | Pros | Cons |\n",
        "|--------|---------|------|------|\n",
        "| **PCA** | Linear projection | Fast, interpretable | Linear only |\n",
        "| **t-SNE** | Visualization | Great for viz | Slow, not for general dim reduction |\n",
        "| **UMAP** | Visualization + DR | Fast, preserves structure | Newer, parameter tuning |\n",
        "| **Autoencoder** | Complex patterns | Non-linear, flexible | Needs more data, tuning |\n",
        "\n",
        "### Anomaly Detection\n",
        "\n",
        "| Method | Approach | Best For |\n",
        "|--------|----------|----------|\n",
        "| **Statistical** | Z-score, IQR | Gaussian data, simple cases |\n",
        "| **Isolation Forest** | Tree-based isolation | High-dim, scalable |\n",
        "| **DBSCAN** | Density | Spatial data, clusters + outliers |\n",
        "\n",
        "---\n",
        "\n",
        "## 🎯 Algorithm Selection Guide\n",
        "\n",
        "```\n",
        "START\n",
        "  ↓\n",
        "Do you know the number of clusters?\n",
        "  ├─ YES → K-Means or GMM\n",
        "  └─ NO → DBSCAN or Hierarchical\n",
        "       ↓\n",
        "Are clusters spherical?\n",
        "  ├─ YES → K-Means (fastest)\n",
        "  └─ NO → DBSCAN (arbitrary shapes)\n",
        "       ↓\n",
        "Do you need probability assignments?\n",
        "  ├─ YES → GMM (soft clustering)\n",
        "  └─ NO → K-Means (hard clustering)\n",
        "       ↓\n",
        "Do you need to detect outliers?\n",
        "  ├─ YES → DBSCAN or Isolation Forest\n",
        "  └─ NO → K-Means or GMM\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 📚 Key Principles\n",
        "\n",
        "1. **Always scale your data** before clustering (except tree-based methods)\n",
        "2. **Try multiple algorithms** - no single algorithm works for all data\n",
        "3. **Use multiple metrics** - single metric can be misleading\n",
        "4. **Domain validation is crucial** - metrics don't tell the whole story\n",
        "5. **Visualize results** - 2D/3D plots reveal insights\n",
        "\n",
        "---\n",
        "\n",
        "## 🚀 Next Steps\n",
        "\n",
        "### Advanced Topics (Lecture 18+):\n",
        "- Deep Autoencoders for complex patterns\n",
        "- Variational Autoencoders (VAE) for generation\n",
        "- Self-supervised learning\n",
        "- Graph-based clustering\n",
        "- Time-series clustering\n",
        "\n",
        "### Practice Recommendations:\n",
        "1. Apply to real-world datasets (Kaggle, UCI ML Repository)\n",
        "2. Combine multiple techniques (dim reduction → clustering)\n",
        "3. Build end-to-end pipelines\n",
        "4. Experiment with parameters systematically\n",
        "\n",
        "---\n",
        "\n",
        "## 📖 Resources\n",
        "\n",
        "**Documentation:**\n",
        "- Scikit-learn: https://scikit-learn.org/stable/modules/clustering.html\n",
        "- UMAP: https://umap-learn.readthedocs.io/\n",
        "\n",
        "**Books:**\n",
        "- \"Hands-On Machine Learning\" by Aurélien Géron\n",
        "- \"Pattern Recognition and Machine Learning\" by Christopher Bishop\n",
        "\n",
        "**Papers:**\n",
        "- K-Means++: Arthur & Vassilvitskii (2007)\n",
        "- DBSCAN: Ester et al. (1996)\n",
        "- t-SNE: van der Maaten & Hinton (2008)\n",
        "- UMAP: McInnes et al. (2018)\n",
        "\n",
        "---\n",
        "\n",
        "## 🎉 Congratulations!\n",
        "\n",
        "You've completed the comprehensive hands-on practice for **Lecture 17: Clustering and Unsupervised Learning Fundamentals**!\n",
        "\n",
        "You now have practical experience with:\n",
        "- ✅ 5+ clustering algorithms\n",
        "- ✅ Multiple evaluation metrics\n",
        "- ✅ Dimensionality reduction techniques\n",
        "- ✅ Anomaly detection methods\n",
        "- ✅ Real-world applications\n",
        "\n",
        "Keep practicing and exploring! 🚀"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "**Created by:** Ho-min Park  \n",
        "**Contact:** homin.park@ghent.ac.kr | powersimmani@gmail.com  \n",
        "**Date:** 2024  \n",
        "**License:** Educational Use  \n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}