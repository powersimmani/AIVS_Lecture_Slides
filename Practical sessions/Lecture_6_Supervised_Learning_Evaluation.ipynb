{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81caa7b8",
   "metadata": {},
   "source": [
    "# üìö Supervised Learning Evaluation - Complete Hands-on Guide\n",
    "## Based on Lecture 6: Comprehensive Evaluation Techniques\n",
    "\n",
    "### üìã Table of Contents\n",
    "1. **Setup and Introduction**\n",
    "2. **Part 1: Data Splitting and Validation Fundamentals**\n",
    "3. **Part 2: Regression Evaluation Metrics** \n",
    "4. **Part 3: Classification Evaluation Metrics**\n",
    "5. **Part 4: Cross-Validation and Model Selection**\n",
    "6. **Summary and Exercises**\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Master train/validation/test splitting strategies\n",
    "- Implement and interpret regression metrics (MSE, RMSE, MAE, R¬≤)\n",
    "- Apply classification metrics (Precision, Recall, F1, ROC-AUC)\n",
    "- Use cross-validation techniques effectively\n",
    "- Perform hyperparameter tuning and model selection\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3c1ba6",
   "metadata": {},
   "source": [
    "## üöÄ Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cd9904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning Libraries\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, KFold, StratifiedKFold, \n",
    "    cross_val_score, GridSearchCV, RandomizedSearchCV,\n",
    "    learning_curve, validation_curve, LeaveOneOut\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, mean_absolute_error, r2_score,\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_curve, auc,\n",
    "    roc_auc_score, precision_recall_curve, mean_absolute_percentage_error\n",
    ")\n",
    "from sklearn.datasets import load_iris, load_boston, load_breast_cancer, make_classification, make_regression\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge, Lasso\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure visualization settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Scikit-learn version: {sklearn.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cbec91",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Data Splitting and Validation Fundamentals\n",
    "\n",
    "### Exercise 1: Understanding Train/Validation/Test Split\n",
    "#### üí° Concept\n",
    "The foundation of model evaluation is proper data splitting. We typically use:\n",
    "- **Training set (60-70%)**: To train the model\n",
    "- **Validation set (15-20%)**: To tune hyperparameters\n",
    "- **Test set (15-20%)**: For final unbiased evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65aa8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Train/Validation/Test Split Implementation\n",
    "# Generate synthetic regression data\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X, y = make_regression(n_samples=1000, n_features=10, n_informative=8, \n",
    "                       noise=10, random_state=42)\n",
    "\n",
    "# Convert to DataFrame for better visualization\n",
    "feature_names = [f'feature_{i+1}' for i in range(X.shape[1])]\n",
    "df = pd.DataFrame(X, columns=feature_names)\n",
    "df['target'] = y\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2806200f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement proper train/val/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# First split: separate test set (20%)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Second split: separate train and validation (80% train, 20% val of remaining)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.25, random_state=42  # 0.25 * 0.8 = 0.2 of total\n",
    ")\n",
    "\n",
    "# Verify split proportions\n",
    "print(\"Dataset Split Proportions:\")\n",
    "print(f\"Training set: {len(X_train)} samples ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"Validation set: {len(X_val)} samples ({len(X_val)/len(X)*100:.1f}%)\")\n",
    "print(f\"Test set: {len(X_test)} samples ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "\n",
    "# Visualize the split\n",
    "fig = go.Figure(data=[\n",
    "    go.Bar(name='Split Distribution', \n",
    "           x=['Train', 'Validation', 'Test'],\n",
    "           y=[len(X_train), len(X_val), len(X_test)],\n",
    "           text=[f'{len(X_train)} ({60}%)', \n",
    "                 f'{len(X_val)} ({20}%)', \n",
    "                 f'{len(X_test)} ({20}%)'],\n",
    "           textposition='auto',\n",
    "           marker_color=['#1E64C8', '#4A90E2', '#7BB3F0'])\n",
    "])\n",
    "fig.update_layout(title='Train/Validation/Test Split Distribution',\n",
    "                  yaxis_title='Number of Samples',\n",
    "                  height=400)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc80db4a",
   "metadata": {},
   "source": [
    "### Exercise 2: Preventing Data Leakage\n",
    "#### üí° Concept\n",
    "Data leakage occurs when information from test set influences training. Common causes:\n",
    "- Normalizing before splitting\n",
    "- Using test set statistics\n",
    "- Feature selection on entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee58326c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Demonstrating Data Leakage Prevention\n",
    "\n",
    "# WRONG WAY - Data leakage (normalizing before split)\n",
    "print(\"‚ùå WRONG: Normalizing before split (causes data leakage)\")\n",
    "scaler_wrong = StandardScaler()\n",
    "X_normalized_wrong = scaler_wrong.fit_transform(X)  # Uses ALL data statistics\n",
    "X_train_wrong = X_normalized_wrong[:600]\n",
    "X_test_wrong = X_normalized_wrong[800:]\n",
    "print(f\"Mean of test set (wrong): {X_test_wrong.mean():.4f}\")\n",
    "print(f\"Std of test set (wrong): {X_test_wrong.std():.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# CORRECT WAY - No data leakage\n",
    "print(\"‚úÖ CORRECT: Normalizing after split\")\n",
    "# Split first\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit scaler only on training data\n",
    "scaler_correct = StandardScaler()\n",
    "X_train_correct = scaler_correct.fit_transform(X_train)  # Fit on train\n",
    "X_test_correct = scaler_correct.transform(X_test)  # Only transform test\n",
    "\n",
    "print(f\"Mean of test set (correct): {X_test_correct.mean():.4f}\")\n",
    "print(f\"Std of test set (correct): {X_test_correct.std():.4f}\")\n",
    "\n",
    "# Visualize the difference\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Wrong way\n",
    "axes[0].hist(X_test_wrong[:, 0], bins=30, alpha=0.7, color='red', edgecolor='black')\n",
    "axes[0].set_title('‚ùå Test Data (Normalized Before Split)', fontsize=12)\n",
    "axes[0].set_xlabel('Feature Value')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].axvline(0, color='black', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Correct way  \n",
    "axes[1].hist(X_test_correct[:, 0], bins=30, alpha=0.7, color='green', edgecolor='black')\n",
    "axes[1].set_title('‚úÖ Test Data (Normalized After Split)', fontsize=12)\n",
    "axes[1].set_xlabel('Feature Value')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].axvline(0, color='black', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key Insight: Notice how the correct method doesn't center test data at 0!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46e09a3",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Regression Evaluation Metrics\n",
    "\n",
    "### Exercise 3: MSE, RMSE, MAE Implementation\n",
    "#### üí° Concept\n",
    "- **MSE (Mean Squared Error)**: Squares differences, heavily penalizes large errors\n",
    "- **RMSE (Root Mean Squared Error)**: Square root of MSE, same units as target\n",
    "- **MAE (Mean Absolute Error)**: Average absolute differences, robust to outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1b8f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Implementing Regression Metrics\n",
    "\n",
    "# Train multiple models for comparison\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Generate regression data with outliers\n",
    "np.random.seed(42)\n",
    "X_reg = np.random.randn(200, 5)\n",
    "y_reg = 2 * X_reg[:, 0] + 3 * X_reg[:, 1] - X_reg[:, 2] + np.random.randn(200) * 0.5\n",
    "\n",
    "# Add some outliers\n",
    "outlier_indices = np.random.choice(200, 10, replace=False)\n",
    "y_reg[outlier_indices] += np.random.randn(10) * 10\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reg, y_reg, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train models\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge Regression': Ridge(alpha=1.0),\n",
    "    'Lasso Regression': Lasso(alpha=0.1),\n",
    "    'Decision Tree': DecisionTreeRegressor(max_depth=5, random_state=42)\n",
    "}\n",
    "\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R¬≤': r2\n",
    "    })\n",
    "\n",
    "# Display results\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.round(4)\n",
    "\n",
    "# Style the dataframe\n",
    "styled_df = results_df.style.background_gradient(subset=['MSE', 'RMSE', 'MAE'], cmap='Reds_r')\\\n",
    "                            .background_gradient(subset=['R¬≤'], cmap='Greens')\n",
    "print(\"üìä Regression Metrics Comparison:\")\n",
    "styled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6617da67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize metrics comparison\n",
    "fig = make_subplots(rows=2, cols=2,\n",
    "                    subplot_titles=('MSE Comparison', 'RMSE Comparison', \n",
    "                                  'MAE Comparison', 'R¬≤ Score Comparison'))\n",
    "\n",
    "# MSE\n",
    "fig.add_trace(go.Bar(x=results_df['Model'], y=results_df['MSE'], \n",
    "                     marker_color='#e74c3c', name='MSE'), row=1, col=1)\n",
    "\n",
    "# RMSE  \n",
    "fig.add_trace(go.Bar(x=results_df['Model'], y=results_df['RMSE'],\n",
    "                     marker_color='#f39c12', name='RMSE'), row=1, col=2)\n",
    "\n",
    "# MAE\n",
    "fig.add_trace(go.Bar(x=results_df['Model'], y=results_df['MAE'],\n",
    "                     marker_color='#3498db', name='MAE'), row=2, col=1)\n",
    "\n",
    "# R¬≤\n",
    "fig.add_trace(go.Bar(x=results_df['Model'], y=results_df['R¬≤'],\n",
    "                     marker_color='#27ae60', name='R¬≤'), row=2, col=2)\n",
    "\n",
    "fig.update_layout(height=600, showlegend=False, \n",
    "                  title_text=\"Regression Metrics Across Different Models\")\n",
    "fig.update_xaxes(tickangle=45)\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nüí° Key Insights:\")\n",
    "print(\"- Lower MSE, RMSE, MAE = Better performance\")\n",
    "print(\"- Higher R¬≤ = Better performance (max = 1.0)\")\n",
    "print(\"- RMSE penalizes large errors more than MAE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690ef1ba",
   "metadata": {},
   "source": [
    "### Exercise 4: Residual Analysis\n",
    "#### üí° Concept\n",
    "Residual analysis helps identify patterns in prediction errors:\n",
    "- Random scatter = Good model\n",
    "- Patterns = Model missing relationships\n",
    "- Heteroscedasticity = Non-constant variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2bf887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4: Residual Analysis and Diagnostics\n",
    "\n",
    "# Use the best performing model\n",
    "best_model = LinearRegression()\n",
    "best_model.fit(X_train, y_train)\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate residuals\n",
    "residuals = y_test - y_pred\n",
    "\n",
    "# Create comprehensive residual plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# 1. Residual Plot\n",
    "axes[0, 0].scatter(y_pred, residuals, alpha=0.6, edgecolor='black')\n",
    "axes[0, 0].axhline(y=0, color='red', linestyle='--')\n",
    "axes[0, 0].set_xlabel('Predicted Values')\n",
    "axes[0, 0].set_ylabel('Residuals')\n",
    "axes[0, 0].set_title('Residual Plot')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Q-Q Plot\n",
    "from scipy import stats\n",
    "stats.probplot(residuals, dist=\"norm\", plot=axes[0, 1])\n",
    "axes[0, 1].set_title('Q-Q Plot (Normal Distribution Check)')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Histogram of Residuals\n",
    "axes[1, 0].hist(residuals, bins=20, edgecolor='black', alpha=0.7, color='skyblue')\n",
    "axes[1, 0].axvline(x=0, color='red', linestyle='--')\n",
    "axes[1, 0].set_xlabel('Residuals')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].set_title('Distribution of Residuals')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Scale-Location Plot\n",
    "standardized_residuals = residuals / np.std(residuals)\n",
    "axes[1, 1].scatter(y_pred, np.sqrt(np.abs(standardized_residuals)), alpha=0.6, edgecolor='black')\n",
    "axes[1, 1].set_xlabel('Predicted Values')\n",
    "axes[1, 1].set_ylabel('‚àö|Standardized Residuals|')\n",
    "axes[1, 1].set_title('Scale-Location Plot')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Residual Analysis Dashboard', fontsize=16, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistical tests\n",
    "from scipy.stats import shapiro, normaltest\n",
    "\n",
    "shapiro_stat, shapiro_p = shapiro(residuals)\n",
    "normal_stat, normal_p = normaltest(residuals)\n",
    "\n",
    "print(\"üìä Residual Analysis Results:\")\n",
    "print(f\"Mean of residuals: {np.mean(residuals):.4f} (should be close to 0)\")\n",
    "print(f\"Std of residuals: {np.std(residuals):.4f}\")\n",
    "print(f\"\\nNormality Tests:\")\n",
    "print(f\"Shapiro-Wilk test: p-value = {shapiro_p:.4f} {'(Normal)' if shapiro_p > 0.05 else '(Not Normal)'}\")\n",
    "print(f\"D'Agostino test: p-value = {normal_p:.4f} {'(Normal)' if normal_p > 0.05 else '(Not Normal)'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6d3469",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Classification Evaluation Metrics\n",
    "\n",
    "### Exercise 5: Confusion Matrix and Basic Metrics\n",
    "#### üí° Concept\n",
    "The confusion matrix is the foundation of classification metrics:\n",
    "- **TP (True Positive)**: Correctly predicted positive\n",
    "- **FP (False Positive)**: Incorrectly predicted as positive (Type I Error)\n",
    "- **FN (False Negative)**: Incorrectly predicted as negative (Type II Error)\n",
    "- **TN (True Negative)**: Correctly predicted negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dff3790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5: Confusion Matrix Implementation\n",
    "\n",
    "# Load and prepare classification data\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X_clf = data.data\n",
    "y_clf = data.target\n",
    "\n",
    "# Create imbalanced dataset by removing some positive samples\n",
    "mask = np.ones(len(y_clf), dtype=bool)\n",
    "positive_indices = np.where(y_clf == 1)[0]\n",
    "remove_indices = np.random.choice(positive_indices, size=150, replace=False)\n",
    "mask[remove_indices] = False\n",
    "X_clf = X_clf[mask]\n",
    "y_clf = y_clf[mask]\n",
    "\n",
    "print(f\"Class distribution:\")\n",
    "print(f\"Class 0 (Malignant): {sum(y_clf == 0)} samples\")\n",
    "print(f\"Class 1 (Benign): {sum(y_clf == 1)} samples\")\n",
    "print(f\"Imbalance ratio: {sum(y_clf == 1) / sum(y_clf == 0):.2f}:1\")\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_clf, y_clf, test_size=0.3, \n",
    "                                                    random_state=42, stratify=y_clf)\n",
    "\n",
    "# Train classifier\n",
    "clf = LogisticRegression(max_iter=1000, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Create interactive confusion matrix\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "    z=cm,\n",
    "    x=['Predicted Negative', 'Predicted Positive'],\n",
    "    y=['Actual Negative', 'Actual Positive'],\n",
    "    text=cm,\n",
    "    texttemplate=\"%{text}\",\n",
    "    textfont={\"size\": 20},\n",
    "    colorscale='Blues',\n",
    "    showscale=True\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Confusion Matrix',\n",
    "    xaxis_title='Predicted Label',\n",
    "    yaxis_title='Actual Label',\n",
    "    width=500,\n",
    "    height=400\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Extract metrics from confusion matrix\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "print(f\"\\nüìä Confusion Matrix Components:\")\n",
    "print(f\"True Negatives (TN): {tn}\")\n",
    "print(f\"False Positives (FP): {fp} (Type I Error)\")\n",
    "print(f\"False Negatives (FN): {fn} (Type II Error)\")\n",
    "print(f\"True Positives (TP): {tp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ee8387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate all classification metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "# Create metrics summary\n",
    "metrics_data = {\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall (Sensitivity)', 'Specificity', 'F1-Score'],\n",
    "    'Formula': [\n",
    "        '(TP + TN) / Total',\n",
    "        'TP / (TP + FP)',\n",
    "        'TP / (TP + FN)', \n",
    "        'TN / (TN + FP)',\n",
    "        '2 √ó (Precision √ó Recall) / (Precision + Recall)'\n",
    "    ],\n",
    "    'Value': [accuracy, precision, recall, specificity, f1],\n",
    "    'Interpretation': [\n",
    "        'Overall correctness',\n",
    "        'When we predict positive, how often are we right?',\n",
    "        'Of all actual positives, how many did we find?',\n",
    "        'Of all actual negatives, how many did we correctly identify?',\n",
    "        'Harmonic mean of Precision and Recall'\n",
    "    ]\n",
    "}\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "metrics_df['Value'] = metrics_df['Value'].round(4)\n",
    "\n",
    "# Display with styling\n",
    "styled = metrics_df.style.bar(subset=['Value'], color='lightgreen', vmin=0, vmax=1)\n",
    "print(\"\\nüìä Classification Metrics Summary:\")\n",
    "styled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c1838e",
   "metadata": {},
   "source": [
    "### Exercise 6: ROC Curve and AUC\n",
    "#### üí° Concept\n",
    "ROC (Receiver Operating Characteristic) curve plots:\n",
    "- **True Positive Rate (Sensitivity)** vs **False Positive Rate (1-Specificity)**\n",
    "- AUC (Area Under Curve) summarizes performance: 0.5 = random, 1.0 = perfect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef68bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 6: ROC Curve and AUC Implementation\n",
    "\n",
    "# Train multiple classifiers for comparison\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "classifiers = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'SVM': SVC(probability=True, random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "}\n",
    "\n",
    "# Calculate ROC curves\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Get probability predictions\n",
    "    if hasattr(clf, 'predict_proba'):\n",
    "        y_proba = clf.predict_proba(X_test)[:, 1]\n",
    "    else:\n",
    "        y_proba = clf.decision_function(X_test)\n",
    "    \n",
    "    # Calculate ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "    auc_score = auc(fpr, tpr)\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=f'{name} (AUC = {auc_score:.3f})')\n",
    "\n",
    "# Plot random classifier\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Classifier (AUC = 0.500)')\n",
    "\n",
    "# Formatting\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)', fontsize=12)\n",
    "plt.ylabel('True Positive Rate (Sensitivity)', fontsize=12)\n",
    "plt.title('ROC Curves Comparison', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower right', fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1.05])\n",
    "\n",
    "# Add shaded area for best model\n",
    "best_clf = LogisticRegression(max_iter=1000, random_state=42)\n",
    "best_clf.fit(X_train, y_train)\n",
    "y_proba_best = best_clf.predict_proba(X_test)[:, 1]\n",
    "fpr_best, tpr_best, _ = roc_curve(y_test, y_proba_best)\n",
    "plt.fill_between(fpr_best, 0, tpr_best, alpha=0.1, color='blue')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä AUC Interpretation Guide:\")\n",
    "print(\"‚Ä¢ 0.90 - 1.00 = Excellent\")\n",
    "print(\"‚Ä¢ 0.80 - 0.90 = Good\")\n",
    "print(\"‚Ä¢ 0.70 - 0.80 = Fair\")\n",
    "print(\"‚Ä¢ 0.60 - 0.70 = Poor\")\n",
    "print(\"‚Ä¢ 0.50 - 0.60 = Fail\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df88316f",
   "metadata": {},
   "source": [
    "### Exercise 7: Precision-Recall Curve\n",
    "#### üí° Concept\n",
    "Precision-Recall curve is especially useful for imbalanced datasets:\n",
    "- Shows trade-off between Precision and Recall\n",
    "- More informative than ROC for imbalanced classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84ba9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 7: Precision-Recall Curve Analysis\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Plot 1: Precision-Recall Curves\n",
    "for name, clf in classifiers.items():\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_proba = clf.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, y_proba)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    \n",
    "    axes[0].plot(recall, precision, linewidth=2, label=f'{name} (AUC = {pr_auc:.3f})')\n",
    "\n",
    "# Baseline (random classifier)\n",
    "baseline = sum(y_test) / len(y_test)\n",
    "axes[0].axhline(y=baseline, color='k', linestyle='--', linewidth=1, \n",
    "                label=f'Baseline (y = {baseline:.3f})')\n",
    "\n",
    "axes[0].set_xlabel('Recall', fontsize=12)\n",
    "axes[0].set_ylabel('Precision', fontsize=12)\n",
    "axes[0].set_title('Precision-Recall Curves', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(loc='lower left', fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_xlim([0, 1])\n",
    "axes[0].set_ylim([0, 1.05])\n",
    "\n",
    "# Plot 2: Threshold Analysis\n",
    "clf = LogisticRegression(max_iter=1000, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "y_proba = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "thresholds = np.linspace(0, 1, 100)\n",
    "metrics_by_threshold = []\n",
    "\n",
    "for thresh in thresholds:\n",
    "    y_pred_thresh = (y_proba >= thresh).astype(int)\n",
    "    if len(np.unique(y_pred_thresh)) > 1 and len(np.unique(y_test)) > 1:\n",
    "        prec = precision_score(y_test, y_pred_thresh, zero_division=0)\n",
    "        rec = recall_score(y_test, y_pred_thresh, zero_division=0)\n",
    "        f1_thresh = f1_score(y_test, y_pred_thresh, zero_division=0)\n",
    "        metrics_by_threshold.append({'threshold': thresh, 'precision': prec, \n",
    "                                    'recall': rec, 'f1': f1_thresh})\n",
    "\n",
    "metrics_thresh_df = pd.DataFrame(metrics_by_threshold)\n",
    "\n",
    "axes[1].plot(metrics_thresh_df['threshold'], metrics_thresh_df['precision'], \n",
    "            label='Precision', linewidth=2)\n",
    "axes[1].plot(metrics_thresh_df['threshold'], metrics_thresh_df['recall'], \n",
    "            label='Recall', linewidth=2)\n",
    "axes[1].plot(metrics_thresh_df['threshold'], metrics_thresh_df['f1'], \n",
    "            label='F1-Score', linewidth=2, linestyle='--')\n",
    "\n",
    "axes[1].set_xlabel('Decision Threshold', fontsize=12)\n",
    "axes[1].set_ylabel('Metric Value', fontsize=12)\n",
    "axes[1].set_title('Metrics vs Decision Threshold', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(loc='best', fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_xlim([0, 1])\n",
    "axes[1].set_ylim([0, 1.05])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find optimal threshold based on F1-score\n",
    "optimal_idx = metrics_thresh_df['f1'].idxmax()\n",
    "optimal_threshold = metrics_thresh_df.loc[optimal_idx, 'threshold']\n",
    "\n",
    "print(f\"\\nüéØ Optimal Decision Threshold (based on F1-Score): {optimal_threshold:.3f}\")\n",
    "print(f\"   ‚Ä¢ Precision at optimal: {metrics_thresh_df.loc[optimal_idx, 'precision']:.3f}\")\n",
    "print(f\"   ‚Ä¢ Recall at optimal: {metrics_thresh_df.loc[optimal_idx, 'recall']:.3f}\")\n",
    "print(f\"   ‚Ä¢ F1-Score at optimal: {metrics_thresh_df.loc[optimal_idx, 'f1']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdb0d3f",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Cross-Validation and Model Selection\n",
    "\n",
    "### Exercise 8: K-Fold Cross-Validation\n",
    "#### üí° Concept\n",
    "K-Fold CV divides data into K equal folds:\n",
    "- Each fold serves as validation once\n",
    "- Provides K performance estimates\n",
    "- More reliable than single train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f75ec5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 8: K-Fold Cross-Validation Implementation\n",
    "\n",
    "# Prepare data\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "# Implement K-Fold CV manually to show the process\n",
    "k_folds = 5\n",
    "kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'SVM': SVC(random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "}\n",
    "\n",
    "# Store results\n",
    "cv_results = {name: {'scores': [], 'mean': 0, 'std': 0} for name in models.keys()}\n",
    "\n",
    "# Perform K-Fold CV for each model\n",
    "print(\"üîÑ Performing K-Fold Cross-Validation...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nEvaluating: {name}\")\n",
    "    fold_scores = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X), 1):\n",
    "        # Split data\n",
    "        X_train_fold, X_val_fold = X[train_idx], X[val_idx]\n",
    "        y_train_fold, y_val_fold = y[train_idx], y[val_idx]\n",
    "        \n",
    "        # Train and evaluate\n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "        score = model.score(X_val_fold, y_val_fold)\n",
    "        fold_scores.append(score)\n",
    "        print(f\"  Fold {fold}: {score:.4f}\")\n",
    "    \n",
    "    # Calculate statistics\n",
    "    cv_results[name]['scores'] = fold_scores\n",
    "    cv_results[name]['mean'] = np.mean(fold_scores)\n",
    "    cv_results[name]['std'] = np.std(fold_scores)\n",
    "    \n",
    "    print(f\"  Mean: {cv_results[name]['mean']:.4f} (¬±{cv_results[name]['std']:.4f})\")\n",
    "\n",
    "# Create visualization of results\n",
    "fig = go.Figure()\n",
    "\n",
    "for name, results in cv_results.items():\n",
    "    # Add box plot for each model\n",
    "    fig.add_trace(go.Box(\n",
    "        y=results['scores'],\n",
    "        name=name,\n",
    "        boxmean='sd',  # show mean and standard deviation\n",
    "        marker_color=np.random.rand(3,)\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f'{k_folds}-Fold Cross-Validation Results',\n",
    "    yaxis_title='Accuracy Score',\n",
    "    xaxis_title='Model',\n",
    "    height=500,\n",
    "    showlegend=False,\n",
    "    yaxis=dict(range=[0.85, 1.0])\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Summary table\n",
    "summary_df = pd.DataFrame({\n",
    "    'Model': list(cv_results.keys()),\n",
    "    'Mean Accuracy': [cv_results[name]['mean'] for name in cv_results.keys()],\n",
    "    'Std Dev': [cv_results[name]['std'] for name in cv_results.keys()],\n",
    "    'CV Score': [f\"{cv_results[name]['mean']:.4f} (¬±{cv_results[name]['std']:.4f})\" \n",
    "                 for name in cv_results.keys()]\n",
    "})\n",
    "summary_df = summary_df.sort_values('Mean Accuracy', ascending=False)\n",
    "summary_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(\"\\nüìä Cross-Validation Summary:\")\n",
    "display(summary_df.style.background_gradient(subset=['Mean Accuracy'], cmap='Greens'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ece5a7",
   "metadata": {},
   "source": [
    "### Exercise 9: Stratified K-Fold for Imbalanced Data\n",
    "#### üí° Concept\n",
    "Stratified K-Fold maintains class distribution in each fold:\n",
    "- Essential for imbalanced datasets\n",
    "- Ensures representative validation sets\n",
    "- Reduces variance in performance estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795329a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 9: Comparing Regular vs Stratified K-Fold\n",
    "\n",
    "# Create highly imbalanced dataset\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X_imb, y_imb = make_classification(n_samples=1000, n_features=20, n_informative=15,\n",
    "                                   n_redundant=5, n_classes=2, weights=[0.9, 0.1],\n",
    "                                   flip_y=0.01, random_state=42)\n",
    "\n",
    "print(f\"Dataset class distribution:\")\n",
    "print(f\"Class 0: {sum(y_imb == 0)} samples ({sum(y_imb == 0)/len(y_imb)*100:.1f}%)\")\n",
    "print(f\"Class 1: {sum(y_imb == 1)} samples ({sum(y_imb == 1)/len(y_imb)*100:.1f}%)\")\n",
    "print(f\"Imbalance ratio: {sum(y_imb == 0)/sum(y_imb == 1):.1f}:1\")\n",
    "\n",
    "# Compare regular K-Fold vs Stratified K-Fold\n",
    "n_splits = 5\n",
    "kf_regular = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "kf_stratified = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Function to analyze fold distributions\n",
    "def analyze_folds(cv_method, X, y, cv_name):\n",
    "    fold_distributions = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(cv_method.split(X, y), 1):\n",
    "        y_val_fold = y[val_idx]\n",
    "        class_1_ratio = sum(y_val_fold == 1) / len(y_val_fold)\n",
    "        fold_distributions.append({\n",
    "            'Fold': fold,\n",
    "            'Class 0': sum(y_val_fold == 0),\n",
    "            'Class 1': sum(y_val_fold == 1),\n",
    "            'Class 1 Ratio': class_1_ratio * 100\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(fold_distributions)\n",
    "    df['Variance'] = df['Class 1 Ratio'].std()\n",
    "    return df\n",
    "\n",
    "# Analyze both methods\n",
    "regular_folds = analyze_folds(kf_regular, X_imb, y_imb, \"Regular K-Fold\")\n",
    "stratified_folds = analyze_folds(kf_stratified, X_imb, y_imb, \"Stratified K-Fold\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Regular K-Fold\n",
    "axes[0].bar(regular_folds['Fold'], regular_folds['Class 1 Ratio'], \n",
    "           color='coral', edgecolor='black', alpha=0.7)\n",
    "axes[0].axhline(y=10, color='green', linestyle='--', label='True Ratio (10%)')\n",
    "axes[0].set_xlabel('Fold Number')\n",
    "axes[0].set_ylabel('Class 1 Percentage (%)')\n",
    "axes[0].set_title(f'Regular K-Fold\\n(Std Dev: {regular_folds[\"Class 1 Ratio\"].std():.2f}%)')\n",
    "axes[0].legend()\n",
    "axes[0].set_ylim([0, 20])\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Stratified K-Fold\n",
    "axes[1].bar(stratified_folds['Fold'], stratified_folds['Class 1 Ratio'],\n",
    "           color='lightgreen', edgecolor='black', alpha=0.7)\n",
    "axes[1].axhline(y=10, color='green', linestyle='--', label='True Ratio (10%)')\n",
    "axes[1].set_xlabel('Fold Number')\n",
    "axes[1].set_ylabel('Class 1 Percentage (%)')\n",
    "axes[1].set_title(f'Stratified K-Fold\\n(Std Dev: {stratified_folds[\"Class 1 Ratio\"].std():.2f}%)')\n",
    "axes[1].legend()\n",
    "axes[1].set_ylim([0, 20])\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Class Distribution Across Folds: Regular vs Stratified K-Fold', \n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Fold Distribution Analysis:\")\n",
    "print(\"\\nRegular K-Fold:\")\n",
    "display(regular_folds)\n",
    "print(\"\\nStratified K-Fold:\")\n",
    "display(stratified_folds)\n",
    "\n",
    "print(\"\\nüí° Key Insight: Stratified K-Fold maintains consistent class distribution across all folds!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbda55ad",
   "metadata": {},
   "source": [
    "### Exercise 10: Hyperparameter Tuning with Grid Search\n",
    "#### üí° Concept\n",
    "Grid Search systematically explores hyperparameter combinations:\n",
    "- Exhaustive search through parameter grid\n",
    "- Uses cross-validation for each combination\n",
    "- Returns best parameters based on validation score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1b1c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 10: Hyperparameter Tuning Implementation\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "# Prepare data\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define parameter grids for different models\n",
    "param_grids = {\n",
    "    'Logistic Regression': {\n",
    "        'model': LogisticRegression(max_iter=1000, random_state=42),\n",
    "        'params': {\n",
    "            'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "            'penalty': ['l1', 'l2'],\n",
    "            'solver': ['liblinear']\n",
    "        }\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'model': RandomForestClassifier(random_state=42),\n",
    "        'params': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_depth': [None, 10, 20, 30],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4]\n",
    "        }\n",
    "    },\n",
    "    'SVM': {\n",
    "        'model': SVC(random_state=42),\n",
    "        'params': {\n",
    "            'C': [0.1, 1, 10],\n",
    "            'kernel': ['linear', 'rbf', 'poly'],\n",
    "            'gamma': ['scale', 'auto', 0.001, 0.01]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Perform Grid Search for each model\n",
    "print(\"üîç Performing Grid Search for Hyperparameter Tuning...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "best_models = {}\n",
    "results_summary = []\n",
    "\n",
    "for name, config in param_grids.items():\n",
    "    print(f\"\\nTuning {name}...\")\n",
    "    \n",
    "    # Create GridSearchCV object\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=config['model'],\n",
    "        param_grid=config['params'],\n",
    "        cv=5,\n",
    "        scoring='f1',\n",
    "        n_jobs=-1,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Fit grid search\n",
    "    grid_search.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Store best model\n",
    "    best_models[name] = grid_search.best_estimator_\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    y_pred = grid_search.predict(X_test_scaled)\n",
    "    test_f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    # Store results\n",
    "    results_summary.append({\n",
    "        'Model': name,\n",
    "        'Best Params': str(grid_search.best_params_),\n",
    "        'CV F1 Score': grid_search.best_score_,\n",
    "        'Test F1 Score': test_f1,\n",
    "        'Total Combinations': len(grid_search.cv_results_['params'])\n",
    "    })\n",
    "    \n",
    "    print(f\"  Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"  Best CV F1 score: {grid_search.best_score_:.4f}\")\n",
    "    print(f\"  Test F1 score: {test_f1:.4f}\")\n",
    "    print(f\"  Total combinations tested: {len(grid_search.cv_results_['params'])}\")\n",
    "\n",
    "# Display results summary\n",
    "results_df = pd.DataFrame(results_summary)\n",
    "results_df = results_df.sort_values('Test F1 Score', ascending=False)\n",
    "\n",
    "print(\"\\nüìä Grid Search Results Summary:\")\n",
    "display(results_df.style.background_gradient(subset=['CV F1 Score', 'Test F1 Score'], cmap='Greens'))\n",
    "\n",
    "# Visualize hyperparameter importance (example with Random Forest)\n",
    "rf_model = param_grids['Random Forest']['model']\n",
    "rf_grid = GridSearchCV(\n",
    "    estimator=rf_model,\n",
    "    param_grid={'n_estimators': [50, 100, 150, 200],\n",
    "                'max_depth': [5, 10, 20, 30, None]},\n",
    "    cv=5,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Create heatmap of results\n",
    "scores = rf_grid.cv_results_['mean_test_score']\n",
    "scores_array = scores.reshape(5, 4)\n",
    "\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "    z=scores_array,\n",
    "    x=[50, 100, 150, 200],\n",
    "    y=[5, 10, 20, 30, 'None'],\n",
    "    text=scores_array.round(3),\n",
    "    texttemplate='%{text}',\n",
    "    colorscale='Viridis',\n",
    "    colorbar_title='F1 Score'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Random Forest: Hyperparameter Grid Search Results',\n",
    "    xaxis_title='Number of Estimators',\n",
    "    yaxis_title='Max Depth',\n",
    "    width=600,\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nüí° Key Insights:\")\n",
    "print(\"‚Ä¢ Grid Search tests all combinations exhaustively\")\n",
    "print(\"‚Ä¢ Can be computationally expensive for large parameter spaces\")\n",
    "print(\"‚Ä¢ Consider RandomizedSearchCV for faster exploration\")\n",
    "print(\"‚Ä¢ Always validate final model on held-out test set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b24782c",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Summary and Final Exercise\n",
    "\n",
    "### üéØ Complete Model Evaluation Pipeline\n",
    "Let's put everything together in a comprehensive evaluation pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f77aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Exercise: Complete Model Evaluation Pipeline\n",
    "\n",
    "class ModelEvaluationPipeline:\n",
    "    \"\"\"Complete pipeline for model evaluation\"\"\"\n",
    "    \n",
    "    def __init__(self, models, scoring='accuracy'):\n",
    "        self.models = models\n",
    "        self.scoring = scoring\n",
    "        self.results = {}\n",
    "        \n",
    "    def evaluate_models(self, X, y, test_size=0.3, cv_folds=5):\n",
    "        \"\"\"Evaluate multiple models with comprehensive metrics\"\"\"\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        # Scale features\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        print(\"üî¨ Model Evaluation Pipeline Started\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        for name, model in self.models.items():\n",
    "            print(f\"\\nEvaluating: {name}\")\n",
    "            \n",
    "            # Cross-validation\n",
    "            cv_scores = cross_val_score(model, X_train_scaled, y_train, \n",
    "                                       cv=cv_folds, scoring=self.scoring)\n",
    "            \n",
    "            # Train on full training set\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            \n",
    "            # Predictions\n",
    "            y_pred = model.predict(X_test_scaled)\n",
    "            y_proba = model.predict_proba(X_test_scaled)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "            \n",
    "            # Calculate metrics\n",
    "            metrics = {\n",
    "                'CV Score Mean': cv_scores.mean(),\n",
    "                'CV Score Std': cv_scores.std(),\n",
    "                'Test Accuracy': accuracy_score(y_test, y_pred),\n",
    "                'Test Precision': precision_score(y_test, y_pred),\n",
    "                'Test Recall': recall_score(y_test, y_pred),\n",
    "                'Test F1': f1_score(y_test, y_pred),\n",
    "                'Test AUC': roc_auc_score(y_test, y_proba) if y_proba is not None else None\n",
    "            }\n",
    "            \n",
    "            self.results[name] = {\n",
    "                'model': model,\n",
    "                'metrics': metrics,\n",
    "                'predictions': y_pred,\n",
    "                'probabilities': y_proba,\n",
    "                'confusion_matrix': confusion_matrix(y_test, y_pred)\n",
    "            }\n",
    "            \n",
    "            print(f\"  CV Score: {metrics['CV Score Mean']:.4f} (¬±{metrics['CV Score Std']:.4f})\")\n",
    "            print(f\"  Test F1: {metrics['Test F1']:.4f}\")\n",
    "            print(f\"  Test AUC: {metrics['Test AUC']:.4f}\" if metrics['Test AUC'] else \"  Test AUC: N/A\")\n",
    "        \n",
    "        return self.results\n",
    "    \n",
    "    def plot_comparison(self):\n",
    "        \"\"\"Create comprehensive comparison visualizations\"\"\"\n",
    "        \n",
    "        # Prepare data for plotting\n",
    "        models_list = []\n",
    "        metrics_dict = {\n",
    "            'Accuracy': [], 'Precision': [], 'Recall': [], 'F1': [], 'AUC': []\n",
    "        }\n",
    "        \n",
    "        for name, result in self.results.items():\n",
    "            models_list.append(name)\n",
    "            metrics_dict['Accuracy'].append(result['metrics']['Test Accuracy'])\n",
    "            metrics_dict['Precision'].append(result['metrics']['Test Precision'])\n",
    "            metrics_dict['Recall'].append(result['metrics']['Test Recall'])\n",
    "            metrics_dict['F1'].append(result['metrics']['Test F1'])\n",
    "            metrics_dict['AUC'].append(result['metrics']['Test AUC'] or 0)\n",
    "        \n",
    "        # Create subplots\n",
    "        fig = make_subplots(\n",
    "            rows=2, cols=3,\n",
    "            subplot_titles=['Accuracy', 'Precision', 'Recall', 'F1 Score', 'AUC Score', 'Overall Comparison'],\n",
    "            specs=[[{'type': 'bar'}, {'type': 'bar'}, {'type': 'bar'}],\n",
    "                   [{'type': 'bar'}, {'type': 'bar'}, {'type': 'scatter'}]]\n",
    "        )\n",
    "        \n",
    "        # Individual metrics\n",
    "        colors = ['#1E64C8', '#4A90E2', '#7BB3F0', '#9FC5E8']\n",
    "        \n",
    "        for idx, (metric, values) in enumerate(metrics_dict.items()):\n",
    "            if idx < 5:\n",
    "                row = idx // 3 + 1\n",
    "                col = idx % 3 + 1\n",
    "                fig.add_trace(\n",
    "                    go.Bar(x=models_list, y=values, marker_color=colors, showlegend=False),\n",
    "                    row=row, col=col\n",
    "                )\n",
    "        \n",
    "        # Overall comparison (radar chart simulation)\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=['Accuracy', 'Precision', 'Recall', 'F1', 'AUC'] * len(models_list),\n",
    "                      y=[val for metric_vals in metrics_dict.values() for val in metric_vals],\n",
    "                      mode='markers+lines',\n",
    "                      marker=dict(size=10),\n",
    "                      showlegend=False),\n",
    "            row=2, col=3\n",
    "        )\n",
    "        \n",
    "        fig.update_layout(height=600, title_text=\"Model Performance Comparison Dashboard\")\n",
    "        fig.show()\n",
    "        \n",
    "        return fig\n",
    "    \n",
    "    def generate_report(self):\n",
    "        \"\"\"Generate comprehensive evaluation report\"\"\"\n",
    "        \n",
    "        report = []\n",
    "        report.append(\"\\n\" + \"=\" * 60)\n",
    "        report.append(\"üìä MODEL EVALUATION REPORT\")\n",
    "        report.append(\"=\" * 60)\n",
    "        \n",
    "        # Find best model\n",
    "        best_model = max(self.results.items(), \n",
    "                        key=lambda x: x[1]['metrics']['Test F1'])\n",
    "        \n",
    "        report.append(f\"\\nüèÜ Best Model: {best_model[0]}\")\n",
    "        report.append(f\"   Test F1 Score: {best_model[1]['metrics']['Test F1']:.4f}\")\n",
    "        \n",
    "        # Detailed metrics table\n",
    "        metrics_data = []\n",
    "        for name, result in self.results.items():\n",
    "            metrics_data.append({\n",
    "                'Model': name,\n",
    "                'CV Score': f\"{result['metrics']['CV Score Mean']:.4f} (¬±{result['metrics']['CV Score Std']:.4f})\",\n",
    "                'Accuracy': f\"{result['metrics']['Test Accuracy']:.4f}\",\n",
    "                'Precision': f\"{result['metrics']['Test Precision']:.4f}\",\n",
    "                'Recall': f\"{result['metrics']['Test Recall']:.4f}\",\n",
    "                'F1': f\"{result['metrics']['Test F1']:.4f}\",\n",
    "                'AUC': f\"{result['metrics']['Test AUC']:.4f}\" if result['metrics']['Test AUC'] else 'N/A'\n",
    "            })\n",
    "        \n",
    "        report_df = pd.DataFrame(metrics_data)\n",
    "        \n",
    "        print(\"\\n\".join(report))\n",
    "        print(\"\\nüìà Detailed Metrics:\")\n",
    "        display(report_df.style.highlight_max(subset=['Accuracy', 'Precision', 'Recall', 'F1', 'AUC']))\n",
    "        \n",
    "        return report_df\n",
    "\n",
    "# Initialize and run pipeline\n",
    "models_to_evaluate = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'SVM (RBF)': SVC(kernel='rbf', probability=True, random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "}\n",
    "\n",
    "# Load data\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "# Run pipeline\n",
    "pipeline = ModelEvaluationPipeline(models_to_evaluate, scoring='f1')\n",
    "results = pipeline.evaluate_models(X, y)\n",
    "\n",
    "# Generate visualizations\n",
    "pipeline.plot_comparison()\n",
    "\n",
    "# Generate report\n",
    "report = pipeline.generate_report()\n",
    "\n",
    "print(\"\\n‚úÖ Pipeline Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37c93ca",
   "metadata": {},
   "source": [
    "---\n",
    "## üìù Practice Exercises\n",
    "\n",
    "### Your Turn: Challenge Problems\n",
    "\n",
    "1. **Imbalanced Classification**: Create a highly imbalanced dataset (95:5 ratio) and compare different evaluation metrics\n",
    "2. **Time Series Splitting**: Implement time-based cross-validation for temporal data\n",
    "3. **Multi-class Classification**: Extend the pipeline to handle 3+ classes with macro/micro/weighted averaging\n",
    "4. **Custom Metrics**: Create a custom scoring function that penalizes false negatives 3x more than false positives\n",
    "5. **Ensemble Evaluation**: Combine predictions from multiple models and evaluate the ensemble\n",
    "\n",
    "### üéØ Key Takeaways\n",
    "- Always use proper train/validation/test splits\n",
    "- Choose metrics appropriate for your problem and data\n",
    "- Prevent data leakage by splitting before preprocessing\n",
    "- Use cross-validation for robust performance estimates\n",
    "- Consider class imbalance when selecting metrics\n",
    "- Hyperparameter tuning should use validation set, never test set\n",
    "- Multiple metrics provide different perspectives on performance"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
