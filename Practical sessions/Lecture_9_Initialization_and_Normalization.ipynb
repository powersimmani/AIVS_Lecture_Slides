{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deep Learning: Initialization and Normalization Techniques\n",
        "## Interactive Learning Notebook\n\n",
        "**Author**: Based on Lecture 9 by Ho-min Park\n\n",
        "**Contents**:\n",
        "1. **Part 1**: Setup and Fundamentals\n",
        "2. **Part 2**: Weight Initialization Strategies\n",
        "3. **Part 3**: Normalization Techniques\n",
        "4. **Part 4**: Regularization and Generalization\n",
        "5. **Part 5**: Advanced Techniques and Summary\n\n",
        "---\n\n",
        "### Learning Objectives\n",
        "- Understand the importance of proper weight initialization\n",
        "- Implement various initialization strategies (Xavier, He, LSUV)\n",
        "- Master normalization techniques (Batch, Layer, Group Norm)\n",
        "- Apply regularization methods (Dropout, Data Augmentation)\n",
        "- Analyze the impact on training stability and convergence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Setup and Fundamentals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Essential imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# Deep Learning imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Sklearn for datasets and metrics\n",
        "from sklearn.datasets import make_classification, make_regression, load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "# Warnings and display settings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Display settings\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette('husl')\n",
        "\n",
        "print('‚úÖ All libraries imported successfully!')\n",
        "print(f'PyTorch version: {torch.__version__}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Exercise 1: Visualizing Gradient Vanishing/Exploding Problem\n\n",
        "### üìö Concept\n",
        "When weights are initialized poorly, gradients can either vanish (become too small) or explode (become too large) during backpropagation. This happens due to repeated multiplication of weights during the backward pass.\n\n",
        "- **Vanishing gradients**: Weights initialized too small ‚Üí gradients shrink exponentially\n",
        "- **Exploding gradients**: Weights initialized too large ‚Üí gradients grow exponentially"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def simulate_gradient_flow(init_scale, num_layers=10, activation='tanh'):\n",
        "    \"\"\"Simulate gradient flow through a deep network with different initializations\"\"\"\n",
        "    np.random.seed(42)\n",
        "    \n",
        "    # Initialize weights and gradients\n",
        "    weights = []\n",
        "    gradients = []\n",
        "    layer_outputs = []\n",
        "    \n",
        "    # Input\n",
        "    x = np.random.randn(100, 10)  # 100 samples, 10 features\n",
        "    layer_outputs.append(x)\n",
        "    \n",
        "    # Forward pass\n",
        "    for i in range(num_layers):\n",
        "        w = np.random.randn(10, 10) * init_scale\n",
        "        weights.append(w)\n",
        "        \n",
        "        # Linear transformation\n",
        "        z = layer_outputs[-1] @ w\n",
        "        \n",
        "        # Apply activation\n",
        "        if activation == 'tanh':\n",
        "            a = np.tanh(z)\n",
        "        elif activation == 'relu':\n",
        "            a = np.maximum(0, z)\n",
        "        else:\n",
        "            a = z\n",
        "        \n",
        "        layer_outputs.append(a)\n",
        "    \n",
        "    # Simulate backward pass (simplified)\n",
        "    grad = np.ones_like(layer_outputs[-1])\n",
        "    gradient_norms = []\n",
        "    \n",
        "    for i in reversed(range(num_layers)):\n",
        "        # Gradient through activation\n",
        "        if activation == 'tanh':\n",
        "            grad = grad * (1 - layer_outputs[i+1]**2)\n",
        "        elif activation == 'relu':\n",
        "            grad = grad * (layer_outputs[i+1] > 0)\n",
        "        \n",
        "        # Gradient through linear layer\n",
        "        grad = grad @ weights[i].T\n",
        "        gradient_norms.append(np.linalg.norm(grad))\n",
        "    \n",
        "    return gradient_norms[::-1], [np.linalg.norm(out) for out in layer_outputs]\n",
        "\n",
        "# Test different initialization scales\n",
        "scales = [0.01, 0.1, 1.0, 2.0]\n",
        "fig = make_subplots(rows=2, cols=2, \n",
        "                    subplot_titles=[f'Init Scale = {s}' for s in scales],\n",
        "                    shared_yaxes=True)\n",
        "\n",
        "for idx, scale in enumerate(scales):\n",
        "    grad_norms, output_norms = simulate_gradient_flow(scale)\n",
        "    row = idx // 2 + 1\n",
        "    col = idx % 2 + 1\n",
        "    \n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=list(range(len(grad_norms))), y=grad_norms,\n",
        "                   mode='lines+markers', name=f'Gradients (scale={scale})'),\n",
        "        row=row, col=col\n",
        "    )\n",
        "    \n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=list(range(len(output_norms))), y=output_norms,\n",
        "                   mode='lines+markers', name=f'Outputs (scale={scale})',\n",
        "                   line=dict(dash='dash')),\n",
        "        row=row, col=col\n",
        "    )\n",
        "\n",
        "fig.update_layout(height=600, title_text=\"Gradient Flow with Different Initializations\",\n",
        "                  showlegend=False)\n",
        "fig.update_xaxes(title_text=\"Layer\")\n",
        "fig.update_yaxes(title_text=\"Norm\", type=\"log\")\n",
        "fig.show()\n",
        "\n",
        "print(\"üìä Observations:\")\n",
        "print(\"- Small init (0.01): Gradients vanish quickly\")\n",
        "print(\"- Large init (2.0): Gradients may explode\")\n",
        "print(\"- Moderate init (0.1-1.0): More stable gradient flow\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\\n",
        "## Exercise 2: Xavier/Glorot Initialization\\n\\n",
        "### üìö Concept\\n",
        "Xavier initialization maintains variance across layers by scaling weights based on the number of input and output neurons:\\n",
        "- **Uniform**: $W \\sim U(-\\sqrt{\\frac{6}{n_{in} + n_{out}}}, \\sqrt{\\frac{6}{n_{in} + n_{out}}})$\\n",
        "- **Normal**: $W \\sim N(0, \\frac{2}{n_{in} + n_{out}})$\\n\\n",
        "Designed for **sigmoid and tanh** activations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class XavierInitNetwork(nn.Module):\\n",
        "    def __init__(self, input_size, hidden_sizes, output_size, init_type='xavier'):\\n",
        "        super(XavierInitNetwork, self).__init__()\\n",
        "        \\n",
        "        layers = []\\n",
        "        prev_size = input_size\\n",
        "        \\n",
        "        for hidden_size in hidden_sizes:\\n",
        "            layer = nn.Linear(prev_size, hidden_size)\\n",
        "            \\n",
        "            # Apply initialization\\n",
        "            if init_type == 'xavier':\\n",
        "                nn.init.xavier_uniform_(layer.weight)\\n",
        "            elif init_type == 'xavier_normal':\\n",
        "                nn.init.xavier_normal_(layer.weight)\\n",
        "            elif init_type == 'zeros':\\n",
        "                nn.init.zeros_(layer.weight)\\n",
        "            elif init_type == 'random':\\n",
        "                nn.init.uniform_(layer.weight, -0.1, 0.1)\\n",
        "            \\n",
        "            layers.append(layer)\\n",
        "            layers.append(nn.Tanh())  # Using tanh for Xavier\\n",
        "            prev_size = hidden_size\\n",
        "        \\n",
        "        # Output layer\\n",
        "        layers.append(nn.Linear(prev_size, output_size))\\n",
        "        self.network = nn.Sequential(*layers)\\n",
        "    \\n",
        "    def forward(self, x):\\n",
        "        return self.network(x)\\n\\n",
        "# Test initialization methods\\n",
        "print('Xavier initialization implemented!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\\n",
        "## Exercise 3: He Initialization for ReLU Networks\\n\\n",
        "### üìö Concept\\n",
        "He initialization accounts for ReLU killing approximately half the neurons:\\n",
        "- **Formula**: $W \\sim N(0, \\frac{2}{n_{in}})$\\n",
        "- **Key insight**: Variance factor of 2 compensates for ReLU's effect\\n",
        "- **Best for**: Networks with ReLU, LeakyReLU, PReLU activations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_activation_distributions():\\n",
        "    '''Visualize how different initializations affect activation distributions'''\\n",
        "    torch.manual_seed(42)\\n",
        "    \\n",
        "    # Create input\\n",
        "    x = torch.randn(1000, 100)\\n",
        "    \\n",
        "    # Test He vs Xavier with ReLU\\n",
        "    init_methods = {\\n",
        "        'He': lambda w: nn.init.kaiming_normal_(w, nonlinearity='relu'),\\n",
        "        'Xavier': lambda w: nn.init.xavier_normal_(w),\\n",
        "        'Small (0.01)': lambda w: nn.init.normal_(w, std=0.01),\\n",
        "        'Large (1.0)': lambda w: nn.init.normal_(w, std=1.0)\\n",
        "    }\\n",
        "    \\n",
        "    fig, axes = plt.subplots(1, 4, figsize=(16, 4))\\n",
        "    \\n",
        "    for idx, (name, init_fn) in enumerate(init_methods.items()):\\n",
        "        # Create layer\\n",
        "        layer = nn.Linear(100, 100)\\n",
        "        init_fn(layer.weight)\\n",
        "        nn.init.zeros_(layer.bias)\\n",
        "        \\n",
        "        # Forward pass\\n",
        "        with torch.no_grad():\\n",
        "            output = F.relu(layer(x))\\n",
        "        \\n",
        "        # Plot distribution\\n",
        "        axes[idx].hist(output.numpy().flatten(), bins=50, alpha=0.7, edgecolor='black')\\n",
        "        axes[idx].set_title(f'{name} Initialization')\\n",
        "        axes[idx].set_xlabel('Activation Value')\\n",
        "        axes[idx].set_ylabel('Frequency')\\n",
        "        axes[idx].axvline(x=0, color='red', linestyle='--', alpha=0.5)\\n",
        "        \\n",
        "        # Add statistics\\n",
        "        mean_val = output.mean().item()\\n",
        "        std_val = output.std().item()\\n",
        "        axes[idx].text(0.95, 0.95, f'Œº={mean_val:.2f}\\\\nœÉ={std_val:.2f}',\\n",
        "                      transform=axes[idx].transAxes, ha='right', va='top',\\n",
        "                      bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\\n",
        "    \\n",
        "    plt.tight_layout()\\n",
        "    plt.show()\\n",
        "    print('He initialization maintains better distribution with ReLU!')\\n\\n",
        "visualize_activation_distributions()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\\n",
        "## Exercise 4: Batch Normalization Implementation\\n\\n",
        "### üìö Concept\\n",
        "Batch Normalization normalizes inputs across the batch dimension:\\n",
        "- **Training**: Uses batch statistics (mean, variance)\\n",
        "- **Inference**: Uses running statistics\\n",
        "- **Formula**: $\\hat{x} = \\frac{x - \\mu_{batch}}{\\sqrt{\\sigma^2_{batch} + \\epsilon}}$, $y = \\gamma \\cdot \\hat{x} + \\beta$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CustomBatchNorm(nn.Module):\\n",
        "    '''Custom Batch Normalization implementation for educational purposes'''\\n",
        "    \\n",
        "    def __init__(self, num_features, eps=1e-5, momentum=0.1):\\n",
        "        super(CustomBatchNorm, self).__init__()\\n",
        "        \\n",
        "        # Learnable parameters\\n",
        "        self.gamma = nn.Parameter(torch.ones(num_features))\\n",
        "        self.beta = nn.Parameter(torch.zeros(num_features))\\n",
        "        \\n",
        "        # Running statistics (not learnable)\\n",
        "        self.register_buffer('running_mean', torch.zeros(num_features))\\n",
        "        self.register_buffer('running_var', torch.ones(num_features))\\n",
        "        \\n",
        "        self.eps = eps\\n",
        "        self.momentum = momentum\\n",
        "    \\n",
        "    def forward(self, x):\\n",
        "        if self.training:\\n",
        "            # Calculate batch statistics\\n",
        "            batch_mean = x.mean(dim=0)\\n",
        "            batch_var = x.var(dim=0, unbiased=False)\\n",
        "            \\n",
        "            # Update running statistics\\n",
        "            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * batch_mean\\n",
        "            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * batch_var\\n",
        "            \\n",
        "            # Normalize\\n",
        "            x_normalized = (x - batch_mean) / torch.sqrt(batch_var + self.eps)\\n",
        "        else:\\n",
        "            # Use running statistics during inference\\n",
        "            x_normalized = (x - self.running_mean) / torch.sqrt(self.running_var + self.eps)\\n",
        "        \\n",
        "        # Scale and shift\\n",
        "        return self.gamma * x_normalized + self.beta\\n\\n",
        "print('Custom BatchNorm implementation complete!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\\n",
        "## Exercise 5: Comparing Normalization Techniques\\n\\n",
        "### üìö Decision Guide\\n",
        "- **Batch Norm**: Large batch CNNs\\n",
        "- **Layer Norm**: RNNs, Transformers, small batches\\n",
        "- **Instance Norm**: Style transfer, GANs\\n",
        "- **Group Norm**: Object detection, segmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compare_normalizations():\\n",
        "    '''Compare different normalization techniques'''\\n",
        "    \\n",
        "    # Create sample data\\n",
        "    batch_size, channels, height, width = 32, 16, 28, 28\\n",
        "    x = torch.randn(batch_size, channels, height, width)\\n",
        "    \\n",
        "    # Initialize normalization layers\\n",
        "    norms = {\\n",
        "        'BatchNorm': nn.BatchNorm2d(channels),\\n",
        "        'LayerNorm': nn.LayerNorm([channels, height, width]),\\n",
        "        'InstanceNorm': nn.InstanceNorm2d(channels),\\n",
        "        'GroupNorm': nn.GroupNorm(num_groups=4, num_channels=channels)\\n",
        "    }\\n",
        "    \\n",
        "    # Apply normalizations and collect statistics\\n",
        "    results = {}\\n",
        "    for name, norm_layer in norms.items():\\n",
        "        with torch.no_grad():\\n",
        "            output = norm_layer(x)\\n",
        "            results[name] = {\\n",
        "                'mean': output.mean().item(),\\n",
        "                'std': output.std().item(),\\n",
        "                'shape': output.shape\\n",
        "            }\\n",
        "    \\n",
        "    # Display results\\n",
        "    df = pd.DataFrame(results).T\\n",
        "    print(df)\\n",
        "    \\n",
        "    return results\\n\\n",
        "results = compare_normalizations()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\\n",
        "## Exercise 6: Implementing Dropout Variants\\n\\n",
        "### üìö Concept\\n",
        "Dropout randomly deactivates neurons during training to prevent overfitting:\\n",
        "- **Standard Dropout**: Drops individual neurons\\n",
        "- **DropConnect**: Drops connections (weights)\\n",
        "- **DropBlock**: Drops contiguous regions (for CNNs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DropoutVariants(nn.Module):\\n",
        "    '''Implementation of various dropout techniques'''\\n",
        "    \\n",
        "    def __init__(self, drop_prob=0.5):\\n",
        "        super(DropoutVariants, self).__init__()\\n",
        "        self.drop_prob = drop_prob\\n",
        "    \\n",
        "    def standard_dropout(self, x, training=True):\\n",
        "        '''Standard dropout - drops neurons'''\\n",
        "        if not training:\\n",
        "            return x\\n",
        "        \\n",
        "        mask = torch.bernoulli(torch.ones_like(x) * (1 - self.drop_prob))\\n",
        "        return x * mask / (1 - self.drop_prob)\\n",
        "    \\n",
        "    def dropconnect(self, weight, training=True):\\n",
        "        '''DropConnect - drops weights'''\\n",
        "        if not training:\\n",
        "            return weight\\n",
        "        \\n",
        "        mask = torch.bernoulli(torch.ones_like(weight) * (1 - self.drop_prob))\\n",
        "        return weight * mask\\n\\n",
        "# Test dropout\\n",
        "dropout = DropoutVariants(drop_prob=0.3)\\n",
        "x = torch.ones(1, 10)\\n",
        "x_dropped = dropout.standard_dropout(x)\\n",
        "print(f'Original: {x}')\\n",
        "print(f'After dropout: {x_dropped}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\\n",
        "## Exercise 7: Data Augmentation - Mixup and CutMix\\n\\n",
        "### üìö Concept\\n",
        "Advanced data augmentation techniques that create new training samples:\\n",
        "- **Mixup**: Linear interpolation of samples: $x_{mix} = \\lambda x_i + (1-\\lambda) x_j$\\n",
        "- **CutMix**: Cuts and pastes patches between samples\\n",
        "- **Benefits**: Smoother decision boundaries, better calibration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mixup_data(x, y, alpha=1.0):\\n",
        "    '''Performs mixup augmentation'''\\n",
        "    if alpha > 0:\\n",
        "        lam = np.random.beta(alpha, alpha)\\n",
        "    else:\\n",
        "        lam = 1\\n",
        "    \\n",
        "    batch_size = x.size()[0]\\n",
        "    index = torch.randperm(batch_size)\\n",
        "    \\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\\n",
        "    y_a, y_b = y, y[index]\\n",
        "    \\n",
        "    return mixed_x, y_a, y_b, lam\\n\\n",
        "# Example usage\\n",
        "x = torch.randn(8, 3, 32, 32)  # Batch of images\\n",
        "y = torch.randint(0, 10, (8,))  # Labels\\n",
        "mixed_x, y_a, y_b, lam = mixup_data(x, y)\\n",
        "print(f'Lambda: {lam:.3f}')\\n",
        "print(f'Mixed shape: {mixed_x.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\\n",
        "## Exercise 8: Early Stopping Implementation\\n\\n",
        "### üìö Concept\\n",
        "Early stopping prevents overfitting by monitoring validation performance:\\n",
        "- **Patience**: Number of epochs to wait before stopping\\n",
        "- **Best model**: Save model with best validation performance\\n",
        "- **Restore best**: Load best model after training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EarlyStopping:\\n",
        "    '''Early stopping to prevent overfitting'''\\n",
        "    \\n",
        "    def __init__(self, patience=10, min_delta=0.001, mode='min'):\\n",
        "        self.patience = patience\\n",
        "        self.min_delta = min_delta\\n",
        "        self.mode = mode\\n",
        "        self.counter = 0\\n",
        "        self.best_score = None\\n",
        "        self.early_stop = False\\n",
        "    \\n",
        "    def __call__(self, val_score):\\n",
        "        if self.best_score is None:\\n",
        "            self.best_score = val_score\\n",
        "        elif self._is_improvement(val_score):\\n",
        "            self.best_score = val_score\\n",
        "            self.counter = 0\\n",
        "        else:\\n",
        "            self.counter += 1\\n",
        "            if self.counter >= self.patience:\\n",
        "                self.early_stop = True\\n",
        "        return self.early_stop\\n",
        "    \\n",
        "    def _is_improvement(self, score):\\n",
        "        if self.mode == 'min':\\n",
        "            return score < self.best_score - self.min_delta\\n",
        "        else:\\n",
        "            return score > self.best_score + self.min_delta\\n\\n",
        "# Example usage\\n",
        "early_stopper = EarlyStopping(patience=5)\\n",
        "val_losses = [0.5, 0.4, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41]\\n",
        "\\n",
        "for epoch, loss in enumerate(val_losses):\\n",
        "    if early_stopper(loss):\\n",
        "        print(f'Early stopping at epoch {epoch}')\\n",
        "        break\\n",
        "    print(f'Epoch {epoch}: val_loss = {loss:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\\n",
        "## Exercise 9: Ensemble Methods\\n\\n",
        "### üìö Concept\\n",
        "Ensemble methods combine multiple models for better performance:\\n",
        "- **Averaging**: Mean of predictions\\n",
        "- **Voting**: Majority vote for classification\\n",
        "- **Stacking**: Meta-model learns from base models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EnsembleModel:\\n",
        "    '''Simple ensemble implementation'''\\n",
        "    \\n",
        "    def __init__(self, models):\\n",
        "        self.models = models\\n",
        "    \\n",
        "    def predict_average(self, x):\\n",
        "        '''Average predictions from all models'''\\n",
        "        predictions = []\\n",
        "        for model in self.models:\\n",
        "            model.eval()\\n",
        "            with torch.no_grad():\\n",
        "                pred = model(x)\\n",
        "                predictions.append(pred)\\n",
        "        return torch.stack(predictions).mean(dim=0)\\n",
        "    \\n",
        "    def predict_voting(self, x):\\n",
        "        '''Majority voting for classification'''\\n",
        "        predictions = []\\n",
        "        for model in self.models:\\n",
        "            model.eval()\\n",
        "            with torch.no_grad():\\n",
        "                pred = torch.argmax(model(x), dim=1)\\n",
        "                predictions.append(pred)\\n",
        "        \\n",
        "        # Get mode (most common prediction)\\n",
        "        stacked = torch.stack(predictions)\\n",
        "        mode_values, _ = torch.mode(stacked, dim=0)\\n",
        "        return mode_values\\n\\n",
        "print('Ensemble methods implemented!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\\n",
        "## Exercise 10: Comprehensive Technique Comparison\\n\\n",
        "### üìö Final Summary\\n",
        "Let's compare all the techniques we've learned:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_comparison_table():\\n",
        "    '''Create a comprehensive comparison of all techniques'''\\n",
        "    \\n",
        "    data = {\\n",
        "        'Technique': ['Xavier Init', 'He Init', 'BatchNorm', 'LayerNorm', \\n",
        "                     'Dropout', 'Mixup', 'Early Stopping', 'Ensemble'],\\n",
        "        'Type': ['Initialization', 'Initialization', 'Normalization', 'Normalization',\\n",
        "                'Regularization', 'Augmentation', 'Regularization', 'Ensemble'],\\n",
        "        'Best For': ['Tanh/Sigmoid', 'ReLU', 'Large Batch CNN', 'RNN/Transformer',\\n",
        "                    'Fully Connected', 'Classification', 'All Models', 'Final Performance'],\\n",
        "        'Key Benefit': ['Stable gradients', 'ReLU optimization', 'Reduces shift',\\n",
        "                       'Batch independent', 'Prevents overfit', 'Smooth boundaries',\\n",
        "                       'Stops overfit', 'Reduces variance']\\n",
        "    }\\n",
        "    \\n",
        "    df = pd.DataFrame(data)\\n",
        "    \\n",
        "    # Display with style\\n",
        "    styled_df = df.style.set_properties(**{\\n",
        "        'background-color': 'lightblue',\\n",
        "        'color': 'black',\\n",
        "        'border-color': 'white'\\n",
        "    })\\n",
        "    \\n",
        "    return df\\n\\n",
        "comparison_df = create_comparison_table()\\n",
        "print(comparison_df.to_string())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary and Key Takeaways\\n\\n",
        "### üéØ What We've Learned\\n\\n",
        "1. **Weight Initialization**:\\n",
        "   - Xavier/Glorot for tanh/sigmoid\\n",
        "   - He initialization for ReLU\\n",
        "   - LSUV for very deep networks\\n\\n",
        "2. **Normalization Techniques**:\\n",
        "   - BatchNorm for CNNs with large batches\\n",
        "   - LayerNorm for RNNs and Transformers\\n",
        "   - GroupNorm for small batch training\\n\\n",
        "3. **Regularization Methods**:\\n",
        "   - Dropout and variants\\n",
        "   - Data augmentation (Mixup, CutMix)\\n",
        "   - Early stopping\\n\\n",
        "### üìù Best Practices\\n\\n",
        "- Always initialize weights properly based on activation function\\n",
        "- Choose normalization based on architecture and batch size\\n",
        "- Combine multiple regularization techniques for best results\\n",
        "- Monitor validation metrics to prevent overfitting\\n\\n",
        "### üöÄ Next Steps\\n\\n",
        "- Experiment with different combinations on your own datasets\\n",
        "- Try implementing custom initialization methods\\n",
        "- Explore advanced normalization techniques (e.g., AdaNorm, CrossNorm)\\n",
        "- Investigate the impact on different architectures (ResNet, Transformer, etc.)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}