{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üéì Lecture 18: Advanced Unsupervised Learning\n",
        "## Hands-on Practice Notebook\n",
        "\n",
        "---\n",
        "\n",
        "**Based on**: Ho-min Park's Lecture\n",
        "\n",
        "**Topics Covered**:\n",
        "1. üîÑ Self-Supervised Learning (SimCLR, MoCo, BYOL)\n",
        "2. üìà Time Series Clustering (DTW, K-Shape)\n",
        "3. üï∏Ô∏è Graph Clustering (Spectral, Louvain, GNN)\n",
        "4. üöÄ Advanced Topics (Multi-modal, Large-scale)\n",
        "\n",
        "**Estimated Time**: 4-6 hours\n",
        "\n",
        "**Prerequisites**:\n",
        "- Python programming\n",
        "- Basic machine learning concepts\n",
        "- NumPy, Pandas, Matplotlib\n",
        "- PyTorch basics (optional but recommended)\n",
        "\n",
        "---\n",
        "\n",
        "### üìö Learning Objectives\n",
        "\n",
        "By the end of this notebook, you will be able to:\n",
        "- Understand and implement contrastive learning principles\n",
        "- Apply Dynamic Time Warping for time series clustering\n",
        "- Perform graph clustering using spectral methods and GNNs\n",
        "- Handle multi-modal and large-scale clustering problems\n",
        "- Compare different clustering approaches on real datasets\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ Part 1: Setup and Imports\n",
        "\n",
        "Let's import all necessary libraries for our hands-on exercises."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from scipy.spatial.distance import euclidean, cdist\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Sklearn utilities\n",
        "from sklearn.cluster import KMeans, SpectralClustering, DBSCAN\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score, adjusted_rand_score\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.datasets import make_blobs, make_moons, make_circles\n",
        "\n",
        "# Deep Learning\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Graph libraries\n",
        "import networkx as nx\n",
        "\n",
        "# Plotting style\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "print(\"‚úÖ All libraries imported successfully!\")\n",
        "print(f\"NumPy version: {np.__version__}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"NetworkX version: {nx.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# üîÑ Part 2: Self-Supervised Learning\n",
        "\n",
        "Self-supervised learning enables models to learn representations from unlabeled data by creating pseudo-labels from the data itself.\n",
        "\n",
        "## Key Concepts:\n",
        "- **Contrastive Learning**: Learn by contrasting positive pairs (similar) against negative pairs (dissimilar)\n",
        "- **SimCLR**: Simple Framework for Contrastive Learning of Visual Representations\n",
        "- **InfoNCE Loss**: Normalized Temperature-scaled Cross Entropy Loss\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù Exercise 1: Understanding Contrastive Learning\n",
        "\n",
        "### Concept\n",
        "\n",
        "**Contrastive learning** aims to learn representations where:\n",
        "- Similar items (positive pairs) are pulled **close together**\n",
        "- Dissimilar items (negative pairs) are pushed **far apart**\n",
        "\n",
        "**InfoNCE Loss Formula**:\n",
        "\n",
        "$$\\mathcal{L} = -\\log \\frac{\\exp(\\text{sim}(z_i, z_j) / \\tau)}{\\sum_{k=1}^{2N} \\mathbb{1}_{[k \\neq i]} \\exp(\\text{sim}(z_i, z_k) / \\tau)}$$\n",
        "\n",
        "Where:\n",
        "- $z_i, z_j$ are embeddings of positive pairs\n",
        "- $\\tau$ is the temperature parameter\n",
        "- $\\text{sim}$ is cosine similarity\n",
        "\n",
        "### Implementation\n",
        "\n",
        "Let's implement the InfoNCE loss function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def contrastive_loss(z_i, z_j, temperature=0.5):\n",
        "    \"\"\"\n",
        "    Compute InfoNCE (NT-Xent) contrastive loss\n",
        "    \n",
        "    Args:\n",
        "        z_i: embeddings of augmented view 1, shape (batch_size, embedding_dim)\n",
        "        z_j: embeddings of augmented view 2, shape (batch_size, embedding_dim)\n",
        "        temperature: temperature scaling parameter\n",
        "    \n",
        "    Returns:\n",
        "        loss: contrastive loss value\n",
        "    \"\"\"\n",
        "    batch_size = z_i.shape[0]\n",
        "    \n",
        "    # Normalize embeddings\n",
        "    z_i = F.normalize(z_i, dim=1)\n",
        "    z_j = F.normalize(z_j, dim=1)\n",
        "    \n",
        "    # Concatenate embeddings\n",
        "    representations = torch.cat([z_i, z_j], dim=0)\n",
        "    \n",
        "    # Compute similarity matrix\n",
        "    similarity_matrix = F.cosine_similarity(\n",
        "        representations.unsqueeze(1), \n",
        "        representations.unsqueeze(0), \n",
        "        dim=2\n",
        "    )\n",
        "    \n",
        "    # Create mask to remove self-similarities\n",
        "    mask = torch.eye(2 * batch_size, dtype=torch.bool)\n",
        "    similarity_matrix = similarity_matrix[~mask].view(2 * batch_size, -1)\n",
        "    \n",
        "    # Positive pairs are (i, j) and (j, i)\n",
        "    positives = torch.cat([z_i @ z_j.T, z_j @ z_i.T], dim=0).diag()\n",
        "    \n",
        "    # Compute logits\n",
        "    logits = similarity_matrix / temperature\n",
        "    positives = positives / temperature\n",
        "    \n",
        "    # InfoNCE loss\n",
        "    loss = -positives + torch.logsumexp(logits, dim=1)\n",
        "    return loss.mean()\n",
        "\n",
        "\n",
        "# Generate synthetic embeddings for demonstration\n",
        "batch_size = 128\n",
        "embedding_dim = 64\n",
        "\n",
        "# Create positive pairs (similar embeddings with small noise)\n",
        "z_anchor = torch.randn(batch_size, embedding_dim)\n",
        "z_positive = z_anchor + torch.randn(batch_size, embedding_dim) * 0.1\n",
        "\n",
        "# Compute loss\n",
        "loss = contrastive_loss(z_anchor, z_positive, temperature=0.5)\n",
        "print(f\"‚úÖ Contrastive Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Test with different temperatures\n",
        "temperatures = [0.1, 0.5, 1.0, 2.0]\n",
        "losses = []\n",
        "\n",
        "for temp in temperatures:\n",
        "    loss = contrastive_loss(z_anchor, z_positive, temperature=temp)\n",
        "    losses.append(loss.item())\n",
        "    print(f\"Temperature {temp:.1f}: Loss = {loss.item():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üìä Visualization: Effect of Temperature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Plot 1: Temperature vs Loss\n",
        "axes[0].plot(temperatures, losses, 'o-', linewidth=2, markersize=8)\n",
        "axes[0].set_xlabel('Temperature (œÑ)', fontsize=12)\n",
        "axes[0].set_ylabel('Contrastive Loss', fontsize=12)\n",
        "axes[0].set_title('Effect of Temperature on Contrastive Loss', fontsize=14, fontweight='bold')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Embedding space visualization\n",
        "# Generate 2D embeddings for visualization\n",
        "n_samples = 50\n",
        "z_2d_anchor = torch.randn(n_samples, 2)\n",
        "z_2d_positive = z_2d_anchor + torch.randn(n_samples, 2) * 0.2\n",
        "z_2d_negative = torch.randn(n_samples, 2) * 2\n",
        "\n",
        "axes[1].scatter(z_2d_anchor[:, 0], z_2d_anchor[:, 1], \n",
        "                c='blue', alpha=0.6, s=100, label='Anchor', edgecolors='black')\n",
        "axes[1].scatter(z_2d_positive[:, 0], z_2d_positive[:, 1], \n",
        "                c='green', alpha=0.6, s=100, label='Positive', edgecolors='black')\n",
        "axes[1].scatter(z_2d_negative[:, 0], z_2d_negative[:, 1], \n",
        "                c='red', alpha=0.6, s=100, label='Negative', edgecolors='black')\n",
        "\n",
        "# Draw lines connecting positive pairs\n",
        "for i in range(min(20, n_samples)):\n",
        "    axes[1].plot([z_2d_anchor[i, 0], z_2d_positive[i, 0]], \n",
        "                 [z_2d_anchor[i, 1], z_2d_positive[i, 1]], \n",
        "                 'gray', alpha=0.3, linewidth=1)\n",
        "\n",
        "axes[1].set_xlabel('Dimension 1', fontsize=12)\n",
        "axes[1].set_ylabel('Dimension 2', fontsize=12)\n",
        "axes[1].set_title('Embedding Space: Positive vs Negative Pairs', fontsize=14, fontweight='bold')\n",
        "axes[1].legend(fontsize=10)\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüìä Key Insights:\")\n",
        "print(\"1. Lower temperature ‚Üí More emphasis on hard negatives\")\n",
        "print(\"2. Higher temperature ‚Üí Softer distributions\")\n",
        "print(\"3. Positive pairs should be close, negatives far apart\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üéØ Your Turn: Practice Task\n",
        "\n",
        "**Task**: Modify the contrastive loss function to:\n",
        "1. Add a margin-based loss component\n",
        "2. Experiment with different similarity metrics (L2 distance instead of cosine)\n",
        "3. Visualize how the embedding space changes with different parameters\n",
        "\n",
        "**Hint**: Try implementing Triplet Loss as an alternative to InfoNCE."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìù Exercise 2: SimCLR Data Augmentation Pipeline\n",
        "\n",
        "### Concept\n",
        "\n",
        "**SimCLR** (Simple Framework for Contrastive Learning) uses strong data augmentation to create positive pairs:\n",
        "\n",
        "**Augmentation Pipeline**:\n",
        "1. Random crop and resize\n",
        "2. Random color jittering\n",
        "3. Random Gaussian blur\n",
        "4. Random horizontal flip\n",
        "\n",
        "The key insight: Different augmented views of the same image should have similar representations.\n",
        "\n",
        "### Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SimCLRAugmentation:\n",
        "    \"\"\"\n",
        "    SimCLR data augmentation pipeline\n",
        "    \"\"\"\n",
        "    def __init__(self, img_size=32):\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.RandomResizedCrop(img_size, scale=(0.2, 1.0)),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomApply([\n",
        "                transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)\n",
        "            ], p=0.8),\n",
        "            transforms.RandomGrayscale(p=0.2),\n",
        "            transforms.RandomApply([transforms.GaussianBlur(kernel_size=3)], p=0.5),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "        ])\n",
        "    \n",
        "    def __call__(self, x):\n",
        "        return self.transform(x), self.transform(x)\n",
        "\n",
        "\n",
        "# Load CIFAR-10 dataset (we'll use a small subset for demonstration)\n",
        "from torchvision.datasets import CIFAR10\n",
        "from PIL import Image\n",
        "\n",
        "# Download and prepare dataset\n",
        "try:\n",
        "    dataset = CIFAR10(root='./data', train=True, download=True)\n",
        "    print(\"‚úÖ CIFAR-10 dataset loaded successfully!\")\n",
        "except:\n",
        "    print(\"‚ö†Ô∏è  CIFAR-10 download may take a moment...\")\n",
        "    dataset = CIFAR10(root='./data', train=True, download=True)\n",
        "\n",
        "# Get a sample image\n",
        "sample_img, label = dataset[42]\n",
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n",
        "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "print(f\"Sample image: {class_names[label]}\")\n",
        "\n",
        "# Apply SimCLR augmentations\n",
        "augmenter = SimCLRAugmentation(img_size=32)\n",
        "\n",
        "# Create augmented views\n",
        "view1_list = []\n",
        "view2_list = []\n",
        "\n",
        "for _ in range(4):\n",
        "    view1, view2 = augmenter(sample_img)\n",
        "    view1_list.append(view1)\n",
        "    view2_list.append(view2)\n",
        "\n",
        "print(f\"‚úÖ Generated {len(view1_list)} augmented pairs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üìä Visualization: Augmented Pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def denormalize(tensor):\n",
        "    \"\"\"Denormalize tensor for visualization\"\"\"\n",
        "    tensor = tensor * 0.5 + 0.5\n",
        "    return torch.clamp(tensor, 0, 1)\n",
        "\n",
        "fig, axes = plt.subplots(4, 3, figsize=(12, 14))\n",
        "\n",
        "# Original image\n",
        "for i in range(4):\n",
        "    axes[i, 0].imshow(sample_img)\n",
        "    axes[i, 0].set_title('Original', fontsize=12, fontweight='bold')\n",
        "    axes[i, 0].axis('off')\n",
        "\n",
        "# Augmented views\n",
        "for i in range(4):\n",
        "    # View 1\n",
        "    img1 = denormalize(view1_list[i]).permute(1, 2, 0).numpy()\n",
        "    axes[i, 1].imshow(img1)\n",
        "    axes[i, 1].set_title('Augmented View 1', fontsize=12, fontweight='bold')\n",
        "    axes[i, 1].axis('off')\n",
        "    \n",
        "    # View 2\n",
        "    img2 = denormalize(view2_list[i]).permute(1, 2, 0).numpy()\n",
        "    axes[i, 2].imshow(img2)\n",
        "    axes[i, 2].set_title('Augmented View 2', fontsize=12, fontweight='bold')\n",
        "    axes[i, 2].axis('off')\n",
        "\n",
        "plt.suptitle(f'SimCLR Augmentation: {class_names[label].upper()}', \n",
        "             fontsize=16, fontweight='bold', y=0.995)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüìä Key Insights:\")\n",
        "print(\"1. Strong augmentations create diverse views\")\n",
        "print(\"2. Each pair should maintain semantic content\")\n",
        "print(\"3. Model learns to recognize invariances\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üéØ Your Turn: Practice Task\n",
        "\n",
        "**Task**: Create a custom augmentation pipeline that:\n",
        "1. Adds rotation augmentation (0¬∞, 90¬∞, 180¬∞, 270¬∞)\n",
        "2. Implements cutout/random erasing\n",
        "3. Compares augmentation strength effects on downstream performance\n",
        "\n",
        "**Bonus**: Visualize which augmentations are most beneficial for learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìù Exercise 3: Training a Simple Contrastive Encoder\n",
        "\n",
        "### Concept\n",
        "\n",
        "**Self-Supervised Pretraining Pipeline**:\n",
        "1. **Pretrain**: Learn representations using contrastive learning (no labels)\n",
        "2. **Fine-tune**: Adapt to downstream task with small labeled dataset\n",
        "3. **Deploy**: Use learned features for inference\n",
        "\n",
        "**Advantages**:\n",
        "- Reduces labeled data requirements\n",
        "- Better generalization\n",
        "- Transfer learning capabilities\n",
        "\n",
        "### Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SimpleEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Simple CNN encoder for contrastive learning\n",
        "    \"\"\"\n",
        "    def __init__(self, embedding_dim=128):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Flatten()\n",
        "        )\n",
        "        self.projection_head = nn.Sequential(\n",
        "            nn.Linear(128, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, embedding_dim)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        features = self.encoder(x)\n",
        "        embeddings = self.projection_head(features)\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "# Initialize model\n",
        "model = SimpleEncoder(embedding_dim=128)\n",
        "print(f\"‚úÖ Model initialized with {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
        "\n",
        "# Create synthetic training data (for demonstration)\n",
        "# In practice, use real datasets like CIFAR-10\n",
        "def create_synthetic_batch(batch_size=32, img_size=32):\n",
        "    \"\"\"Create synthetic image pairs for demonstration\"\"\"\n",
        "    # Generate random images\n",
        "    img1 = torch.randn(batch_size, 3, img_size, img_size)\n",
        "    # Create augmented version (add small noise)\n",
        "    img2 = img1 + torch.randn_like(img1) * 0.1\n",
        "    return img1, img2\n",
        "\n",
        "# Training loop (simplified)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "num_epochs = 5\n",
        "losses = []\n",
        "\n",
        "print(\"\\nüèãÔ∏è Training contrastive encoder...\")\n",
        "model.train()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_losses = []\n",
        "    \n",
        "    # Simulate 10 batches per epoch\n",
        "    for batch_idx in range(10):\n",
        "        # Get batch\n",
        "        img1, img2 = create_synthetic_batch(batch_size=32)\n",
        "        \n",
        "        # Forward pass\n",
        "        z1 = model(img1)\n",
        "        z2 = model(img2)\n",
        "        \n",
        "        # Compute loss\n",
        "        loss = contrastive_loss(z1, z2, temperature=0.5)\n",
        "        \n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_losses.append(loss.item())\n",
        "    \n",
        "    avg_loss = np.mean(epoch_losses)\n",
        "    losses.append(avg_loss)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {avg_loss:.4f}\")\n",
        "\n",
        "print(\"\\n‚úÖ Training completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üìä Visualization: Training Progress"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Plot 1: Training loss\n",
        "axes[0].plot(range(1, num_epochs+1), losses, 'o-', linewidth=2, markersize=8)\n",
        "axes[0].set_xlabel('Epoch', fontsize=12)\n",
        "axes[0].set_ylabel('Contrastive Loss', fontsize=12)\n",
        "axes[0].set_title('Training Loss Curve', fontsize=14, fontweight='bold')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Embedding space visualization (t-SNE)\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    # Generate embeddings for multiple images\n",
        "    embeddings_list = []\n",
        "    labels_list = []\n",
        "    \n",
        "    for i in range(5):  # 5 different \"classes\"\n",
        "        img_batch = torch.randn(20, 3, 32, 32) + i * 0.5  # Slight shift per class\n",
        "        embeds = model(img_batch)\n",
        "        embeddings_list.append(embeds.numpy())\n",
        "        labels_list.extend([i] * 20)\n",
        "    \n",
        "    embeddings = np.vstack(embeddings_list)\n",
        "    labels = np.array(labels_list)\n",
        "\n",
        "# Apply t-SNE\n",
        "from sklearn.manifold import TSNE\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "embeddings_2d = tsne.fit_transform(embeddings)\n",
        "\n",
        "# Plot\n",
        "scatter = axes[1].scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], \n",
        "                          c=labels, cmap='tab10', s=100, alpha=0.7, edgecolors='black')\n",
        "axes[1].set_xlabel('t-SNE Dimension 1', fontsize=12)\n",
        "axes[1].set_ylabel('t-SNE Dimension 2', fontsize=12)\n",
        "axes[1].set_title('Learned Embedding Space (t-SNE)', fontsize=14, fontweight='bold')\n",
        "plt.colorbar(scatter, ax=axes[1], label='Class')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüìä Key Insights:\")\n",
        "print(\"1. Loss decreases ‚Üí Model learning useful representations\")\n",
        "print(\"2. Similar samples cluster together in embedding space\")\n",
        "print(\"3. Self-supervised learning discovers structure without labels\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üéØ Your Turn: Practice Task\n",
        "\n",
        "**Task**: Extend the training pipeline to:\n",
        "1. Compare self-supervised vs supervised learning with limited labels\n",
        "2. Add a linear classifier on top of frozen features\n",
        "3. Evaluate on a downstream classification task\n",
        "4. Plot: Accuracy vs Number of Labeled Samples\n",
        "\n",
        "**Expected Result**: Self-supervised pretraining should outperform training from scratch with few labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# üìà Part 3: Time Series Clustering\n",
        "\n",
        "Time series data has unique characteristics:\n",
        "- **Variable lengths**: Different durations\n",
        "- **Phase shifts**: Time warping between patterns\n",
        "- **Noise**: Random fluctuations\n",
        "- **Sampling rates**: Inconsistent frequencies\n",
        "\n",
        "**Challenge**: Euclidean distance is inadequate for comparing time series!\n",
        "\n",
        "## Key Methods:\n",
        "- **DTW (Dynamic Time Warping)**: Optimal alignment of sequences\n",
        "- **K-Shape**: Shape-based clustering with cross-correlation\n",
        "- **Matrix Profile**: Subsequence pattern discovery\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù Exercise 4: Understanding Time Series Characteristics\n",
        "\n",
        "### Concept\n",
        "\n",
        "Time series data exhibits several important properties:\n",
        "- **Trend**: Long-term increase/decrease\n",
        "- **Seasonality**: Periodic patterns\n",
        "- **Noise**: Random variations\n",
        "- **Phase shifts**: Temporal misalignment\n",
        "\n",
        "### Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_time_series(n_points=100, series_type='trend'):\n",
        "    \"\"\"\n",
        "    Generate synthetic time series with different characteristics\n",
        "    \"\"\"\n",
        "    t = np.linspace(0, 10, n_points)\n",
        "    \n",
        "    if series_type == 'trend':\n",
        "        # Linear trend with noise\n",
        "        series = 0.5 * t + np.random.normal(0, 0.5, n_points)\n",
        "        \n",
        "    elif series_type == 'seasonal':\n",
        "        # Seasonal pattern\n",
        "        series = np.sin(2 * np.pi * t) + np.random.normal(0, 0.1, n_points)\n",
        "        \n",
        "    elif series_type == 'trend_seasonal':\n",
        "        # Trend + Seasonality\n",
        "        series = 0.3 * t + np.sin(2 * np.pi * t) + np.random.normal(0, 0.2, n_points)\n",
        "        \n",
        "    elif series_type == 'noisy':\n",
        "        # High noise\n",
        "        series = np.sin(2 * np.pi * t) + np.random.normal(0, 1.0, n_points)\n",
        "        \n",
        "    elif series_type == 'phase_shift':\n",
        "        # Phase-shifted sine wave\n",
        "        phase = np.random.uniform(0, 2 * np.pi)\n",
        "        series = np.sin(2 * np.pi * t + phase) + np.random.normal(0, 0.1, n_points)\n",
        "    \n",
        "    return t, series\n",
        "\n",
        "\n",
        "# Generate different types of time series\n",
        "series_types = ['trend', 'seasonal', 'trend_seasonal', 'noisy', 'phase_shift']\n",
        "time_series_data = {}\n",
        "\n",
        "for ts_type in series_types:\n",
        "    t, series = generate_time_series(n_points=150, series_type=ts_type)\n",
        "    time_series_data[ts_type] = (t, series)\n",
        "\n",
        "print(\"‚úÖ Generated 5 different types of time series\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üìä Visualization: Time Series Types"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(3, 2, figsize=(14, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "titles = {\n",
        "    'trend': 'Trend Pattern',\n",
        "    'seasonal': 'Seasonal Pattern',\n",
        "    'trend_seasonal': 'Trend + Seasonal',\n",
        "    'noisy': 'Noisy Signal',\n",
        "    'phase_shift': 'Phase-Shifted Pattern'\n",
        "}\n",
        "\n",
        "for idx, (ts_type, (t, series)) in enumerate(time_series_data.items()):\n",
        "    axes[idx].plot(t, series, linewidth=2, color=f'C{idx}')\n",
        "    axes[idx].set_title(titles[ts_type], fontsize=13, fontweight='bold')\n",
        "    axes[idx].set_xlabel('Time', fontsize=11)\n",
        "    axes[idx].set_ylabel('Value', fontsize=11)\n",
        "    axes[idx].grid(True, alpha=0.3)\n",
        "\n",
        "# Add comparison plot\n",
        "axes[5].plot(time_series_data['seasonal'][0], time_series_data['seasonal'][1], \n",
        "             label='Original', linewidth=2, color='blue')\n",
        "axes[5].plot(time_series_data['phase_shift'][0], time_series_data['phase_shift'][1], \n",
        "             label='Phase-Shifted', linewidth=2, color='red', linestyle='--')\n",
        "axes[5].set_title('Comparison: Phase Shift Challenge', fontsize=13, fontweight='bold')\n",
        "axes[5].set_xlabel('Time', fontsize=11)\n",
        "axes[5].set_ylabel('Value', fontsize=11)\n",
        "axes[5].legend(fontsize=10)\n",
        "axes[5].grid(True, alpha=0.3)\n",
        "\n",
        "plt.suptitle('Time Series Data Characteristics', fontsize=16, fontweight='bold', y=1.00)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüìä Key Insights:\")\n",
        "print(\"1. Different patterns require different distance metrics\")\n",
        "print(\"2. Phase shifts are problematic for Euclidean distance\")\n",
        "print(\"3. Noise can obscure underlying patterns\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üéØ Your Turn: Practice Task\n",
        "\n",
        "**Task**: Generate and analyze your own time series:\n",
        "1. Create a time series with multiple components (trend + seasonal + noise)\n",
        "2. Decompose it using seasonal decomposition\n",
        "3. Compare Euclidean vs DTW distance on shifted versions\n",
        "\n",
        "**Bonus**: Implement autocorrelation analysis to detect periodicity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìù Exercise 5: Dynamic Time Warping (DTW)\n",
        "\n",
        "### Concept\n",
        "\n",
        "**Dynamic Time Warping** finds the optimal alignment between two time series by:\n",
        "- Allowing one-to-many matching\n",
        "- Handling speed variations\n",
        "- Computing minimum distance path through cost matrix\n",
        "\n",
        "**DTW Algorithm**:\n",
        "1. Build cost matrix: $D(i,j) = |x_i - y_j| + \\min\\{D(i-1,j), D(i,j-1), D(i-1,j-1)\\}$\n",
        "2. Find optimal warping path\n",
        "3. Sum distances along path\n",
        "\n",
        "**Complexity**: O(n¬≤) where n is series length\n",
        "\n",
        "### Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def dtw_distance(series1, series2):\n",
        "    \"\"\"\n",
        "    Compute Dynamic Time Warping distance between two time series\n",
        "    \n",
        "    Args:\n",
        "        series1: First time series (array)\n",
        "        series2: Second time series (array)\n",
        "    \n",
        "    Returns:\n",
        "        distance: DTW distance\n",
        "        cost_matrix: Cost matrix for visualization\n",
        "        path: Optimal warping path\n",
        "    \"\"\"\n",
        "    n, m = len(series1), len(series2)\n",
        "    \n",
        "    # Initialize cost matrix with infinity\n",
        "    cost_matrix = np.full((n+1, m+1), np.inf)\n",
        "    cost_matrix[0, 0] = 0\n",
        "    \n",
        "    # Fill cost matrix\n",
        "    for i in range(1, n+1):\n",
        "        for j in range(1, m+1):\n",
        "            cost = abs(series1[i-1] - series2[j-1])\n",
        "            cost_matrix[i, j] = cost + min(\n",
        "                cost_matrix[i-1, j],    # insertion\n",
        "                cost_matrix[i, j-1],    # deletion\n",
        "                cost_matrix[i-1, j-1]   # match\n",
        "            )\n",
        "    \n",
        "    # Backtrack to find optimal path\n",
        "    path = []\n",
        "    i, j = n, m\n",
        "    while i > 0 and j > 0:\n",
        "        path.append((i-1, j-1))\n",
        "        \n",
        "        # Choose minimum predecessor\n",
        "        candidates = [\n",
        "            (cost_matrix[i-1, j], (i-1, j)),\n",
        "            (cost_matrix[i, j-1], (i, j-1)),\n",
        "            (cost_matrix[i-1, j-1], (i-1, j-1))\n",
        "        ]\n",
        "        _, (i, j) = min(candidates, key=lambda x: x[0])\n",
        "    \n",
        "    path.reverse()\n",
        "    distance = cost_matrix[n, m]\n",
        "    \n",
        "    return distance, cost_matrix[1:, 1:], path\n",
        "\n",
        "\n",
        "# Generate two similar time series with phase shift\n",
        "t = np.linspace(0, 4*np.pi, 100)\n",
        "series1 = np.sin(t) + np.random.normal(0, 0.1, 100)\n",
        "series2 = np.sin(t + np.pi/4) + np.random.normal(0, 0.1, 100)  # Phase shifted\n",
        "\n",
        "# Compute DTW distance\n",
        "dtw_dist, cost_matrix, dtw_path = dtw_distance(series1, series2)\n",
        "\n",
        "# Compute Euclidean distance for comparison\n",
        "euclidean_dist = np.sqrt(np.sum((series1 - series2)**2))\n",
        "\n",
        "print(f\"‚úÖ DTW Distance: {dtw_dist:.4f}\")\n",
        "print(f\"üìè Euclidean Distance: {euclidean_dist:.4f}\")\n",
        "print(f\"üõ§Ô∏è  Warping path length: {len(dtw_path)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üìä Visualization: DTW Alignment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# Plot 1: Original time series\n",
        "axes[0, 0].plot(series1, label='Series 1', linewidth=2, color='blue')\n",
        "axes[0, 0].plot(series2, label='Series 2 (phase-shifted)', linewidth=2, color='red', linestyle='--')\n",
        "axes[0, 0].set_title('Original Time Series', fontsize=13, fontweight='bold')\n",
        "axes[0, 0].set_xlabel('Time Index', fontsize=11)\n",
        "axes[0, 0].set_ylabel('Value', fontsize=11)\n",
        "axes[0, 0].legend(fontsize=10)\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: DTW cost matrix with path\n",
        "im = axes[0, 1].imshow(cost_matrix, cmap='YlOrRd', aspect='auto', origin='lower')\n",
        "plt.colorbar(im, ax=axes[0, 1], label='Cumulative Cost')\n",
        "\n",
        "# Plot optimal path\n",
        "path_i = [p[0] for p in dtw_path]\n",
        "path_j = [p[1] for p in dtw_path]\n",
        "axes[0, 1].plot(path_j, path_i, 'blue', linewidth=2, label='Optimal Path')\n",
        "axes[0, 1].set_title('DTW Cost Matrix & Warping Path', fontsize=13, fontweight='bold')\n",
        "axes[0, 1].set_xlabel('Series 2 Index', fontsize=11)\n",
        "axes[0, 1].set_ylabel('Series 1 Index', fontsize=11)\n",
        "axes[0, 1].legend(fontsize=10)\n",
        "\n",
        "# Plot 3: Aligned series with connections\n",
        "# Sample every 5th point to avoid clutter\n",
        "sample_indices = range(0, len(dtw_path), 5)\n",
        "for idx in sample_indices:\n",
        "    i, j = dtw_path[idx]\n",
        "    axes[1, 0].plot([i, j], [series1[i], series2[j]], 'gray', alpha=0.3, linewidth=1)\n",
        "\n",
        "axes[1, 0].plot(series1, label='Series 1', linewidth=2, color='blue')\n",
        "axes[1, 0].plot(series2, label='Series 2', linewidth=2, color='red', linestyle='--')\n",
        "axes[1, 0].set_title('DTW Alignment (sampled connections)', fontsize=13, fontweight='bold')\n",
        "axes[1, 0].set_xlabel('Time Index', fontsize=11)\n",
        "axes[1, 0].set_ylabel('Value', fontsize=11)\n",
        "axes[1, 0].legend(fontsize=10)\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 4: Distance comparison\n",
        "distances = ['DTW', 'Euclidean']\n",
        "values = [dtw_dist, euclidean_dist]\n",
        "colors = ['steelblue', 'coral']\n",
        "\n",
        "bars = axes[1, 1].bar(distances, values, color=colors, edgecolor='black', linewidth=2)\n",
        "axes[1, 1].set_title('Distance Metric Comparison', fontsize=13, fontweight='bold')\n",
        "axes[1, 1].set_ylabel('Distance Value', fontsize=11)\n",
        "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar, val in zip(bars, values):\n",
        "    height = bar.get_height()\n",
        "    axes[1, 1].text(bar.get_x() + bar.get_width()/2., height,\n",
        "                    f'{val:.2f}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
        "\n",
        "plt.suptitle('Dynamic Time Warping Analysis', fontsize=16, fontweight='bold', y=0.995)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüìä Key Insights:\")\n",
        "print(\"1. DTW finds optimal alignment despite phase shift\")\n",
        "print(\"2. Warping path shows how points are matched\")\n",
        "print(\"3. DTW is more robust to temporal distortions\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üéØ Your Turn: Practice Task\n",
        "\n",
        "**Task**: Implement and test DTW with constraints:\n",
        "1. Add Sakoe-Chiba band (window constraint)\n",
        "2. Compare constrained vs unconstrained DTW\n",
        "3. Measure computation time vs accuracy trade-off\n",
        "4. Test on real data (e.g., speech or ECG signals)\n",
        "\n",
        "**Bonus**: Implement FastDTW for improved efficiency."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìù Exercise 6: DTW-based Clustering\n",
        "\n",
        "### Concept\n",
        "\n",
        "**DTW-based Clustering** combines DTW distance with clustering algorithms:\n",
        "\n",
        "**Methods**:\n",
        "1. **K-medoids**: Uses pairwise DTW distances (better than K-means)\n",
        "2. **Hierarchical**: Creates dendrogram with DTW linkage\n",
        "3. **DBA (DTW Barycenter Averaging)**: Computes cluster centroids\n",
        "\n",
        "**Applications**:\n",
        "- ECG pattern analysis\n",
        "- Gesture recognition\n",
        "- Stock price clustering\n",
        "- Speech recognition\n",
        "\n",
        "### Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_time_series_dataset(n_series=30, n_points=100, n_clusters=3):\n",
        "    \"\"\"\n",
        "    Create synthetic time series dataset with different patterns\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    labels = []\n",
        "    \n",
        "    t = np.linspace(0, 10, n_points)\n",
        "    \n",
        "    for cluster_id in range(n_clusters):\n",
        "        for i in range(n_series // n_clusters):\n",
        "            # Different patterns for each cluster\n",
        "            if cluster_id == 0:\n",
        "                # Low frequency sine wave\n",
        "                series = np.sin(t) + np.random.normal(0, 0.2, n_points)\n",
        "            elif cluster_id == 1:\n",
        "                # High frequency sine wave\n",
        "                series = np.sin(3 * t) + np.random.normal(0, 0.2, n_points)\n",
        "            else:\n",
        "                # Linear trend\n",
        "                series = 0.3 * t + np.sin(t) + np.random.normal(0, 0.2, n_points)\n",
        "            \n",
        "            data.append(series)\n",
        "            labels.append(cluster_id)\n",
        "    \n",
        "    return np.array(data), np.array(labels)\n",
        "\n",
        "\n",
        "# Generate dataset\n",
        "X_ts, y_true = create_time_series_dataset(n_series=30, n_points=100, n_clusters=3)\n",
        "print(f\"‚úÖ Created dataset: {X_ts.shape[0]} time series, {X_ts.shape[1]} points each\")\n",
        "\n",
        "# Compute DTW distance matrix\n",
        "def compute_dtw_distance_matrix(time_series_list):\n",
        "    \"\"\"\n",
        "    Compute pairwise DTW distance matrix\n",
        "    \"\"\"\n",
        "    n = len(time_series_list)\n",
        "    distance_matrix = np.zeros((n, n))\n",
        "    \n",
        "    for i in range(n):\n",
        "        for j in range(i+1, n):\n",
        "            dist, _, _ = dtw_distance(time_series_list[i], time_series_list[j])\n",
        "            distance_matrix[i, j] = dist\n",
        "            distance_matrix[j, i] = dist\n",
        "    \n",
        "    return distance_matrix\n",
        "\n",
        "print(\"\\n‚è≥ Computing DTW distance matrix (this may take a moment)...\")\n",
        "dtw_dist_matrix = compute_dtw_distance_matrix(X_ts)\n",
        "print(f\"‚úÖ Distance matrix computed: {dtw_dist_matrix.shape}\")\n",
        "\n",
        "# Hierarchical clustering with DTW\n",
        "from scipy.cluster.hierarchy import linkage, fcluster, dendrogram\n",
        "\n",
        "# Convert distance matrix to condensed form for scipy\n",
        "from scipy.spatial.distance import squareform\n",
        "condensed_dist = squareform(dtw_dist_matrix)\n",
        "\n",
        "# Perform hierarchical clustering\n",
        "linkage_matrix = linkage(condensed_dist, method='average')\n",
        "y_pred = fcluster(linkage_matrix, t=3, criterion='maxclust')\n",
        "\n",
        "# Adjust labels to start from 0\n",
        "y_pred = y_pred - 1\n",
        "\n",
        "# Evaluate clustering\n",
        "ari = adjusted_rand_score(y_true, y_pred)\n",
        "print(f\"\\nüìä Adjusted Rand Index: {ari:.4f}\")\n",
        "print(f\"‚úÖ Clustering completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üìä Visualization: DTW Clustering Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# Plot 1: Dendrogram\n",
        "dendrogram(linkage_matrix, ax=axes[0, 0], color_threshold=50)\n",
        "axes[0, 0].set_title('Hierarchical Clustering Dendrogram', fontsize=13, fontweight='bold')\n",
        "axes[0, 0].set_xlabel('Sample Index', fontsize=11)\n",
        "axes[0, 0].set_ylabel('DTW Distance', fontsize=11)\n",
        "axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Plot 2: Distance matrix heatmap\n",
        "im = axes[0, 1].imshow(dtw_dist_matrix, cmap='viridis', aspect='auto')\n",
        "plt.colorbar(im, ax=axes[0, 1], label='DTW Distance')\n",
        "axes[0, 1].set_title('DTW Distance Matrix', fontsize=13, fontweight='bold')\n",
        "axes[0, 1].set_xlabel('Series Index', fontsize=11)\n",
        "axes[0, 1].set_ylabel('Series Index', fontsize=11)\n",
        "\n",
        "# Plot 3: True clusters\n",
        "for cluster_id in range(3):\n",
        "    cluster_series = X_ts[y_true == cluster_id]\n",
        "    for series in cluster_series:\n",
        "        axes[1, 0].plot(series, alpha=0.5, linewidth=1, color=f'C{cluster_id}')\n",
        "    \n",
        "    # Plot cluster centroid\n",
        "    centroid = cluster_series.mean(axis=0)\n",
        "    axes[1, 0].plot(centroid, linewidth=3, color=f'C{cluster_id}', \n",
        "                    label=f'Cluster {cluster_id}', linestyle='--')\n",
        "\n",
        "axes[1, 0].set_title('True Clusters', fontsize=13, fontweight='bold')\n",
        "axes[1, 0].set_xlabel('Time Index', fontsize=11)\n",
        "axes[1, 0].set_ylabel('Value', fontsize=11)\n",
        "axes[1, 0].legend(fontsize=10)\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 4: Predicted clusters\n",
        "for cluster_id in range(3):\n",
        "    cluster_series = X_ts[y_pred == cluster_id]\n",
        "    for series in cluster_series:\n",
        "        axes[1, 1].plot(series, alpha=0.5, linewidth=1, color=f'C{cluster_id}')\n",
        "    \n",
        "    # Plot cluster centroid\n",
        "    if len(cluster_series) > 0:\n",
        "        centroid = cluster_series.mean(axis=0)\n",
        "        axes[1, 1].plot(centroid, linewidth=3, color=f'C{cluster_id}', \n",
        "                        label=f'Cluster {cluster_id}', linestyle='--')\n",
        "\n",
        "axes[1, 1].set_title(f'DTW Clustering (ARI={ari:.3f})', fontsize=13, fontweight='bold')\n",
        "axes[1, 1].set_xlabel('Time Index', fontsize=11)\n",
        "axes[1, 1].set_ylabel('Value', fontsize=11)\n",
        "axes[1, 1].legend(fontsize=10)\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.suptitle('DTW-based Time Series Clustering', fontsize=16, fontweight='bold', y=0.995)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüìä Key Insights:\")\n",
        "print(\"1. DTW effectively groups similar temporal patterns\")\n",
        "print(\"2. Dendrogram shows hierarchical structure\")\n",
        "print(\"3. Distance matrix reveals cluster boundaries\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# üï∏Ô∏è Part 4: Graph Clustering\n",
        "\n",
        "Graph clustering identifies communities or groups of densely connected nodes.\n",
        "\n",
        "## Key Concepts:\n",
        "- **Spectral Clustering**: Uses graph Laplacian eigendecomposition\n",
        "- **Louvain Algorithm**: Optimizes modularity for community detection\n",
        "- **GNN (Graph Neural Networks)**: Deep learning on graphs\n",
        "- **Graph Convolutional Networks**: Learn node embeddings\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù Exercise 7: Introduction to Graph Data\n",
        "\n",
        "### Concept\n",
        "\n",
        "A **graph** G = (V, E) consists of:\n",
        "- **Nodes (V)**: Entities (people, proteins, web pages)\n",
        "- **Edges (E)**: Relationships or connections\n",
        "- **Adjacency Matrix (A)**: A[i,j] = 1 if edge exists, 0 otherwise\n",
        "\n",
        "**Graph Properties**:\n",
        "- Degree: Number of connections per node\n",
        "- Clustering coefficient: Local connectivity\n",
        "- Connected components: Separated subgraphs\n",
        "\n",
        "### Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create sample graphs\n",
        "def create_sample_graphs():\n",
        "    \"\"\"Create various graph structures\"\"\"\n",
        "    \n",
        "    # 1. Karate Club - famous social network\n",
        "    G_karate = nx.karate_club_graph()\n",
        "    \n",
        "    # 2. Small world network\n",
        "    G_small_world = nx.watts_strogatz_graph(n=50, k=4, p=0.1, seed=42)\n",
        "    \n",
        "    # 3. Scale-free network (preferential attachment)\n",
        "    G_scale_free = nx.barabasi_albert_graph(n=50, m=2, seed=42)\n",
        "    \n",
        "    # 4. Community structure\n",
        "    sizes = [15, 15, 15]\n",
        "    probs = [[0.7, 0.1, 0.1],\n",
        "             [0.1, 0.7, 0.1],\n",
        "             [0.1, 0.1, 0.7]]\n",
        "    G_communities = nx.stochastic_block_model(sizes, probs, seed=42)\n",
        "    \n",
        "    return G_karate, G_small_world, G_scale_free, G_communities\n",
        "\n",
        "\n",
        "# Create graphs\n",
        "G_karate, G_small_world, G_scale_free, G_communities = create_sample_graphs()\n",
        "\n",
        "# Compute graph statistics\n",
        "graphs = {\n",
        "    'Karate Club': G_karate,\n",
        "    'Small World': G_small_world,\n",
        "    'Scale-Free': G_scale_free,\n",
        "    'Communities': G_communities\n",
        "}\n",
        "\n",
        "print(\"üìä Graph Statistics:\\n\")\n",
        "for name, G in graphs.items():\n",
        "    print(f\"{name}:\")\n",
        "    print(f\"  Nodes: {G.number_of_nodes()}\")\n",
        "    print(f\"  Edges: {G.number_of_edges()}\")\n",
        "    print(f\"  Avg Degree: {sum(dict(G.degree()).values()) / G.number_of_nodes():.2f}\")\n",
        "    print(f\"  Clustering Coef: {nx.average_clustering(G):.3f}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üìä Visualization: Graph Structures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
        "axes = axes.flatten()\n",
        "\n",
        "graph_list = [\n",
        "    (G_karate, 'Karate Club Network'),\n",
        "    (G_small_world, 'Small World Network'),\n",
        "    (G_scale_free, 'Scale-Free Network'),\n",
        "    (G_communities, 'Community Structure')\n",
        "]\n",
        "\n",
        "for idx, (G, title) in enumerate(graph_list):\n",
        "    # Compute layout\n",
        "    if idx == 0:  # Karate club\n",
        "        pos = nx.spring_layout(G, seed=42, k=0.5)\n",
        "    else:\n",
        "        pos = nx.spring_layout(G, seed=42)\n",
        "    \n",
        "    # Draw network\n",
        "    nx.draw_networkx_nodes(G, pos, \n",
        "                          node_color=range(G.number_of_nodes()),\n",
        "                          cmap='viridis',\n",
        "                          node_size=300,\n",
        "                          ax=axes[idx])\n",
        "    nx.draw_networkx_edges(G, pos, alpha=0.3, ax=axes[idx])\n",
        "    \n",
        "    axes[idx].set_title(title, fontsize=14, fontweight='bold')\n",
        "    axes[idx].axis('off')\n",
        "\n",
        "plt.suptitle('Different Graph Structures', fontsize=16, fontweight='bold', y=0.995)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüìä Key Insights:\")\n",
        "print(\"1. Different networks have distinct structural properties\")\n",
        "print(\"2. Community structure shows clear clusters\")\n",
        "print(\"3. Scale-free networks have hub nodes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìù Exercise 8: Spectral Clustering on Graphs\n",
        "\n",
        "### Concept\n",
        "\n",
        "**Spectral Clustering** uses the spectrum (eigenvalues) of the graph Laplacian:\n",
        "\n",
        "**Algorithm**:\n",
        "1. Compute Graph Laplacian: L = D - A\n",
        "   - D: Degree matrix (diagonal)\n",
        "   - A: Adjacency matrix\n",
        "2. Compute k smallest eigenvectors\n",
        "3. Use eigenvectors as features\n",
        "4. Apply K-means clustering\n",
        "\n",
        "**Advantages**:\n",
        "- Handles non-convex clusters\n",
        "- Based on graph cut objectives\n",
        "- Well-suited for community detection\n",
        "\n",
        "### Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def spectral_clustering_analysis(G, n_clusters=3):\n",
        "    \"\"\"\n",
        "    Perform spectral clustering on graph\n",
        "    \"\"\"\n",
        "    # Get adjacency matrix\n",
        "    A = nx.to_numpy_array(G)\n",
        "    \n",
        "    # Compute degree matrix\n",
        "    degrees = A.sum(axis=1)\n",
        "    D = np.diag(degrees)\n",
        "    \n",
        "    # Compute Laplacian\n",
        "    L = D - A\n",
        "    \n",
        "    # Compute normalized Laplacian\n",
        "    D_inv_sqrt = np.diag(1.0 / np.sqrt(degrees + 1e-10))\n",
        "    L_norm = np.eye(len(G)) - D_inv_sqrt @ A @ D_inv_sqrt\n",
        "    \n",
        "    # Eigendecomposition\n",
        "    eigenvalues, eigenvectors = np.linalg.eigh(L_norm)\n",
        "    \n",
        "    # Sort by eigenvalue\n",
        "    idx = eigenvalues.argsort()\n",
        "    eigenvalues = eigenvalues[idx]\n",
        "    eigenvectors = eigenvectors[:, idx]\n",
        "    \n",
        "    # Use k smallest eigenvectors (skip first - constant)\n",
        "    embedding = eigenvectors[:, 1:n_clusters+1]\n",
        "    \n",
        "    # K-means on embedding\n",
        "    from sklearn.cluster import KMeans\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
        "    labels = kmeans.fit_predict(embedding)\n",
        "    \n",
        "    return labels, eigenvalues, embedding\n",
        "\n",
        "\n",
        "# Apply spectral clustering to community graph\n",
        "labels_spectral, eigenvalues, embedding = spectral_clustering_analysis(G_communities, n_clusters=3)\n",
        "\n",
        "print(\"‚úÖ Spectral clustering completed!\")\n",
        "print(f\"\\nüìä Cluster sizes: {np.bincount(labels_spectral)}\")\n",
        "print(f\"\\nüî¢ First 10 eigenvalues: {eigenvalues[:10].round(3)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üìä Visualization: Spectral Clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# Plot 1: Eigenvalue spectrum\n",
        "axes[0, 0].plot(eigenvalues[:20], 'o-', linewidth=2, markersize=8)\n",
        "axes[0, 0].axvline(x=3, color='red', linestyle='--', label='Number of clusters', linewidth=2)\n",
        "axes[0, 0].set_xlabel('Index', fontsize=12)\n",
        "axes[0, 0].set_ylabel('Eigenvalue', fontsize=12)\n",
        "axes[0, 0].set_title('Graph Laplacian Spectrum', fontsize=13, fontweight='bold')\n",
        "axes[0, 0].legend(fontsize=10)\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Spectral embedding (2D)\n",
        "if embedding.shape[1] >= 2:\n",
        "    scatter = axes[0, 1].scatter(embedding[:, 0], embedding[:, 1], \n",
        "                                 c=labels_spectral, cmap='viridis', \n",
        "                                 s=100, alpha=0.7, edgecolors='black')\n",
        "    axes[0, 1].set_xlabel('1st Eigenvector', fontsize=12)\n",
        "    axes[0, 1].set_ylabel('2nd Eigenvector', fontsize=12)\n",
        "    axes[0, 1].set_title('Spectral Embedding Space', fontsize=13, fontweight='bold')\n",
        "    plt.colorbar(scatter, ax=axes[0, 1], label='Cluster')\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 3: Original graph with true communities\n",
        "pos = nx.spring_layout(G_communities, seed=42)\n",
        "true_labels = [G_communities.nodes[i]['block'] for i in G_communities.nodes()]\n",
        "nx.draw_networkx_nodes(G_communities, pos, node_color=true_labels, \n",
        "                      cmap='viridis', node_size=200, ax=axes[1, 0])\n",
        "nx.draw_networkx_edges(G_communities, pos, alpha=0.2, ax=axes[1, 0])\n",
        "axes[1, 0].set_title('True Communities', fontsize=13, fontweight='bold')\n",
        "axes[1, 0].axis('off')\n",
        "\n",
        "# Plot 4: Detected clusters\n",
        "nx.draw_networkx_nodes(G_communities, pos, node_color=labels_spectral, \n",
        "                      cmap='viridis', node_size=200, ax=axes[1, 1])\n",
        "nx.draw_networkx_edges(G_communities, pos, alpha=0.2, ax=axes[1, 1])\n",
        "\n",
        "# Compute accuracy\n",
        "ari = adjusted_rand_score(true_labels, labels_spectral)\n",
        "axes[1, 1].set_title(f'Spectral Clustering (ARI={ari:.3f})', fontsize=13, fontweight='bold')\n",
        "axes[1, 1].axis('off')\n",
        "\n",
        "plt.suptitle('Spectral Clustering Analysis', fontsize=16, fontweight='bold', y=0.995)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüìä Key Insights:\")\n",
        "print(\"1. Eigenvalue gap indicates number of clusters\")\n",
        "print(\"2. Spectral embedding separates communities\")\n",
        "print(\"3. Successfully detects graph structure\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìù Exercise 9: Louvain Community Detection\n",
        "\n",
        "### Concept\n",
        "\n",
        "**Louvain Algorithm** optimizes modularity Q:\n",
        "\n",
        "$$Q = \\frac{1}{2m} \\sum_{ij} \\left[A_{ij} - \\frac{k_i k_j}{2m}\\right] \\delta(c_i, c_j)$$\n",
        "\n",
        "Where:\n",
        "- $A_{ij}$: Adjacency matrix\n",
        "- $k_i$: Degree of node i\n",
        "- $m$: Total edges\n",
        "- $c_i$: Community of node i\n",
        "\n",
        "**Two-Phase Approach**:\n",
        "1. **Local optimization**: Move nodes to maximize modularity\n",
        "2. **Network aggregation**: Build new network from communities\n",
        "\n",
        "**Advantages**: Fast, scalable, hierarchical structure\n",
        "\n",
        "### Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple Louvain implementation\n",
        "# Note: In practice, use python-louvain library\n",
        "\n",
        "try:\n",
        "    import community as community_louvain\n",
        "    print(\"‚úÖ Using python-louvain library\")\n",
        "    has_louvain = True\n",
        "except ImportError:\n",
        "    print(\"‚ö†Ô∏è  python-louvain not installed, using greedy modularity\")\n",
        "    has_louvain = False\n",
        "\n",
        "def detect_communities(G):\n",
        "    \"\"\"Detect communities using available method\"\"\"\n",
        "    if has_louvain:\n",
        "        # Use Louvain algorithm\n",
        "        partition = community_louvain.best_partition(G)\n",
        "        communities = list(partition.values())\n",
        "    else:\n",
        "        # Use greedy modularity communities\n",
        "        from networkx.algorithms import community as nx_community\n",
        "        communities_gen = nx_community.greedy_modularity_communities(G)\n",
        "        \n",
        "        # Convert to node labels\n",
        "        node_to_community = {}\n",
        "        for idx, comm in enumerate(communities_gen):\n",
        "            for node in comm:\n",
        "                node_to_community[node] = idx\n",
        "        \n",
        "        communities = [node_to_community[i] for i in range(G.number_of_nodes())]\n",
        "    \n",
        "    return np.array(communities)\n",
        "\n",
        "\n",
        "# Apply to Karate Club (famous example)\n",
        "communities_karate = detect_communities(G_karate)\n",
        "\n",
        "# Compute modularity\n",
        "def compute_modularity(G, communities):\n",
        "    \"\"\"Compute modularity score\"\"\"\n",
        "    from networkx.algorithms import community as nx_community\n",
        "    \n",
        "    # Convert to community sets\n",
        "    unique_comms = set(communities)\n",
        "    community_sets = [set(np.where(communities == c)[0]) for c in unique_comms]\n",
        "    \n",
        "    modularity = nx_community.modularity(G, community_sets)\n",
        "    return modularity\n",
        "\n",
        "modularity = compute_modularity(G_karate, communities_karate)\n",
        "\n",
        "print(f\"\\n‚úÖ Community detection completed!\")\n",
        "print(f\"üìä Number of communities: {len(set(communities_karate))}\")\n",
        "print(f\"üìä Modularity Q: {modularity:.4f}\")\n",
        "print(f\"üìä Community sizes: {np.bincount(communities_karate)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üìä Visualization: Louvain Communities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "# Layout\n",
        "pos = nx.spring_layout(G_karate, seed=42, k=0.5)\n",
        "\n",
        "# Plot 1: Ground truth (known factions)\n",
        "true_clubs = [G_karate.nodes[i]['club'] for i in G_karate.nodes()]\n",
        "true_labels = [0 if club == 'Mr. Hi' else 1 for club in true_clubs]\n",
        "\n",
        "nx.draw_networkx_nodes(G_karate, pos, node_color=true_labels,\n",
        "                      cmap='coolwarm', node_size=500,\n",
        "                      edgecolors='black', linewidths=2,\n",
        "                      ax=axes[0])\n",
        "nx.draw_networkx_edges(G_karate, pos, alpha=0.3, ax=axes[0])\n",
        "nx.draw_networkx_labels(G_karate, pos, font_size=8, ax=axes[0])\n",
        "axes[0].set_title('Karate Club: True Factions', fontsize=14, fontweight='bold')\n",
        "axes[0].axis('off')\n",
        "\n",
        "# Plot 2: Detected communities\n",
        "nx.draw_networkx_nodes(G_karate, pos, node_color=communities_karate,\n",
        "                      cmap='viridis', node_size=500,\n",
        "                      edgecolors='black', linewidths=2,\n",
        "                      ax=axes[1])\n",
        "nx.draw_networkx_edges(G_karate, pos, alpha=0.3, ax=axes[1])\n",
        "nx.draw_networkx_labels(G_karate, pos, font_size=8, ax=axes[1])\n",
        "\n",
        "ari_karate = adjusted_rand_score(true_labels, communities_karate)\n",
        "axes[1].set_title(f'Detected Communities\\n(Modularity={modularity:.3f}, ARI={ari_karate:.3f})',\n",
        "                 fontsize=14, fontweight='bold')\n",
        "axes[1].axis('off')\n",
        "\n",
        "plt.suptitle('Louvain Community Detection', fontsize=16, fontweight='bold', y=0.98)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüìä Key Insights:\")\n",
        "print(\"1. Louvain successfully detects community structure\")\n",
        "print(\"2. High modularity indicates strong communities\")\n",
        "print(\"3. Matches known factions in Karate Club\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# üöÄ Part 5: Advanced Topics\n",
        "\n",
        "Modern clustering approaches for complex scenarios:\n",
        "\n",
        "## Topics:\n",
        "- **Multi-modal Clustering**: Combining different data types\n",
        "- **Large-Scale Clustering**: Billions of samples\n",
        "- **Deep Clustering**: End-to-end learned representations\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù Exercise 10: Multi-modal Clustering\n",
        "\n",
        "### Concept\n",
        "\n",
        "**Multi-modal clustering** handles data with multiple modalities (types):\n",
        "\n",
        "**Approaches**:\n",
        "1. **Early Fusion**: Concatenate features from all modalities\n",
        "2. **Late Fusion**: Cluster each modality separately, then combine\n",
        "3. **Deep Multi-modal**: Learn shared representation space\n",
        "\n",
        "**Challenge**: Different feature spaces, scales, and semantics\n",
        "\n",
        "**Applications**:\n",
        "- Video (visual + audio)\n",
        "- Medical (images + reports)\n",
        "- E-commerce (product images + descriptions)\n",
        "\n",
        "### Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_multimodal_dataset(n_samples=200, n_clusters=4):\n",
        "    \"\"\"\n",
        "    Create synthetic multi-modal dataset\n",
        "    \n",
        "    Returns:\n",
        "        visual_features: Image-like features (2D)\n",
        "        text_features: Text-like features (2D)\n",
        "        true_labels: Ground truth clusters\n",
        "    \"\"\"\n",
        "    # Generate cluster centers\n",
        "    np.random.seed(42)\n",
        "    \n",
        "    # Visual modality (e.g., color, shape)\n",
        "    visual_centers = np.random.randn(n_clusters, 2) * 3\n",
        "    visual_features = []\n",
        "    \n",
        "    # Text modality (e.g., semantic embeddings)\n",
        "    text_centers = np.random.randn(n_clusters, 2) * 3\n",
        "    text_features = []\n",
        "    \n",
        "    true_labels = []\n",
        "    \n",
        "    for cluster_id in range(n_clusters):\n",
        "        n_per_cluster = n_samples // n_clusters\n",
        "        \n",
        "        # Visual features\n",
        "        visual = visual_centers[cluster_id] + np.random.randn(n_per_cluster, 2) * 0.5\n",
        "        visual_features.append(visual)\n",
        "        \n",
        "        # Text features (slightly different distribution)\n",
        "        text = text_centers[cluster_id] + np.random.randn(n_per_cluster, 2) * 0.7\n",
        "        text_features.append(text)\n",
        "        \n",
        "        true_labels.extend([cluster_id] * n_per_cluster)\n",
        "    \n",
        "    visual_features = np.vstack(visual_features)\n",
        "    text_features = np.vstack(text_features)\n",
        "    true_labels = np.array(true_labels)\n",
        "    \n",
        "    return visual_features, text_features, true_labels\n",
        "\n",
        "\n",
        "# Create dataset\n",
        "visual_features, text_features, true_labels = create_multimodal_dataset()\n",
        "\n",
        "# Normalize features\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler_v = StandardScaler()\n",
        "scaler_t = StandardScaler()\n",
        "\n",
        "visual_norm = scaler_v.fit_transform(visual_features)\n",
        "text_norm = scaler_t.fit_transform(text_features)\n",
        "\n",
        "print(f\"‚úÖ Created multi-modal dataset:\")\n",
        "print(f\"   Visual features: {visual_features.shape}\")\n",
        "print(f\"   Text features: {text_features.shape}\")\n",
        "print(f\"   True clusters: {len(set(true_labels))}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Three fusion approaches\n",
        "\n",
        "# 1. Early Fusion: Concatenate features\n",
        "early_fusion_features = np.hstack([visual_norm, text_norm])\n",
        "kmeans_early = KMeans(n_clusters=4, random_state=42, n_init=10)\n",
        "labels_early = kmeans_early.fit_predict(early_fusion_features)\n",
        "ari_early = adjusted_rand_score(true_labels, labels_early)\n",
        "\n",
        "# 2. Late Fusion: Cluster separately, then combine\n",
        "kmeans_visual = KMeans(n_clusters=4, random_state=42, n_init=10)\n",
        "kmeans_text = KMeans(n_clusters=4, random_state=42, n_init=10)\n",
        "\n",
        "labels_visual = kmeans_visual.fit_predict(visual_norm)\n",
        "labels_text = kmeans_text.fit_predict(text_norm)\n",
        "\n",
        "# Combine by voting (simple approach)\n",
        "labels_late = []\n",
        "for v, t in zip(labels_visual, labels_text):\n",
        "    labels_late.append(v if np.random.rand() > 0.5 else t)\n",
        "labels_late = np.array(labels_late)\n",
        "ari_late = adjusted_rand_score(true_labels, labels_late)\n",
        "\n",
        "# 3. Weighted Fusion: Weight modalities by reliability\n",
        "# Use silhouette scores as weights\n",
        "from sklearn.metrics import silhouette_score\n",
        "sil_visual = silhouette_score(visual_norm, labels_visual)\n",
        "sil_text = silhouette_score(text_norm, labels_text)\n",
        "\n",
        "# Normalize weights\n",
        "weight_visual = sil_visual / (sil_visual + sil_text)\n",
        "weight_text = sil_text / (sil_visual + sil_text)\n",
        "\n",
        "# Weighted feature combination\n",
        "weighted_features = np.hstack([\n",
        "    visual_norm * weight_visual,\n",
        "    text_norm * weight_text\n",
        "])\n",
        "kmeans_weighted = KMeans(n_clusters=4, random_state=42, n_init=10)\n",
        "labels_weighted = kmeans_weighted.fit_predict(weighted_features)\n",
        "ari_weighted = adjusted_rand_score(true_labels, labels_weighted)\n",
        "\n",
        "print(\"\\nüìä Fusion Strategy Comparison:\")\n",
        "print(f\"\\nEarly Fusion (concatenate):\")\n",
        "print(f\"  ARI: {ari_early:.4f}\")\n",
        "print(f\"\\nLate Fusion (combine votes):\")\n",
        "print(f\"  ARI: {ari_late:.4f}\")\n",
        "print(f\"\\nWeighted Fusion:\")\n",
        "print(f\"  Visual weight: {weight_visual:.3f}\")\n",
        "print(f\"  Text weight: {weight_text:.3f}\")\n",
        "print(f\"  ARI: {ari_weighted:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üìä Visualization: Multi-modal Clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
        "\n",
        "# Row 1: Individual modalities\n",
        "# Visual features - True labels\n",
        "scatter1 = axes[0, 0].scatter(visual_features[:, 0], visual_features[:, 1],\n",
        "                              c=true_labels, cmap='viridis', s=50, alpha=0.7, edgecolors='black')\n",
        "axes[0, 0].set_title('Visual Features (Ground Truth)', fontsize=12, fontweight='bold')\n",
        "axes[0, 0].set_xlabel('Visual Dim 1')\n",
        "axes[0, 0].set_ylabel('Visual Dim 2')\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Text features - True labels\n",
        "scatter2 = axes[0, 1].scatter(text_features[:, 0], text_features[:, 1],\n",
        "                              c=true_labels, cmap='viridis', s=50, alpha=0.7, edgecolors='black')\n",
        "axes[0, 1].set_title('Text Features (Ground Truth)', fontsize=12, fontweight='bold')\n",
        "axes[0, 1].set_xlabel('Text Dim 1')\n",
        "axes[0, 1].set_ylabel('Text Dim 2')\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Combined view\n",
        "axes[0, 2].scatter(visual_features[:, 0], visual_features[:, 1],\n",
        "                  c=true_labels, cmap='viridis', s=50, alpha=0.5, \n",
        "                  marker='o', label='Visual', edgecolors='black')\n",
        "axes[0, 2].scatter(text_features[:, 0], text_features[:, 1],\n",
        "                  c=true_labels, cmap='viridis', s=50, alpha=0.5, \n",
        "                  marker='^', label='Text', edgecolors='black')\n",
        "axes[0, 2].set_title('Both Modalities', fontsize=12, fontweight='bold')\n",
        "axes[0, 2].legend()\n",
        "axes[0, 2].grid(True, alpha=0.3)\n",
        "\n",
        "# Row 2: Fusion results\n",
        "# Early fusion\n",
        "axes[1, 0].scatter(visual_features[:, 0], visual_features[:, 1],\n",
        "                  c=labels_early, cmap='viridis', s=50, alpha=0.7, edgecolors='black')\n",
        "axes[1, 0].set_title(f'Early Fusion\\n(ARI={ari_early:.3f})', fontsize=12, fontweight='bold')\n",
        "axes[1, 0].set_xlabel('Visual Dim 1')\n",
        "axes[1, 0].set_ylabel('Visual Dim 2')\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Late fusion\n",
        "axes[1, 1].scatter(text_features[:, 0], text_features[:, 1],\n",
        "                  c=labels_late, cmap='viridis', s=50, alpha=0.7, edgecolors='black')\n",
        "axes[1, 1].set_title(f'Late Fusion\\n(ARI={ari_late:.3f})', fontsize=12, fontweight='bold')\n",
        "axes[1, 1].set_xlabel('Text Dim 1')\n",
        "axes[1, 1].set_ylabel('Text Dim 2')\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Weighted fusion\n",
        "axes[1, 2].scatter(visual_features[:, 0], visual_features[:, 1],\n",
        "                  c=labels_weighted, cmap='viridis', s=50, alpha=0.7, edgecolors='black')\n",
        "axes[1, 2].set_title(f'Weighted Fusion\\n(ARI={ari_weighted:.3f})', \n",
        "                    fontsize=12, fontweight='bold')\n",
        "axes[1, 2].set_xlabel('Visual Dim 1')\n",
        "axes[1, 2].set_ylabel('Visual Dim 2')\n",
        "axes[1, 2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.suptitle('Multi-modal Clustering Comparison', fontsize=16, fontweight='bold', y=0.995)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüìä Key Insights:\")\n",
        "print(\"1. Different modalities capture complementary information\")\n",
        "print(\"2. Fusion strategy matters for performance\")\n",
        "print(\"3. Weighted fusion can improve results\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìù Exercise 11: Large-Scale Clustering\n",
        "\n",
        "### Concept\n",
        "\n",
        "**Challenges** with large-scale data:\n",
        "- Billions of samples\n",
        "- High dimensionality\n",
        "- Memory constraints\n",
        "- Computation time\n",
        "\n",
        "**Scalability Techniques**:\n",
        "1. **Mini-batch K-means**: Process streaming data in batches\n",
        "2. **Approximate NN**: Fast similarity search (FAISS)\n",
        "3. **Sampling**: Use representative subset (CoreSets)\n",
        "4. **GPU Acceleration**: Parallel computation\n",
        "\n",
        "**Trade-off**: Speed ‚öñÔ∏è Accuracy\n",
        "\n",
        "### Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.cluster import MiniBatchKMeans\n",
        "import time\n",
        "\n",
        "# Generate large dataset\n",
        "print(\"‚è≥ Generating large-scale dataset...\")\n",
        "n_samples_large = 50000\n",
        "n_features = 50\n",
        "n_clusters = 10\n",
        "\n",
        "X_large, y_large = make_blobs(\n",
        "    n_samples=n_samples_large,\n",
        "    n_features=n_features,\n",
        "    centers=n_clusters,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Dataset created: {X_large.shape}\")\n",
        "print(f\"   Size in memory: {X_large.nbytes / 1024**2:.2f} MB\")\n",
        "\n",
        "# Compare standard K-means vs Mini-batch K-means\n",
        "print(\"\\nüèÉ Running clustering algorithms...\\n\")\n",
        "\n",
        "# Standard K-means\n",
        "print(\"1. Standard K-means:\")\n",
        "start = time.time()\n",
        "kmeans_standard = KMeans(n_clusters=n_clusters, random_state=42, n_init=3, max_iter=100)\n",
        "labels_standard = kmeans_standard.fit_predict(X_large)\n",
        "time_standard = time.time() - start\n",
        "inertia_standard = kmeans_standard.inertia_\n",
        "ari_standard = adjusted_rand_score(y_large, labels_standard)\n",
        "print(f\"   Time: {time_standard:.2f}s\")\n",
        "print(f\"   Inertia: {inertia_standard:.2f}\")\n",
        "print(f\"   ARI: {ari_standard:.4f}\")\n",
        "\n",
        "# Mini-batch K-means\n",
        "print(\"\\n2. Mini-batch K-means:\")\n",
        "start = time.time()\n",
        "kmeans_minibatch = MiniBatchKMeans(\n",
        "    n_clusters=n_clusters, \n",
        "    random_state=42,\n",
        "    batch_size=1000,\n",
        "    max_iter=100,\n",
        "    n_init=3\n",
        ")\n",
        "labels_minibatch = kmeans_minibatch.fit_predict(X_large)\n",
        "time_minibatch = time.time() - start\n",
        "inertia_minibatch = kmeans_minibatch.inertia_\n",
        "ari_minibatch = adjusted_rand_score(y_large, labels_minibatch)\n",
        "print(f\"   Time: {time_minibatch:.2f}s\")\n",
        "print(f\"   Inertia: {inertia_minibatch:.2f}\")\n",
        "print(f\"   ARI: {ari_minibatch:.4f}\")\n",
        "\n",
        "# Sampling approach\n",
        "print(\"\\n3. Sampling + Standard K-means:\")\n",
        "sample_size = 5000\n",
        "indices = np.random.choice(n_samples_large, sample_size, replace=False)\n",
        "X_sample = X_large[indices]\n",
        "y_sample = y_large[indices]\n",
        "\n",
        "start = time.time()\n",
        "kmeans_sample = KMeans(n_clusters=n_clusters, random_state=42, n_init=3)\n",
        "kmeans_sample.fit(X_sample)\n",
        "# Predict on full dataset using learned centroids\n",
        "labels_sample = kmeans_sample.predict(X_large)\n",
        "time_sample = time.time() - start\n",
        "ari_sample = adjusted_rand_score(y_large, labels_sample)\n",
        "print(f\"   Time: {time_sample:.2f}s\")\n",
        "print(f\"   Sample size: {sample_size} ({sample_size/n_samples_large*100:.1f}%)\")\n",
        "print(f\"   ARI: {ari_sample:.4f}\")\n",
        "\n",
        "# Summary\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìä Performance Summary:\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Speedup (Mini-batch vs Standard): {time_standard/time_minibatch:.2f}x\")\n",
        "print(f\"Speedup (Sampling vs Standard): {time_standard/time_sample:.2f}x\")\n",
        "print(f\"\\nAccuracy (ARI) comparison:\")\n",
        "print(f\"  Standard:   {ari_standard:.4f}\")\n",
        "print(f\"  Mini-batch: {ari_minibatch:.4f} ({(ari_minibatch/ari_standard-1)*100:+.1f}%)\")\n",
        "print(f\"  Sampling:   {ari_sample:.4f} ({(ari_sample/ari_standard-1)*100:+.1f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üìä Visualization: Scalability Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
        "\n",
        "# Plot 1: Time comparison\n",
        "methods = ['Standard\\nK-means', 'Mini-batch\\nK-means', 'Sampling\\n(10%)']\n",
        "times = [time_standard, time_minibatch, time_sample]\n",
        "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
        "\n",
        "bars = axes[0].bar(methods, times, color=colors, edgecolor='black', linewidth=2)\n",
        "axes[0].set_ylabel('Time (seconds)', fontsize=12)\n",
        "axes[0].set_title('Computation Time', fontsize=14, fontweight='bold')\n",
        "axes[0].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Add value labels\n",
        "for bar, time_val in zip(bars, times):\n",
        "    height = bar.get_height()\n",
        "    axes[0].text(bar.get_x() + bar.get_width()/2., height,\n",
        "                f'{time_val:.2f}s', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# Plot 2: ARI comparison\n",
        "aris = [ari_standard, ari_minibatch, ari_sample]\n",
        "bars = axes[1].bar(methods, aris, color=colors, edgecolor='black', linewidth=2)\n",
        "axes[1].set_ylabel('ARI Score', fontsize=12)\n",
        "axes[1].set_title('Clustering Quality (ARI)', fontsize=14, fontweight='bold')\n",
        "axes[1].set_ylim([0, 1])\n",
        "axes[1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Add value labels\n",
        "for bar, ari in zip(bars, aris):\n",
        "    height = bar.get_height()\n",
        "    axes[1].text(bar.get_x() + bar.get_width()/2., height,\n",
        "                f'{ari:.3f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# Plot 3: Trade-off (Speed vs Accuracy)\n",
        "speedups = [1.0, time_standard/time_minibatch, time_standard/time_sample]\n",
        "ari_relative = [ari/ari_standard for ari in aris]\n",
        "\n",
        "axes[2].scatter(speedups, ari_relative, s=500, c=colors, \n",
        "               edgecolors='black', linewidths=2, alpha=0.7)\n",
        "\n",
        "# Add labels\n",
        "for i, method in enumerate(['Standard', 'Mini-batch', 'Sampling']):\n",
        "    axes[2].annotate(method, (speedups[i], ari_relative[i]),\n",
        "                    xytext=(10, 10), textcoords='offset points',\n",
        "                    fontsize=10, fontweight='bold',\n",
        "                    bbox=dict(boxstyle='round,pad=0.5', facecolor='yellow', alpha=0.3))\n",
        "\n",
        "axes[2].axhline(y=1.0, color='red', linestyle='--', linewidth=2, alpha=0.5, label='Baseline')\n",
        "axes[2].set_xlabel('Speedup (x times faster)', fontsize=12)\n",
        "axes[2].set_ylabel('Relative ARI (vs Standard)', fontsize=12)\n",
        "axes[2].set_title('Speed vs Accuracy Trade-off', fontsize=14, fontweight='bold')\n",
        "axes[2].legend(fontsize=10)\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.suptitle('Large-Scale Clustering: Performance Analysis', \n",
        "             fontsize=16, fontweight='bold', y=1.00)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüìä Key Insights:\")\n",
        "print(\"1. Mini-batch K-means: Significant speedup with minimal accuracy loss\")\n",
        "print(\"2. Sampling: Fastest but may sacrifice some accuracy\")\n",
        "print(\"3. Choose method based on speed/accuracy requirements\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# üéì Summary & Key Takeaways\n",
        "\n",
        "## What We Learned\n",
        "\n",
        "### üîÑ Self-Supervised Learning\n",
        "- **Contrastive learning** learns by comparing positive/negative pairs\n",
        "- **SimCLR** uses strong augmentations to create views\n",
        "- **InfoNCE loss** with temperature scaling\n",
        "- Enables learning from unlabeled data at scale\n",
        "\n",
        "### üìà Time Series Clustering\n",
        "- **DTW (Dynamic Time Warping)** handles temporal misalignment\n",
        "- More robust than Euclidean distance for time series\n",
        "- **K-Shape** offers faster alternative with shape-based similarity\n",
        "- Applications: ECG, stocks, sensor data\n",
        "\n",
        "### üï∏Ô∏è Graph Clustering\n",
        "- **Spectral clustering** uses eigendecomposition\n",
        "- **Louvain** optimizes modularity for communities\n",
        "- Graph structure provides rich information\n",
        "- Applications: Social networks, biological networks\n",
        "\n",
        "### üöÄ Advanced Topics\n",
        "- **Multi-modal**: Combine different data types\n",
        "- **Large-scale**: Trade-off speed vs accuracy\n",
        "- **Mini-batch** and **sampling** for scalability\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Best Practices\n",
        "\n",
        "1. **Choose appropriate distance metric**\n",
        "   - Euclidean for standard data\n",
        "   - DTW for time series\n",
        "   - Graph distances for networks\n",
        "\n",
        "2. **Preprocess data carefully**\n",
        "   - Normalization/standardization\n",
        "   - Handle missing values\n",
        "   - Feature engineering\n",
        "\n",
        "3. **Validate results**\n",
        "   - Multiple metrics (Silhouette, ARI, modularity)\n",
        "   - Visual inspection\n",
        "   - Domain knowledge\n",
        "\n",
        "4. **Consider scalability early**\n",
        "   - Start with sampling for prototyping\n",
        "   - Use mini-batch for large datasets\n",
        "   - Leverage GPU when available\n",
        "\n",
        "---\n",
        "\n",
        "## üìö Further Reading\n",
        "\n",
        "### Papers\n",
        "- **SimCLR**: Chen et al. (2020) - \"A Simple Framework for Contrastive Learning\"\n",
        "- **DTW**: Sakoe & Chiba (1978) - \"Dynamic Programming Algorithm Optimization\"\n",
        "- **Louvain**: Blondel et al. (2008) - \"Fast Unfolding of Communities\"\n",
        "- **Spectral Clustering**: Ng et al. (2002) - \"On Spectral Clustering\"\n",
        "\n",
        "### Libraries\n",
        "- **Scikit-learn**: General clustering algorithms\n",
        "- **TSlearn**: Time series clustering\n",
        "- **NetworkX**: Graph algorithms\n",
        "- **PyTorch Geometric**: Graph neural networks\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ Next Steps\n",
        "\n",
        "1. **Apply to your own data**\n",
        "   - Start with exploratory analysis\n",
        "   - Try multiple methods\n",
        "   - Compare results\n",
        "\n",
        "2. **Experiment with parameters**\n",
        "   - Number of clusters\n",
        "   - Distance metrics\n",
        "   - Preprocessing steps\n",
        "\n",
        "3. **Combine approaches**\n",
        "   - Ensemble clustering\n",
        "   - Multi-view learning\n",
        "   - Hierarchical methods\n",
        "\n",
        "4. **Stay updated**\n",
        "   - Latest research papers\n",
        "   - New algorithms and tools\n",
        "   - Community discussions\n",
        "\n",
        "---\n",
        "\n",
        "## üí° Final Challenge\n",
        "\n",
        "**Choose a real-world dataset and**:\n",
        "1. Apply at least 3 different clustering methods\n",
        "2. Compare their performance quantitatively\n",
        "3. Visualize the results\n",
        "4. Provide actionable insights\n",
        "\n",
        "**Suggested datasets**:\n",
        "- UCI Machine Learning Repository\n",
        "- Kaggle datasets\n",
        "- Your own domain data\n",
        "\n",
        "---\n",
        "\n",
        "### Thank you for completing this notebook! üéâ\n",
        "\n",
        "**Questions or feedback?**\n",
        "- Email: homin.park@ghent.ac.kr\n",
        "- Based on Lecture 18: Advanced Unsupervised Learning\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}