{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deep Learning Fundamentals: From Shallow to Deep Networks\n",
        "## Interactive Learning Notebook\n\n",
        "**Based on Lecture 7: From Logistic Regression to Multi-layer Perceptrons**\n\n",
        "This notebook provides hands-on practice with deep learning concepts including:\n",
        "- Understanding the limitations of shallow networks\n",
        "- Implementing deep neural networks from scratch\n",
        "- Exploring gradient problems and solutions\n",
        "- Working with modern activation functions\n",
        "- Visualizing network behavior and performance\n\n",
        "### Learning Objectives\n",
        "1. Understand why deep networks are more powerful than shallow ones\n",
        "2. Implement and compare different activation functions\n",
        "3. Diagnose and solve gradient problems\n",
        "4. Build practical deep learning models\n\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Environment Setup and Imports\n",
        "Let's import all necessary libraries for our deep learning experiments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Deep Learning imports\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models, optimizers, callbacks\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import make_moons, make_circles, make_classification\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# Interactive visualizations\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette('husl')\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "print(f'TensorFlow version: {tf.__version__}')\n",
        "print(f'Keras version: {keras.__version__}')\n",
        "print('Setup complete!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Exercise 1: Understanding Shallow vs Deep Networks\n",
        "\n",
        "### Concept: Universal Approximation Theorem\n",
        "The Universal Approximation Theorem states that a shallow network with a single hidden layer can theoretically approximate any continuous function. However, it might need exponentially many neurons to do so efficiently.\n",
        "\n",
        "Let's demonstrate why deep networks are more parameter-efficient than shallow ones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate a complex non-linear dataset\n",
        "def generate_spiral_data(n_samples=1000, n_classes=3, noise=0.2):\n",
        "    \"\"\"Generate spiral dataset for classification\"\"\"\n",
        "    np.random.seed(42)\n",
        "    n_per_class = n_samples // n_classes\n",
        "    \n",
        "    X = []\n",
        "    y = []\n",
        "    \n",
        "    for class_idx in range(n_classes):\n",
        "        theta = np.linspace(0, 4 * np.pi, n_per_class) + (class_idx * 2 * np.pi / n_classes)\n",
        "        radius = np.linspace(0, 1, n_per_class)\n",
        "        \n",
        "        x1 = radius * np.cos(theta) + np.random.randn(n_per_class) * noise\n",
        "        x2 = radius * np.sin(theta) + np.random.randn(n_per_class) * noise\n",
        "        \n",
        "        X.append(np.column_stack([x1, x2]))\n",
        "        y.append(np.full(n_per_class, class_idx))\n",
        "    \n",
        "    return np.vstack(X), np.hstack(y)\n",
        "\n",
        "# Generate data\n",
        "X_spiral, y_spiral = generate_spiral_data()\n",
        "\n",
        "# Visualize the dataset\n",
        "fig = go.Figure()\n",
        "for class_idx in range(3):\n",
        "    mask = y_spiral == class_idx\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=X_spiral[mask, 0],\n",
        "        y=X_spiral[mask, 1],\n",
        "        mode='markers',\n",
        "        name=f'Class {class_idx}',\n",
        "        marker=dict(size=5)\n",
        "    ))\n",
        "\n",
        "fig.update_layout(\n",
        "    title='Complex Spiral Dataset for Classification',\n",
        "    xaxis_title='Feature 1',\n",
        "    yaxis_title='Feature 2',\n",
        "    height=500,\n",
        "    width=700\n",
        ")\n",
        "fig.show()\n",
        "\n",
        "print(f\"Dataset shape: X={X_spiral.shape}, y={y_spiral.shape}\")\n",
        "print(f\"Number of classes: {len(np.unique(y_spiral))}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build and compare shallow vs deep networks\n",
        "def build_shallow_network(input_dim, hidden_units, n_classes):\n",
        "    \"\"\"Build a shallow network with one hidden layer\"\"\"\n",
        "    model = models.Sequential([\n",
        "        layers.Dense(hidden_units, activation='relu', input_shape=(input_dim,)),\n",
        "        layers.Dense(n_classes, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def build_deep_network(input_dim, layer_sizes, n_classes):\n",
        "    \"\"\"Build a deep network with multiple hidden layers\"\"\"\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Dense(layer_sizes[0], activation='relu', input_shape=(input_dim,)))\n",
        "    \n",
        "    for size in layer_sizes[1:]:\n",
        "        model.add(layers.Dense(size, activation='relu'))\n",
        "    \n",
        "    model.add(layers.Dense(n_classes, activation='softmax'))\n",
        "    return model\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_spiral, y_spiral, test_size=0.2, random_state=42, stratify=y_spiral\n",
        ")\n",
        "\n",
        "# Build models\n",
        "shallow_model = build_shallow_network(2, 1000, 3)  # 1000 hidden units\n",
        "deep_model = build_deep_network(2, [100, 100, 100], 3)  # 3 layers of 100 units each\n",
        "\n",
        "# Compile models\n",
        "for model in [shallow_model, deep_model]:\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "# Display model summaries\n",
        "print(\"SHALLOW NETWORK (1 hidden layer with 1000 units):\")\n",
        "print(f\"Total parameters: {shallow_model.count_params():,}\")\n",
        "print(\"\\nDEEP NETWORK (3 hidden layers with 100 units each):\")\n",
        "print(f\"Total parameters: {deep_model.count_params():,}\")\n",
        "print(f\"\\nParameter efficiency: Deep network uses {deep_model.count_params()/shallow_model.count_params():.1%} of shallow network's parameters\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train and compare both models\n",
        "history_shallow = shallow_model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    validation_split=0.2,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "history_deep = deep_model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    validation_split=0.2,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "# Plot training history\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "ax1.plot(history_shallow.history['loss'], label='Shallow - Train', linewidth=2)\n",
        "ax1.plot(history_shallow.history['val_loss'], label='Shallow - Val', linewidth=2, linestyle='--')\n",
        "ax1.plot(history_deep.history['loss'], label='Deep - Train', linewidth=2)\n",
        "ax1.plot(history_deep.history['val_loss'], label='Deep - Val', linewidth=2, linestyle='--')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.set_title('Training Loss Comparison')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "ax2.plot(history_shallow.history['accuracy'], label='Shallow - Train', linewidth=2)\n",
        "ax2.plot(history_shallow.history['val_accuracy'], label='Shallow - Val', linewidth=2, linestyle='--')\n",
        "ax2.plot(history_deep.history['accuracy'], label='Deep - Train', linewidth=2)\n",
        "ax2.plot(history_deep.history['val_accuracy'], label='Deep - Val', linewidth=2, linestyle='--')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Accuracy')\n",
        "ax2.set_title('Training Accuracy Comparison')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate on test set\n",
        "shallow_acc = shallow_model.evaluate(X_test, y_test, verbose=0)[1]\n",
        "deep_acc = deep_model.evaluate(X_test, y_test, verbose=0)[1]\n",
        "\n",
        "print(f\"\\nTest Accuracy:\")\n",
        "print(f\"Shallow Network: {shallow_acc:.3f}\")\n",
        "print(f\"Deep Network: {deep_acc:.3f}\")\n",
        "print(f\"\\nDeep network achieves {(deep_acc/shallow_acc - 1)*100:.1f}% better accuracy with {(1 - deep_model.count_params()/shallow_model.count_params())*100:.1f}% fewer parameters!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ðŸ” Key Insights:\n",
        "- **Parameter Efficiency**: Deep networks achieve better performance with significantly fewer parameters\n",
        "- **Hierarchical Learning**: Deep networks learn features hierarchically, from simple to complex\n",
        "- **Generalization**: Deep networks often generalize better despite having the capacity to overfit\n",
        "\n",
        "### ðŸ’¡ Your Turn:\n",
        "Modify the network architectures above:\n",
        "1. Try different numbers of layers in the deep network (2, 4, 5 layers)\n",
        "2. Experiment with different layer widths (50, 200, 500 units)\n",
        "3. Compare the decision boundaries using the visualization function below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Exercise 2: Visualizing the Vanishing Gradient Problem\n",
        "\n",
        "### Concept: Gradient Flow in Deep Networks\n",
        "During backpropagation, gradients are multiplied through many layers. When using certain activation functions like sigmoid or tanh, these gradients can vanish (approach zero) or explode (become very large)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implement different activation functions\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    s = sigmoid(x)\n",
        "    return s * (1 - s)\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def tanh_derivative(x):\n",
        "    return 1 - np.tanh(x) ** 2\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return (x > 0).astype(float)\n",
        "\n",
        "# Visualize activation functions and their derivatives\n",
        "x = np.linspace(-5, 5, 1000)\n",
        "\n",
        "fig = make_subplots(\n",
        "    rows=2, cols=3,\n",
        "    subplot_titles=('Sigmoid', 'Tanh', 'ReLU',\n",
        "                   'Sigmoid Derivative', 'Tanh Derivative', 'ReLU Derivative')\n",
        ")\n",
        "\n",
        "# Activation functions\n",
        "fig.add_trace(go.Scatter(x=x, y=sigmoid(x), name='Sigmoid'), row=1, col=1)\n",
        "fig.add_trace(go.Scatter(x=x, y=tanh(x), name='Tanh'), row=1, col=2)\n",
        "fig.add_trace(go.Scatter(x=x, y=relu(x), name='ReLU'), row=1, col=3)\n",
        "\n",
        "# Derivatives\n",
        "fig.add_trace(go.Scatter(x=x, y=sigmoid_derivative(x), name='Sigmoid\\''), row=2, col=1)\n",
        "fig.add_trace(go.Scatter(x=x, y=tanh_derivative(x), name='Tanh\\''), row=2, col=2)\n",
        "fig.add_trace(go.Scatter(x=x, y=relu_derivative(x), name='ReLU\\''), row=2, col=3)\n",
        "\n",
        "fig.update_layout(height=600, showlegend=False, title_text=\"Activation Functions and Their Derivatives\")\n",
        "fig.update_xaxes(title_text=\"x\")\n",
        "fig.update_yaxes(title_text=\"y\", row=1)\n",
        "fig.update_yaxes(title_text=\"dy/dx\", row=2)\n",
        "fig.show()\n",
        "\n",
        "print(\"Maximum derivative values:\")\n",
        "print(f\"Sigmoid: {np.max(sigmoid_derivative(x)):.3f}\")\n",
        "print(f\"Tanh: {np.max(tanh_derivative(x)):.3f}\")\n",
        "print(f\"ReLU: {np.max(relu_derivative(x)):.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simulate gradient flow through deep networks\n",
        "def simulate_gradient_flow(activation_fn, derivative_fn, n_layers=10, n_simulations=100):\n",
        "    \"\"\"Simulate gradient backpropagation through multiple layers\"\"\"\n",
        "    gradients_by_layer = []\n",
        "    \n",
        "    for _ in range(n_simulations):\n",
        "        # Random initialization\n",
        "        gradient = 1.0  # Start with gradient of 1 from output\n",
        "        layer_gradients = [gradient]\n",
        "        \n",
        "        for layer in range(n_layers):\n",
        "            # Random pre-activation values\n",
        "            z = np.random.randn()\n",
        "            # Gradient gets multiplied by derivative\n",
        "            gradient *= derivative_fn(z)\n",
        "            layer_gradients.append(gradient)\n",
        "        \n",
        "        gradients_by_layer.append(layer_gradients)\n",
        "    \n",
        "    return np.array(gradients_by_layer)\n",
        "\n",
        "# Simulate for different activation functions\n",
        "n_layers = 20\n",
        "sigmoid_grads = simulate_gradient_flow(sigmoid, sigmoid_derivative, n_layers)\n",
        "tanh_grads = simulate_gradient_flow(tanh, tanh_derivative, n_layers)\n",
        "relu_grads = simulate_gradient_flow(relu, relu_derivative, n_layers)\n",
        "\n",
        "# Visualize gradient flow\n",
        "fig = go.Figure()\n",
        "\n",
        "layers = list(range(n_layers + 1))\n",
        "\n",
        "# Add mean gradient flow for each activation\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=layers,\n",
        "    y=np.mean(sigmoid_grads, axis=0),\n",
        "    name='Sigmoid',\n",
        "    line=dict(width=3),\n",
        "    mode='lines+markers'\n",
        "))\n",
        "\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=layers,\n",
        "    y=np.mean(tanh_grads, axis=0),\n",
        "    name='Tanh',\n",
        "    line=dict(width=3),\n",
        "    mode='lines+markers'\n",
        "))\n",
        "\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=layers,\n",
        "    y=np.mean(relu_grads, axis=0),\n",
        "    name='ReLU',\n",
        "    line=dict(width=3),\n",
        "    mode='lines+markers'\n",
        "))\n",
        "\n",
        "fig.update_layout(\n",
        "    title='Gradient Flow Through Deep Networks',\n",
        "    xaxis_title='Layer (from output to input)',\n",
        "    yaxis_title='Average Gradient Magnitude',\n",
        "    yaxis_type='log',\n",
        "    height=500,\n",
        "    hovermode='x unified'\n",
        ")\n",
        "fig.show()\n",
        "\n",
        "print(\"Final gradient (at input layer):\")\n",
        "print(f\"Sigmoid: {np.mean(sigmoid_grads[:, -1]):.2e}\")\n",
        "print(f\"Tanh: {np.mean(tanh_grads[:, -1]):.2e}\")\n",
        "print(f\"ReLU: {np.mean(relu_grads[:, -1]):.3f}\")\n",
        "print(\"\\nâš ï¸ Notice how sigmoid and tanh gradients vanish!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ðŸ’¡ Your Turn:\n",
        "1. Modify the number of layers and observe how it affects gradient flow\n",
        "2. Implement Leaky ReLU (f(x) = max(0.01x, x)) and add it to the comparison\n",
        "3. What happens if you initialize the network differently?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Exercise 3: Implementing Modern Activation Functions\n",
        "\n",
        "### Concept: Advanced Activations\n",
        "Modern deep learning uses sophisticated activation functions that address the limitations of traditional ones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implement modern activation functions\n",
        "class ModernActivations:\n",
        "    @staticmethod\n",
        "    def leaky_relu(x, alpha=0.01):\n",
        "        \"\"\"Leaky ReLU: allows small negative gradients\"\"\"\n",
        "        return np.where(x > 0, x, alpha * x)\n",
        "    \n",
        "    @staticmethod\n",
        "    def elu(x, alpha=1.0):\n",
        "        \"\"\"Exponential Linear Unit\"\"\"\n",
        "        return np.where(x > 0, x, alpha * (np.exp(x) - 1))\n",
        "    \n",
        "    @staticmethod\n",
        "    def selu(x):\n",
        "        \"\"\"Scaled Exponential Linear Unit (self-normalizing)\"\"\"\n",
        "        alpha = 1.6732632423543772\n",
        "        scale = 1.0507009873554805\n",
        "        return scale * np.where(x > 0, x, alpha * (np.exp(x) - 1))\n",
        "    \n",
        "    @staticmethod\n",
        "    def swish(x, beta=1.0):\n",
        "        \"\"\"Swish: x * sigmoid(Î²x)\"\"\"\n",
        "        return x * sigmoid(beta * x)\n",
        "    \n",
        "    @staticmethod\n",
        "    def gelu(x):\n",
        "        \"\"\"Gaussian Error Linear Unit (approximation)\"\"\"\n",
        "        return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x**3)))\n",
        "    \n",
        "    @staticmethod\n",
        "    def mish(x):\n",
        "        \"\"\"Mish: x * tanh(softplus(x))\"\"\"\n",
        "        return x * np.tanh(np.log(1 + np.exp(x)))\n",
        "\n",
        "# Compare all activation functions\n",
        "x = np.linspace(-3, 3, 1000)\n",
        "activations = ModernActivations()\n",
        "\n",
        "fig = make_subplots(\n",
        "    rows=2, cols=3,\n",
        "    subplot_titles=('Leaky ReLU', 'ELU', 'SELU', 'Swish', 'GELU', 'Mish')\n",
        ")\n",
        "\n",
        "# Plot each activation\n",
        "functions = [\n",
        "    (activations.leaky_relu, 'Leaky ReLU'),\n",
        "    (activations.elu, 'ELU'),\n",
        "    (activations.selu, 'SELU'),\n",
        "    (activations.swish, 'Swish'),\n",
        "    (activations.gelu, 'GELU'),\n",
        "    (activations.mish, 'Mish')\n",
        "]\n",
        "\n",
        "for idx, (func, name) in enumerate(functions):\n",
        "    row = idx // 3 + 1\n",
        "    col = idx % 3 + 1\n",
        "    \n",
        "    y = func(x)\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=x, y=y, name=name, line=dict(width=3)),\n",
        "        row=row, col=col\n",
        "    )\n",
        "    \n",
        "    # Add ReLU for comparison (in light gray)\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=x, y=relu(x), name='ReLU', \n",
        "                  line=dict(color='lightgray', dash='dash')),\n",
        "        row=row, col=col\n",
        "    )\n",
        "\n",
        "fig.update_layout(\n",
        "    height=600,\n",
        "    title_text=\"Modern Activation Functions (compared to ReLU in gray)\",\n",
        "    showlegend=False\n",
        ")\n",
        "fig.update_xaxes(title_text=\"x\")\n",
        "fig.update_yaxes(title_text=\"y\")\n",
        "fig.show()\n",
        "\n",
        "# Compare key properties\n",
        "print(\"Activation Function Properties at x = -1:\")\n",
        "print(f\"ReLU:       {relu(-1):.3f}\")\n",
        "print(f\"Leaky ReLU: {activations.leaky_relu(-1):.3f}\")\n",
        "print(f\"ELU:        {activations.elu(-1):.3f}\")\n",
        "print(f\"SELU:       {activations.selu(-1):.3f}\")\n",
        "print(f\"Swish:      {activations.swish(-1):.3f}\")\n",
        "print(f\"GELU:       {activations.gelu(-1):.3f}\")\n",
        "print(f\"Mish:       {activations.mish(-1):.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Exercise 4: Performance Comparison of Activation Functions\n",
        "\n",
        "### Concept: Choosing the Right Activation\n",
        "Different activation functions perform better in different scenarios. Let's compare them on a real task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a more complex dataset\n",
        "from sklearn.datasets import make_moons, make_circles\n",
        "\n",
        "# Generate combined dataset\n",
        "X1, y1 = make_moons(n_samples=500, noise=0.15, random_state=42)\n",
        "X2, y2 = make_circles(n_samples=500, noise=0.1, factor=0.5, random_state=42)\n",
        "X2 = X2 * 2 + 2  # Shift the circles\n",
        "\n",
        "X_complex = np.vstack([X1, X2])\n",
        "y_complex = np.hstack([y1, y2 + 2])  # 4 classes total\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_complex = scaler.fit_transform(X_complex)\n",
        "\n",
        "# Visualize dataset\n",
        "plt.figure(figsize=(10, 6))\n",
        "scatter = plt.scatter(X_complex[:, 0], X_complex[:, 1], c=y_complex, \n",
        "                     cmap='viridis', alpha=0.6, edgecolors='black', linewidth=0.5)\n",
        "plt.colorbar(scatter, label='Class')\n",
        "plt.title('Complex Classification Dataset')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "print(f\"Dataset shape: {X_complex.shape}\")\n",
        "print(f\"Number of classes: {len(np.unique(y_complex))}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build models with different activation functions\n",
        "def build_model_with_activation(activation, input_dim=2, n_classes=4):\n",
        "    \"\"\"Build a deep network with specified activation function\"\"\"\n",
        "    model = models.Sequential([\n",
        "        layers.Dense(128, activation=activation, input_shape=(input_dim,)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.3),\n",
        "        \n",
        "        layers.Dense(64, activation=activation),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.3),\n",
        "        \n",
        "        layers.Dense(32, activation=activation),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.3),\n",
        "        \n",
        "        layers.Dense(n_classes, activation='softmax')\n",
        "    ])\n",
        "    \n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# Train models with different activations\n",
        "activations_to_test = ['relu', 'elu', 'selu', 'swish', 'gelu']\n",
        "histories = {}\n",
        "models_dict = {}\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_complex, y_complex, test_size=0.2, random_state=42, stratify=y_complex\n",
        ")\n",
        "\n",
        "print(\"Training models with different activation functions...\")\n",
        "for activation in activations_to_test:\n",
        "    print(f\"\\nTraining with {activation.upper()}...\")\n",
        "    \n",
        "    # Build and train model\n",
        "    model = build_model_with_activation(activation)\n",
        "    \n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        epochs=50,\n",
        "        batch_size=32,\n",
        "        validation_split=0.2,\n",
        "        verbose=0,\n",
        "        callbacks=[keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)]\n",
        "    )\n",
        "    \n",
        "    histories[activation] = history\n",
        "    models_dict[activation] = model\n",
        "    \n",
        "    # Evaluate\n",
        "    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
        "    print(f\"{activation.upper()} - Test Accuracy: {test_acc:.3f}\")\n",
        "\n",
        "print(\"\\nTraining complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize training curves for all activations\n",
        "fig = make_subplots(\n",
        "    rows=1, cols=2,\n",
        "    subplot_titles=('Training Accuracy', 'Validation Accuracy')\n",
        ")\n",
        "\n",
        "colors = ['blue', 'red', 'green', 'purple', 'orange']\n",
        "\n",
        "for idx, (activation, history) in enumerate(histories.items()):\n",
        "    # Training accuracy\n",
        "    fig.add_trace(\n",
        "        go.Scatter(\n",
        "            y=history.history['accuracy'],\n",
        "            name=activation.upper(),\n",
        "            line=dict(color=colors[idx], width=2),\n",
        "            showlegend=True\n",
        "        ),\n",
        "        row=1, col=1\n",
        "    )\n",
        "    \n",
        "    # Validation accuracy\n",
        "    fig.add_trace(\n",
        "        go.Scatter(\n",
        "            y=history.history['val_accuracy'],\n",
        "            name=activation.upper(),\n",
        "            line=dict(color=colors[idx], width=2, dash='dash'),\n",
        "            showlegend=False\n",
        "        ),\n",
        "        row=1, col=2\n",
        "    )\n",
        "\n",
        "fig.update_xaxes(title_text=\"Epoch\")\n",
        "fig.update_yaxes(title_text=\"Accuracy\")\n",
        "fig.update_layout(\n",
        "    height=400,\n",
        "    title_text=\"Activation Function Performance Comparison\"\n",
        ")\n",
        "fig.show()\n",
        "\n",
        "# Summary statistics\n",
        "print(\"\\nFinal Performance Summary:\")\n",
        "print(\"-\" * 40)\n",
        "for activation in activations_to_test:\n",
        "    test_loss, test_acc = models_dict[activation].evaluate(X_test, y_test, verbose=0)\n",
        "    train_acc = histories[activation].history['accuracy'][-1]\n",
        "    val_acc = histories[activation].history['val_accuracy'][-1]\n",
        "    \n",
        "    print(f\"{activation.upper():8s} | Train: {train_acc:.3f} | Val: {val_acc:.3f} | Test: {test_acc:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Exercise 5: Visualizing Hierarchical Feature Learning\n",
        "\n",
        "### Concept: Layer-wise Representations\n",
        "Deep networks learn increasingly abstract representations at each layer. Let's visualize this hierarchy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a model with accessible intermediate layers\n",
        "def build_model_with_outputs(input_dim=2, n_classes=4):\n",
        "    \"\"\"Build model that returns intermediate layer outputs\"\"\"\n",
        "    input_layer = layers.Input(shape=(input_dim,))\n",
        "    \n",
        "    # Layer 1\n",
        "    x1 = layers.Dense(128, activation='relu', name='layer1')(input_layer)\n",
        "    x1 = layers.BatchNormalization()(x1)\n",
        "    \n",
        "    # Layer 2\n",
        "    x2 = layers.Dense(64, activation='relu', name='layer2')(x1)\n",
        "    x2 = layers.BatchNormalization()(x2)\n",
        "    \n",
        "    # Layer 3\n",
        "    x3 = layers.Dense(32, activation='relu', name='layer3')(x2)\n",
        "    x3 = layers.BatchNormalization()(x3)\n",
        "    \n",
        "    # Output layer\n",
        "    output = layers.Dense(n_classes, activation='softmax', name='output')(x3)\n",
        "    \n",
        "    # Create model\n",
        "    model = models.Model(inputs=input_layer, outputs=output)\n",
        "    \n",
        "    # Create models for intermediate outputs\n",
        "    layer1_model = models.Model(inputs=input_layer, outputs=x1)\n",
        "    layer2_model = models.Model(inputs=input_layer, outputs=x2)\n",
        "    layer3_model = models.Model(inputs=input_layer, outputs=x3)\n",
        "    \n",
        "    return model, [layer1_model, layer2_model, layer3_model]\n",
        "\n",
        "# Build and train the model\n",
        "main_model, intermediate_models = build_model_with_outputs()\n",
        "\n",
        "main_model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "main_model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=30,\n",
        "    batch_size=32,\n",
        "    validation_split=0.2,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "print(\"Model trained successfully!\")\n",
        "print(f\"Test accuracy: {main_model.evaluate(X_test, y_test, verbose=0)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize intermediate representations using t-SNE\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Get intermediate representations\n",
        "sample_size = 500\n",
        "sample_indices = np.random.choice(len(X_test), sample_size, replace=False)\n",
        "X_sample = X_test[sample_indices]\n",
        "y_sample = y_test[sample_indices]\n",
        "\n",
        "# Get representations from each layer\n",
        "representations = {\n",
        "    'Input': X_sample,\n",
        "    'Layer 1': intermediate_models[0].predict(X_sample, verbose=0),\n",
        "    'Layer 2': intermediate_models[1].predict(X_sample, verbose=0),\n",
        "    'Layer 3': intermediate_models[2].predict(X_sample, verbose=0)\n",
        "}\n",
        "\n",
        "# Apply t-SNE to high-dimensional representations\n",
        "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
        "\n",
        "for idx, (name, rep) in enumerate(representations.items()):\n",
        "    ax = axes[idx]\n",
        "    \n",
        "    # Apply t-SNE if needed\n",
        "    if rep.shape[1] > 2:\n",
        "        tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
        "        rep_2d = tsne.fit_transform(rep)\n",
        "    else:\n",
        "        rep_2d = rep\n",
        "    \n",
        "    # Plot\n",
        "    scatter = ax.scatter(rep_2d[:, 0], rep_2d[:, 1], c=y_sample, \n",
        "                        cmap='viridis', alpha=0.6, s=20)\n",
        "    ax.set_title(f'{name}\\n({rep.shape[1]} dims)')\n",
        "    ax.set_xlabel('Component 1')\n",
        "    ax.set_ylabel('Component 2')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.suptitle('Hierarchical Feature Learning: Layer-wise Representations', fontsize=14, y=1.05)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Notice how the representations become increasingly separable at deeper layers!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Exercise 6: Understanding the Dead ReLU Problem\n",
        "\n",
        "### Concept: Dying Neurons\n",
        "ReLU neurons can 'die' during training, meaning they always output zero and stop learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Monitor dead neurons during training\n",
        "class DeadReLUCallback(keras.callbacks.Callback):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.dead_neurons_history = []\n",
        "    \n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        dead_count = 0\n",
        "        total_count = 0\n",
        "        \n",
        "        for layer in self.model.layers:\n",
        "            if isinstance(layer, layers.Dense) and layer.activation.__name__ == 'relu':\n",
        "                weights = layer.get_weights()[0]  # Get weight matrix\n",
        "                # Check how many neurons have all negative weights\n",
        "                # (more likely to be dead)\n",
        "                neuron_max_weights = np.max(weights, axis=0)\n",
        "                dead = np.sum(neuron_max_weights <= 0)\n",
        "                \n",
        "                dead_count += dead\n",
        "                total_count += weights.shape[1]\n",
        "        \n",
        "        dead_percentage = (dead_count / total_count) * 100 if total_count > 0 else 0\n",
        "        self.dead_neurons_history.append(dead_percentage)\n",
        "\n",
        "# Build models with different initializations\n",
        "def build_relu_model_with_init(init_method):\n",
        "    model = models.Sequential([\n",
        "        layers.Dense(256, activation='relu', kernel_initializer=init_method, input_shape=(2,)),\n",
        "        layers.Dense(128, activation='relu', kernel_initializer=init_method),\n",
        "        layers.Dense(64, activation='relu', kernel_initializer=init_method),\n",
        "        layers.Dense(4, activation='softmax')\n",
        "    ])\n",
        "    \n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# Test different initializations\n",
        "init_methods = {\n",
        "    'zeros': 'zeros',  # Worst case\n",
        "    'random_normal': keras.initializers.RandomNormal(mean=0, stddev=0.5),\n",
        "    'he_normal': 'he_normal',  # Best for ReLU\n",
        "    'glorot_uniform': 'glorot_uniform'  # Xavier initialization\n",
        "}\n",
        "\n",
        "dead_neuron_results = {}\n",
        "\n",
        "print(\"Testing different weight initializations...\\n\")\n",
        "for name, init in init_methods.items():\n",
        "    print(f\"Training with {name} initialization...\")\n",
        "    \n",
        "    model = build_relu_model_with_init(init)\n",
        "    callback = DeadReLUCallback()\n",
        "    \n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        epochs=20,\n",
        "        batch_size=32,\n",
        "        validation_split=0.2,\n",
        "        verbose=0,\n",
        "        callbacks=[callback]\n",
        "    )\n",
        "    \n",
        "    dead_neuron_results[name] = callback.dead_neurons_history\n",
        "    \n",
        "    test_acc = model.evaluate(X_test, y_test, verbose=0)[1]\n",
        "    print(f\"  Test Accuracy: {test_acc:.3f}\")\n",
        "    print(f\"  Final dead neurons: {callback.dead_neurons_history[-1]:.1f}%\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize dead neuron evolution\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "for name, history in dead_neuron_results.items():\n",
        "    plt.plot(history, label=name, linewidth=2, marker='o')\n",
        "\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Percentage of Dead Neurons (%)')\n",
        "plt.title('Dead ReLU Problem with Different Initializations')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "print(\"Key Insights:\")\n",
        "print(\"1. Poor initialization (zeros) causes massive neuron death\")\n",
        "print(\"2. He initialization is designed specifically for ReLU\")\n",
        "print(\"3. Dead neurons stop learning and reduce model capacity\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Exercise 7: Implementing Gradient Clipping\n",
        "\n",
        "### Concept: Controlling Exploding Gradients\n",
        "Gradient clipping prevents gradients from becoming too large, stabilizing training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a dataset that might cause gradient explosion\n",
        "np.random.seed(42)\n",
        "X_unstable = np.random.randn(1000, 10) * 10  # Large input values\n",
        "y_unstable = np.random.randint(0, 2, 1000)\n",
        "\n",
        "X_train_uns, X_test_uns, y_train_uns, y_test_uns = train_test_split(\n",
        "    X_unstable, y_unstable, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Build models with and without gradient clipping\n",
        "def build_model_with_clipping(clip_value=None):\n",
        "    model = models.Sequential([\n",
        "        layers.Dense(128, activation='tanh', input_shape=(10,)),\n",
        "        layers.Dense(128, activation='tanh'),\n",
        "        layers.Dense(128, activation='tanh'),\n",
        "        layers.Dense(128, activation='tanh'),\n",
        "        layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    \n",
        "    # Configure optimizer with gradient clipping\n",
        "    if clip_value:\n",
        "        optimizer = optimizers.Adam(clipvalue=clip_value)\n",
        "    else:\n",
        "        optimizer = optimizers.Adam()\n",
        "    \n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Train without clipping\n",
        "print(\"Training without gradient clipping...\")\n",
        "model_no_clip = build_model_with_clipping(clip_value=None)\n",
        "history_no_clip = model_no_clip.fit(\n",
        "    X_train_uns, y_train_uns,\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    validation_split=0.2,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "# Train with clipping\n",
        "print(\"Training with gradient clipping (clip_value=1.0)...\")\n",
        "model_clip = build_model_with_clipping(clip_value=1.0)\n",
        "history_clip = model_clip.fit(\n",
        "    X_train_uns, y_train_uns,\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    validation_split=0.2,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "print(\"Training complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare training stability\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# Loss comparison\n",
        "ax1.plot(history_no_clip.history['loss'], label='No Clipping', linewidth=2, alpha=0.7)\n",
        "ax1.plot(history_clip.history['loss'], label='With Clipping', linewidth=2)\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.set_title('Training Loss Comparison')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.set_ylim([0, max(history_no_clip.history['loss'][5:])*1.1])  # Zoom in after initial epochs\n",
        "\n",
        "# Validation accuracy comparison\n",
        "ax2.plot(history_no_clip.history['val_accuracy'], label='No Clipping', linewidth=2, alpha=0.7)\n",
        "ax2.plot(history_clip.history['val_accuracy'], label='With Clipping', linewidth=2)\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Validation Accuracy')\n",
        "ax2.set_title('Validation Accuracy Comparison')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Calculate stability metrics\n",
        "loss_variance_no_clip = np.var(history_no_clip.history['loss'][10:])\n",
        "loss_variance_clip = np.var(history_clip.history['loss'][10:])\n",
        "\n",
        "print(f\"\\nTraining Stability (lower is better):\")\n",
        "print(f\"Loss variance without clipping: {loss_variance_no_clip:.4f}\")\n",
        "print(f\"Loss variance with clipping: {loss_variance_clip:.4f}\")\n",
        "print(f\"Improvement: {(1 - loss_variance_clip/loss_variance_no_clip)*100:.1f}% more stable\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Exercise 8: Understanding Batch Normalization\n",
        "\n",
        "### Concept: Internal Covariate Shift\n",
        "Batch Normalization stabilizes training by normalizing inputs to each layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare models with and without batch normalization\n",
        "def build_model_batchnorm(use_batchnorm=True, use_dropout=False):\n",
        "    model = models.Sequential()\n",
        "    \n",
        "    # Input layer\n",
        "    model.add(layers.Dense(256, input_shape=(2,)))\n",
        "    if use_batchnorm:\n",
        "        model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Activation('relu'))\n",
        "    if use_dropout:\n",
        "        model.add(layers.Dropout(0.3))\n",
        "    \n",
        "    # Hidden layers\n",
        "    for units in [128, 64, 32]:\n",
        "        model.add(layers.Dense(units))\n",
        "        if use_batchnorm:\n",
        "            model.add(layers.BatchNormalization())\n",
        "        model.add(layers.Activation('relu'))\n",
        "        if use_dropout:\n",
        "            model.add(layers.Dropout(0.3))\n",
        "    \n",
        "    # Output layer\n",
        "    model.add(layers.Dense(4, activation='softmax'))\n",
        "    \n",
        "    model.compile(\n",
        "        optimizer=optimizers.Adam(learning_rate=0.001),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Train models with different configurations\n",
        "configs = [\n",
        "    ('No BatchNorm, No Dropout', False, False),\n",
        "    ('BatchNorm Only', True, False),\n",
        "    ('Dropout Only', False, True),\n",
        "    ('BatchNorm + Dropout', True, True)\n",
        "]\n",
        "\n",
        "results = {}\n",
        "\n",
        "print(\"Training models with different regularization techniques...\\n\")\n",
        "for name, use_bn, use_dropout in configs:\n",
        "    print(f\"Training: {name}\")\n",
        "    \n",
        "    model = build_model_batchnorm(use_bn, use_dropout)\n",
        "    \n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        epochs=50,\n",
        "        batch_size=32,\n",
        "        validation_split=0.2,\n",
        "        verbose=0,\n",
        "        callbacks=[keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)]\n",
        "    )\n",
        "    \n",
        "    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
        "    \n",
        "    results[name] = {\n",
        "        'history': history,\n",
        "        'test_acc': test_acc,\n",
        "        'final_val_acc': history.history['val_accuracy'][-1]\n",
        "    }\n",
        "    \n",
        "    print(f\"  Test Accuracy: {test_acc:.3f}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the effects of batch normalization\n",
        "fig = make_subplots(\n",
        "    rows=1, cols=2,\n",
        "    subplot_titles=('Training Curves', 'Final Performance')\n",
        ")\n",
        "\n",
        "colors = {'No BatchNorm, No Dropout': 'red',\n",
        "          'BatchNorm Only': 'blue',\n",
        "          'Dropout Only': 'green',\n",
        "          'BatchNorm + Dropout': 'purple'}\n",
        "\n",
        "# Training curves\n",
        "for name, result in results.items():\n",
        "    fig.add_trace(\n",
        "        go.Scatter(\n",
        "            y=result['history'].history['val_accuracy'],\n",
        "            name=name,\n",
        "            line=dict(color=colors[name], width=2)\n",
        "        ),\n",
        "        row=1, col=1\n",
        "    )\n",
        "\n",
        "# Bar chart of final performance\n",
        "names = list(results.keys())\n",
        "test_accs = [results[name]['test_acc'] for name in names]\n",
        "\n",
        "fig.add_trace(\n",
        "    go.Bar(\n",
        "        x=names,\n",
        "        y=test_accs,\n",
        "        marker_color=list(colors.values()),\n",
        "        showlegend=False\n",
        "    ),\n",
        "    row=1, col=2\n",
        ")\n",
        "\n",
        "fig.update_xaxes(title_text=\"Epoch\", row=1, col=1)\n",
        "fig.update_xaxes(title_text=\"Configuration\", tickangle=45, row=1, col=2)\n",
        "fig.update_yaxes(title_text=\"Validation Accuracy\", row=1, col=1)\n",
        "fig.update_yaxes(title_text=\"Test Accuracy\", row=1, col=2)\n",
        "\n",
        "fig.update_layout(height=400, title_text=\"Impact of Batch Normalization and Dropout\")\n",
        "fig.show()\n",
        "\n",
        "print(\"\\nKey Observations:\")\n",
        "print(\"1. BatchNorm accelerates training and improves convergence\")\n",
        "print(\"2. BatchNorm + Dropout provides best regularization\")\n",
        "print(\"3. BatchNorm alone can sometimes be sufficient for regularization\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 5: Summary and Practice Exercises\n",
        "\n",
        "### ðŸŽ¯ Key Takeaways\n",
        "\n",
        "1. **Deep vs Shallow Networks**\n",
        "   - Deep networks are more parameter-efficient\n",
        "   - They learn hierarchical representations\n",
        "   - Feature reuse and composition provide exponential expressiveness\n",
        "\n",
        "2. **Gradient Problems**\n",
        "   - Vanishing gradients: Common with sigmoid/tanh activations\n",
        "   - Exploding gradients: Can occur with poor initialization\n",
        "   - Solutions: Better activations, normalization, gradient clipping\n",
        "\n",
        "3. **Modern Activation Functions**\n",
        "   - ReLU: Simple and effective, but can die\n",
        "   - Leaky ReLU/ELU: Address dead neuron problem\n",
        "   - SELU: Self-normalizing properties\n",
        "   - Swish/GELU: State-of-the-art for many tasks\n",
        "\n",
        "4. **Training Techniques**\n",
        "   - Proper initialization (He/Xavier) is crucial\n",
        "   - Batch normalization stabilizes training\n",
        "   - Gradient clipping prevents explosions\n",
        "   - Dropout provides regularization\n",
        "\n",
        "### ðŸ“ Practice Exercises\n",
        "\n",
        "Try these exercises to deepen your understanding:\n",
        "\n",
        "1. **Architecture Exploration**\n",
        "   - Build a 10-layer network and train it with different activation functions\n",
        "   - Compare convergence speed and final accuracy\n",
        "\n",
        "2. **Custom Activation Function**\n",
        "   - Implement your own activation function (e.g., x * tanh(sqrt(x)))\n",
        "   - Test it on the spiral dataset\n",
        "\n",
        "3. **Gradient Analysis**\n",
        "   - Track gradient magnitudes during training\n",
        "   - Visualize how they change with depth\n",
        "\n",
        "4. **Hyperparameter Study**\n",
        "   - Systematically vary learning rate, batch size, and network depth\n",
        "   - Create a heatmap of performance\n",
        "\n",
        "5. **Real Dataset Challenge**\n",
        "   - Apply these concepts to MNIST or CIFAR-10\n",
        "   - Build the deepest network you can train successfully\n",
        "\n",
        "### ðŸš€ Advanced Topics to Explore\n",
        "\n",
        "- Residual connections (ResNet)\n",
        "- Dense connections (DenseNet)  \n",
        "- Attention mechanisms\n",
        "- Neural Architecture Search (NAS)\n",
        "- Pruning and quantization\n",
        "\n",
        "### ðŸ“š References\n",
        "\n",
        "1. Glorot & Bengio (2010) - Understanding the difficulty of training deep feedforward neural networks\n",
        "2. He et al. (2015) - Delving Deep into Rectifiers\n",
        "3. Ioffe & Szegedy (2015) - Batch Normalization\n",
        "4. Ramachandran et al. (2017) - Searching for Activation Functions\n",
        "5. Klambauer et al. (2017) - Self-Normalizing Neural Networks\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}