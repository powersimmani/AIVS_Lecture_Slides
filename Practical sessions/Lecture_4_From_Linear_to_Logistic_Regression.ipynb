{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# From Linear to Logistic Regression: A Comprehensive Hands-on Tutorial\n",
        "\n",
        "## Learning Objectives\n",
        "- Master advanced linear regression techniques including regularization\n",
        "- Understand the transition from regression to classification\n",
        "- Implement logistic regression from scratch and using scikit-learn\n",
        "- Apply multiclass classification strategies\n",
        "\n",
        "## Structure\n",
        "1. **Setup and Data Preparation**\n",
        "2. **Advanced Linear Regression** (Exercises 1-3)\n",
        "3. **Transition to Classification** (Exercises 4-6)\n",
        "4. **Logistic Regression Implementation** (Exercises 7-8)\n",
        "5. **Multiclass Classification** (Exercises 9-10)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Setup and Imports\n",
        "Import all necessary libraries for our regression journey."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Scikit-learn imports\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "from sklearn.linear_model import (\n",
        "    LinearRegression, Ridge, Lasso, ElasticNet,\n",
        "    LogisticRegression, Perceptron\n",
        ")\n",
        "from sklearn.metrics import (\n",
        "    mean_squared_error, r2_score, mean_absolute_error,\n",
        "    accuracy_score, confusion_matrix, classification_report,\n",
        "    roc_curve, roc_auc_score\n",
        ")\n",
        "from sklearn.datasets import make_regression, make_classification, load_iris\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "# Interactive plotting\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette('husl')\n",
        "np.random.seed(42)\n",
        "\n",
        "print('Setup complete! All libraries loaded successfully.')\n",
        "print(f'NumPy version: {np.__version__}')\n",
        "print(f'Pandas version: {pd.__version__}')\n",
        "print(f'Scikit-learn version: {sklearn.__version__}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Exercise 1: Polynomial Regression and Basis Expansion\n",
        "\n",
        "### Concept\n",
        "Linear models can fit non-linear relationships by transforming features. We transform $x$ into $x, x^2, x^3, ...$ to create polynomial features.\n",
        "\n",
        "**Key Insight**: The model remains linear in parameters (coefficients) even though it's non-linear in the original features.\n",
        "\n",
        "### Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate non-linear synthetic data\n",
        "np.random.seed(42)\n",
        "n_samples = 100\n",
        "X = np.sort(np.random.uniform(-3, 3, n_samples))\n",
        "y_true = 0.5 * X**3 - 2 * X**2 + X + 3\n",
        "y = y_true + np.random.normal(0, 3, n_samples)  # Add noise\n",
        "\n",
        "# Reshape for sklearn\n",
        "X_reshape = X.reshape(-1, 1)\n",
        "\n",
        "# Create polynomial features of different degrees\n",
        "degrees = [1, 3, 5, 10]\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for idx, degree in enumerate(degrees):\n",
        "    # Transform features\n",
        "    poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
        "    X_poly = poly.fit_transform(X_reshape)\n",
        "    \n",
        "    # Fit model\n",
        "    model = LinearRegression()\n",
        "    model.fit(X_poly, y)\n",
        "    \n",
        "    # Predictions for smooth curve\n",
        "    X_test = np.linspace(-3, 3, 300).reshape(-1, 1)\n",
        "    X_test_poly = poly.transform(X_test)\n",
        "    y_pred = model.predict(X_test_poly)\n",
        "    \n",
        "    # Calculate R² score\n",
        "    train_score = model.score(X_poly, y)\n",
        "    \n",
        "    # Plot\n",
        "    axes[idx].scatter(X, y, alpha=0.6, s=30, label='Data')\n",
        "    axes[idx].plot(X_test, y_pred, 'r-', linewidth=2, label=f'Degree {degree}')\n",
        "    axes[idx].plot(X, y_true, 'g--', alpha=0.5, label='True function')\n",
        "    axes[idx].set_xlabel('X')\n",
        "    axes[idx].set_ylabel('y')\n",
        "    axes[idx].set_title(f'Polynomial Degree {degree}\\nR² = {train_score:.3f}')\n",
        "    axes[idx].legend()\n",
        "    axes[idx].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nKey Observations:\")\n",
        "print(\"- Degree 1: Underfitting (can't capture the non-linear pattern)\")\n",
        "print(\"- Degree 3: Good fit (matches the true function well)\")\n",
        "print(\"- Degree 10: Overfitting (fits noise, oscillates wildly)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Turn\n",
        "1. Generate a dataset with a different non-linear relationship (e.g., sine wave)\n",
        "2. Use cross-validation to find the optimal polynomial degree\n",
        "3. Plot validation curves showing train and test scores vs degree"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Exercise 2: Ridge vs Lasso vs Elastic Net Regularization\n",
        "\n",
        "### Concept\n",
        "Regularization prevents overfitting by adding penalty terms:\n",
        "- **Ridge (L2)**: $\\text{Loss} + \\lambda\\sum\\beta_i^2$ - Shrinks coefficients smoothly\n",
        "- **Lasso (L1)**: $\\text{Loss} + \\lambda\\sum|\\beta_i|$ - Can zero out coefficients (feature selection)\n",
        "- **Elastic Net**: Combines L1 and L2 penalties\n",
        "\n",
        "### Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate high-dimensional sparse data\n",
        "n_samples, n_features = 100, 20\n",
        "n_informative = 5  # Only 5 features are actually useful\n",
        "\n",
        "X, y = make_regression(n_samples=n_samples, n_features=n_features,\n",
        "                      n_informative=n_informative, noise=10, random_state=42)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Standardize features (important for regularization)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Test different regularization strengths\n",
        "alphas = np.logspace(-3, 1, 20)\n",
        "\n",
        "# Store results\n",
        "results = {\n",
        "    'Ridge': {'train_scores': [], 'test_scores': [], 'n_nonzero': []},\n",
        "    'Lasso': {'train_scores': [], 'test_scores': [], 'n_nonzero': []},\n",
        "    'ElasticNet': {'train_scores': [], 'test_scores': [], 'n_nonzero': []}\n",
        "}\n",
        "\n",
        "# Fit models with different alphas\n",
        "for alpha in alphas:\n",
        "    # Ridge\n",
        "    ridge = Ridge(alpha=alpha)\n",
        "    ridge.fit(X_train_scaled, y_train)\n",
        "    results['Ridge']['train_scores'].append(ridge.score(X_train_scaled, y_train))\n",
        "    results['Ridge']['test_scores'].append(ridge.score(X_test_scaled, y_test))\n",
        "    results['Ridge']['n_nonzero'].append(np.sum(np.abs(ridge.coef_) > 0.01))\n",
        "    \n",
        "    # Lasso\n",
        "    lasso = Lasso(alpha=alpha, max_iter=1000)\n",
        "    lasso.fit(X_train_scaled, y_train)\n",
        "    results['Lasso']['train_scores'].append(lasso.score(X_train_scaled, y_train))\n",
        "    results['Lasso']['test_scores'].append(lasso.score(X_test_scaled, y_test))\n",
        "    results['Lasso']['n_nonzero'].append(np.sum(lasso.coef_ != 0))\n",
        "    \n",
        "    # Elastic Net\n",
        "    elastic = ElasticNet(alpha=alpha, l1_ratio=0.5, max_iter=1000)\n",
        "    elastic.fit(X_train_scaled, y_train)\n",
        "    results['ElasticNet']['train_scores'].append(elastic.score(X_train_scaled, y_train))\n",
        "    results['ElasticNet']['test_scores'].append(elastic.score(X_test_scaled, y_test))\n",
        "    results['ElasticNet']['n_nonzero'].append(np.sum(elastic.coef_ != 0))\n",
        "\n",
        "# Visualization\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "# Plot 1: Test scores vs alpha\n",
        "for method, color in zip(['Ridge', 'Lasso', 'ElasticNet'], ['blue', 'green', 'red']):\n",
        "    axes[0].semilogx(alphas, results[method]['test_scores'], '-o', \n",
        "                     label=method, color=color, markersize=4)\n",
        "axes[0].set_xlabel('Regularization strength (α)')\n",
        "axes[0].set_ylabel('Test R² Score')\n",
        "axes[0].set_title('Model Performance vs Regularization')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Number of non-zero coefficients\n",
        "for method, color in zip(['Ridge', 'Lasso', 'ElasticNet'], ['blue', 'green', 'red']):\n",
        "    axes[1].semilogx(alphas, results[method]['n_nonzero'], '-o', \n",
        "                     label=method, color=color, markersize=4)\n",
        "axes[1].set_xlabel('Regularization strength (α)')\n",
        "axes[1].set_ylabel('Number of non-zero coefficients')\n",
        "axes[1].set_title('Feature Selection Effect')\n",
        "axes[1].axhline(y=n_informative, color='black', linestyle='--', \n",
        "                label=f'True informative features ({n_informative})')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 3: Coefficient paths for Lasso\n",
        "lasso_coefs = []\n",
        "for alpha in alphas:\n",
        "    lasso = Lasso(alpha=alpha, max_iter=1000)\n",
        "    lasso.fit(X_train_scaled, y_train)\n",
        "    lasso_coefs.append(lasso.coef_)\n",
        "\n",
        "lasso_coefs = np.array(lasso_coefs)\n",
        "for i in range(n_features):\n",
        "    axes[2].semilogx(alphas, lasso_coefs[:, i], alpha=0.7)\n",
        "axes[2].set_xlabel('Regularization strength (α)')\n",
        "axes[2].set_ylabel('Coefficient value')\n",
        "axes[2].set_title('Lasso Coefficient Paths')\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nKey Insights:\")\n",
        "print(\"✓ Ridge: Shrinks all coefficients but keeps all features\")\n",
        "print(\"✓ Lasso: Performs automatic feature selection (sparse solution)\")\n",
        "print(\"✓ Elastic Net: Balance between Ridge and Lasso\")\n",
        "print(f\"\\nTrue number of informative features: {n_informative}\")\n",
        "print(\"Notice how Lasso identifies approximately the right number!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Exercise 3: Feature Selection and Importance\n",
        "\n",
        "### Concept\n",
        "Understanding which features are important helps with:\n",
        "- Model interpretability\n",
        "- Dimensionality reduction\n",
        "- Identifying key drivers\n",
        "\n",
        "### Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a dataset with named features\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "\n",
        "# Load California housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X_housing = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
        "y_housing = housing.target\n",
        "\n",
        "# Split and scale\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_housing, y_housing, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Fit different models\n",
        "models = {\n",
        "    'Linear Regression': LinearRegression(),\n",
        "    'Ridge': Ridge(alpha=1.0),\n",
        "    'Lasso': Lasso(alpha=0.01),\n",
        "    'ElasticNet': ElasticNet(alpha=0.01)\n",
        "}\n",
        "\n",
        "# Store feature importances\n",
        "feature_importance_df = pd.DataFrame()\n",
        "\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    feature_importance_df[name] = np.abs(model.coef_)\n",
        "\n",
        "feature_importance_df.index = X_housing.columns\n",
        "\n",
        "# Create interactive visualization with Plotly\n",
        "fig = go.Figure()\n",
        "\n",
        "for column in feature_importance_df.columns:\n",
        "    fig.add_trace(go.Bar(\n",
        "        name=column,\n",
        "        x=feature_importance_df.index,\n",
        "        y=feature_importance_df[column],\n",
        "        text=feature_importance_df[column].round(3),\n",
        "        textposition='auto',\n",
        "    ))\n",
        "\n",
        "fig.update_layout(\n",
        "    title='Feature Importance Comparison Across Models',\n",
        "    xaxis_title='Features',\n",
        "    yaxis_title='Absolute Coefficient Value',\n",
        "    barmode='group',\n",
        "    height=500,\n",
        "    hovermode='x unified'\n",
        ")\n",
        "\n",
        "fig.show()\n",
        "\n",
        "# Summary statistics\n",
        "print(\"\\nFeature Importance Summary:\")\n",
        "print(\"=\" * 60)\n",
        "print(feature_importance_df.round(3))\n",
        "\n",
        "print(\"\\n\\nTop 3 Most Important Features by Model:\")\n",
        "print(\"=\" * 60)\n",
        "for model_name in feature_importance_df.columns:\n",
        "    top_features = feature_importance_df[model_name].nlargest(3)\n",
        "    print(f\"\\n{model_name}:\")\n",
        "    for feat, importance in top_features.items():\n",
        "        print(f\"  - {feat}: {importance:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Turn\n",
        "1. Implement Recursive Feature Elimination (RFE) to select optimal features\n",
        "2. Compare feature importance from different methods\n",
        "3. Use permutation importance as an alternative method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Exercise 4: Why Linear Regression Fails for Classification\n",
        "\n",
        "### Concept\n",
        "Linear regression predicts continuous values, but classification needs:\n",
        "- Output bounded to [0,1] for probabilities\n",
        "- Discrete class predictions\n",
        "- Appropriate loss function for categorical outcomes\n",
        "\n",
        "### Implementation: Demonstrating the Problem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate binary classification data\n",
        "np.random.seed(42)\n",
        "n_samples = 200\n",
        "\n",
        "# Create two classes with some overlap\n",
        "X_class = np.random.randn(n_samples, 2)\n",
        "y_class = (X_class[:, 0] + 0.5 * X_class[:, 1] > 0.5).astype(int)\n",
        "\n",
        "# Add some outliers\n",
        "X_class[0] = [5, 5]\n",
        "y_class[0] = 1\n",
        "X_class[1] = [-5, -5]\n",
        "y_class[1] = 0\n",
        "\n",
        "# Fit both linear regression and logistic regression\n",
        "linear_reg = LinearRegression()\n",
        "logistic_reg = LogisticRegression()\n",
        "\n",
        "linear_reg.fit(X_class, y_class)\n",
        "logistic_reg.fit(X_class, y_class)\n",
        "\n",
        "# Create mesh for decision boundary\n",
        "h = 0.02\n",
        "x_min, x_max = X_class[:, 0].min() - 1, X_class[:, 0].max() + 1\n",
        "y_min, y_max = X_class[:, 1].min() - 1, X_class[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "\n",
        "# Predictions\n",
        "Z_linear = linear_reg.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
        "Z_logistic = logistic_reg.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1].reshape(xx.shape)\n",
        "\n",
        "# Visualization\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "# Plot 1: Linear Regression Predictions\n",
        "im1 = axes[0].contourf(xx, yy, Z_linear, levels=20, cmap='RdBu', alpha=0.6)\n",
        "axes[0].scatter(X_class[y_class == 0, 0], X_class[y_class == 0, 1], \n",
        "                c='blue', edgecolor='black', s=50, label='Class 0')\n",
        "axes[0].scatter(X_class[y_class == 1, 0], X_class[y_class == 1, 1], \n",
        "                c='red', edgecolor='black', s=50, label='Class 1')\n",
        "axes[0].contour(xx, yy, Z_linear, levels=[0.5], colors='green', linewidths=2)\n",
        "axes[0].set_title('Linear Regression\\n(Unbounded predictions)')\n",
        "axes[0].legend()\n",
        "plt.colorbar(im1, ax=axes[0])\n",
        "\n",
        "# Plot 2: Logistic Regression Probabilities\n",
        "im2 = axes[1].contourf(xx, yy, Z_logistic, levels=20, cmap='RdBu', alpha=0.6)\n",
        "axes[1].scatter(X_class[y_class == 0, 0], X_class[y_class == 0, 1], \n",
        "                c='blue', edgecolor='black', s=50, label='Class 0')\n",
        "axes[1].scatter(X_class[y_class == 1, 0], X_class[y_class == 1, 1], \n",
        "                c='red', edgecolor='black', s=50, label='Class 1')\n",
        "axes[1].contour(xx, yy, Z_logistic, levels=[0.5], colors='green', linewidths=2)\n",
        "axes[1].set_title('Logistic Regression\\n(Probabilities in [0,1])')\n",
        "axes[1].legend()\n",
        "plt.colorbar(im2, ax=axes[1])\n",
        "\n",
        "# Plot 3: Histogram of predictions\n",
        "linear_preds = linear_reg.predict(X_class)\n",
        "logistic_preds = logistic_reg.predict_proba(X_class)[:, 1]\n",
        "\n",
        "axes[2].hist(linear_preds, bins=30, alpha=0.5, label='Linear Reg', color='blue')\n",
        "axes[2].hist(logistic_preds, bins=30, alpha=0.5, label='Logistic Reg', color='red')\n",
        "axes[2].axvline(0, color='black', linestyle='--', alpha=0.5)\n",
        "axes[2].axvline(1, color='black', linestyle='--', alpha=0.5)\n",
        "axes[2].set_xlabel('Predicted Value')\n",
        "axes[2].set_ylabel('Frequency')\n",
        "axes[2].set_title('Distribution of Predictions')\n",
        "axes[2].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nProblems with Linear Regression for Classification:\")\n",
        "print(f\"1. Predictions outside [0,1]: {np.sum((linear_preds < 0) | (linear_preds > 1))} out of {len(linear_preds)}\")\n",
        "print(f\"   Min prediction: {linear_preds.min():.3f}\")\n",
        "print(f\"   Max prediction: {linear_preds.max():.3f}\")\n",
        "print(\"\\n2. Sensitive to outliers (see how outliers affect the decision boundary)\")\n",
        "print(\"3. Inappropriate loss function (squared error doesn't make sense for classes)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Exercise 5: Understanding Sigmoid Function and Odds\n",
        "\n",
        "### Concept\n",
        "The sigmoid function $\\sigma(z) = \\frac{1}{1 + e^{-z}}$ maps any real number to (0,1):\n",
        "- **Odds**: $\\frac{p}{1-p}$ (ratio of probability to its complement)\n",
        "- **Log-odds (logit)**: $\\log\\left(\\frac{p}{1-p}\\right)$ (can take any real value)\n",
        "\n",
        "### Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sigmoid(z):\n",
        "    \"\"\"Sigmoid activation function\"\"\"\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def sigmoid_derivative(z):\n",
        "    \"\"\"Derivative of sigmoid function\"\"\"\n",
        "    s = sigmoid(z)\n",
        "    return s * (1 - s)\n",
        "\n",
        "def odds(p):\n",
        "    \"\"\"Calculate odds from probability\"\"\"\n",
        "    return p / (1 - p)\n",
        "\n",
        "def log_odds(p):\n",
        "    \"\"\"Calculate log-odds from probability\"\"\"\n",
        "    return np.log(odds(p))\n",
        "\n",
        "# Create interactive visualization\n",
        "z = np.linspace(-10, 10, 1000)\n",
        "\n",
        "# Create subplots\n",
        "fig = make_subplots(\n",
        "    rows=2, cols=2,\n",
        "    subplot_titles=('Sigmoid Function', 'Sigmoid Derivative', \n",
        "                   'Probability → Odds', 'Probability → Log-Odds')\n",
        ")\n",
        "\n",
        "# Sigmoid function\n",
        "fig.add_trace(\n",
        "    go.Scatter(x=z, y=sigmoid(z), name='σ(z)', line=dict(color='blue', width=3)),\n",
        "    row=1, col=1\n",
        ")\n",
        "fig.add_hline(y=0.5, line_dash=\"dash\", line_color=\"gray\", row=1, col=1)\n",
        "fig.add_vline(x=0, line_dash=\"dash\", line_color=\"gray\", row=1, col=1)\n",
        "\n",
        "# Sigmoid derivative\n",
        "fig.add_trace(\n",
        "    go.Scatter(x=z, y=sigmoid_derivative(z), name=\"σ'(z)\", \n",
        "               line=dict(color='green', width=3)),\n",
        "    row=1, col=2\n",
        ")\n",
        "\n",
        "# Odds transformation\n",
        "p_range = np.linspace(0.01, 0.99, 100)\n",
        "fig.add_trace(\n",
        "    go.Scatter(x=p_range, y=odds(p_range), name='Odds', \n",
        "               line=dict(color='orange', width=3)),\n",
        "    row=2, col=1\n",
        ")\n",
        "\n",
        "# Log-odds transformation\n",
        "fig.add_trace(\n",
        "    go.Scatter(x=p_range, y=log_odds(p_range), name='Log-Odds', \n",
        "               line=dict(color='red', width=3)),\n",
        "    row=2, col=2\n",
        ")\n",
        "\n",
        "# Update layout\n",
        "fig.update_xaxes(title_text=\"z\", row=1, col=1)\n",
        "fig.update_xaxes(title_text=\"z\", row=1, col=2)\n",
        "fig.update_xaxes(title_text=\"Probability\", row=2, col=1)\n",
        "fig.update_xaxes(title_text=\"Probability\", row=2, col=2)\n",
        "\n",
        "fig.update_yaxes(title_text=\"σ(z)\", row=1, col=1)\n",
        "fig.update_yaxes(title_text=\"σ'(z)\", row=1, col=2)\n",
        "fig.update_yaxes(title_text=\"Odds\", row=2, col=1)\n",
        "fig.update_yaxes(title_text=\"Log-Odds\", row=2, col=2)\n",
        "\n",
        "fig.update_layout(height=700, showlegend=False,\n",
        "                 title_text=\"Sigmoid Function and Related Transformations\")\n",
        "fig.show()\n",
        "\n",
        "# Key properties table\n",
        "print(\"\\nKey Properties of Sigmoid Function:\")\n",
        "print(\"=\" * 50)\n",
        "properties = pd.DataFrame([\n",
        "    ['σ(0)', sigmoid(0)],\n",
        "    ['σ(-∞)', 0],\n",
        "    ['σ(+∞)', 1],\n",
        "    ['σ(z) + σ(-z)', 1],\n",
        "    [\"Max of σ'(z)\", sigmoid_derivative(0)],\n",
        "    [\"Occurs at z=\", 0]\n",
        "], columns=['Property', 'Value'])\n",
        "print(properties.to_string(index=False))\n",
        "\n",
        "print(\"\\n\\nProbability ↔ Odds ↔ Log-Odds Examples:\")\n",
        "print(\"=\" * 50)\n",
        "for p in [0.1, 0.5, 0.75, 0.9, 0.99]:\n",
        "    print(f\"P = {p:.2f} → Odds = {odds(p):.3f} → Log-Odds = {log_odds(p):.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Exercise 6: Perceptron Algorithm - The Simplest Linear Classifier\n",
        "\n",
        "### Concept\n",
        "The Perceptron (1957) is the foundation of neural networks:\n",
        "- Update rule: $w \\leftarrow w + \\eta(y - \\hat{y})x$\n",
        "- Converges if data is linearly separable\n",
        "- No convergence guarantee for non-separable data\n",
        "\n",
        "### Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SimplePerceptron:\n",
        "    \"\"\"Perceptron implementation from scratch\"\"\"\n",
        "    \n",
        "    def __init__(self, learning_rate=0.01, n_iterations=100):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_iterations = n_iterations\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "        self.errors_per_iteration = []\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        \n",
        "        # Initialize parameters\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "        \n",
        "        # Convert labels to -1, 1\n",
        "        y_converted = np.where(y <= 0, -1, 1)\n",
        "        \n",
        "        # Training loop\n",
        "        for iteration in range(self.n_iterations):\n",
        "            errors = 0\n",
        "            \n",
        "            for idx, x_i in enumerate(X):\n",
        "                # Linear output\n",
        "                linear_output = np.dot(x_i, self.weights) + self.bias\n",
        "                # Prediction\n",
        "                y_pred = np.sign(linear_output)\n",
        "                \n",
        "                # Update if misclassified\n",
        "                if y_converted[idx] != y_pred:\n",
        "                    update = self.learning_rate * y_converted[idx]\n",
        "                    self.weights += update * x_i\n",
        "                    self.bias += update\n",
        "                    errors += 1\n",
        "            \n",
        "            self.errors_per_iteration.append(errors)\n",
        "            \n",
        "            # Stop if no errors\n",
        "            if errors == 0:\n",
        "                print(f\"Converged at iteration {iteration + 1}\")\n",
        "                break\n",
        "    \n",
        "    def predict(self, X):\n",
        "        linear_output = np.dot(X, self.weights) + self.bias\n",
        "        return np.where(np.sign(linear_output) <= 0, 0, 1)\n",
        "\n",
        "# Generate linearly separable and non-separable datasets\n",
        "np.random.seed(42)\n",
        "\n",
        "# Dataset 1: Linearly separable\n",
        "X_sep, y_sep = make_classification(n_samples=100, n_features=2, n_redundant=0,\n",
        "                                   n_informative=2, random_state=1,\n",
        "                                   n_clusters_per_class=1)\n",
        "\n",
        "# Dataset 2: Non-linearly separable (XOR-like)\n",
        "X_nonsep = np.random.randn(200, 2)\n",
        "y_nonsep = np.logical_xor(X_nonsep[:, 0] > 0, X_nonsep[:, 1] > 0).astype(int)\n",
        "\n",
        "# Train perceptrons\n",
        "perceptron_sep = SimplePerceptron(learning_rate=0.1, n_iterations=100)\n",
        "perceptron_nonsep = SimplePerceptron(learning_rate=0.1, n_iterations=100)\n",
        "\n",
        "perceptron_sep.fit(X_sep, y_sep)\n",
        "perceptron_nonsep.fit(X_nonsep, y_nonsep)\n",
        "\n",
        "# Compare with sklearn's Perceptron\n",
        "sklearn_perceptron_sep = Perceptron(random_state=42)\n",
        "sklearn_perceptron_sep.fit(X_sep, y_sep)\n",
        "\n",
        "# Visualization\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "\n",
        "# Helper function to plot decision boundary\n",
        "def plot_decision_boundary(ax, X, y, model, title):\n",
        "    h = 0.02\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "    \n",
        "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    \n",
        "    ax.contourf(xx, yy, Z, alpha=0.3, cmap='RdBu')\n",
        "    ax.scatter(X[y == 0, 0], X[y == 0, 1], c='blue', s=50, edgecolor='black')\n",
        "    ax.scatter(X[y == 1, 0], X[y == 1, 1], c='red', s=50, edgecolor='black')\n",
        "    ax.set_title(title)\n",
        "    ax.set_xlabel('Feature 1')\n",
        "    ax.set_ylabel('Feature 2')\n",
        "\n",
        "# Row 1: Linearly separable data\n",
        "plot_decision_boundary(axes[0, 0], X_sep, y_sep, perceptron_sep, \n",
        "                      'Linearly Separable\\n(Our Perceptron)')\n",
        "plot_decision_boundary(axes[0, 1], X_sep, y_sep, sklearn_perceptron_sep, \n",
        "                      'Linearly Separable\\n(Sklearn Perceptron)')\n",
        "\n",
        "# Convergence plot\n",
        "axes[0, 2].plot(perceptron_sep.errors_per_iteration, 'b-o', markersize=4)\n",
        "axes[0, 2].set_xlabel('Iteration')\n",
        "axes[0, 2].set_ylabel('Number of Errors')\n",
        "axes[0, 2].set_title('Convergence (Linearly Separable)')\n",
        "axes[0, 2].grid(True, alpha=0.3)\n",
        "\n",
        "# Row 2: Non-linearly separable data\n",
        "plot_decision_boundary(axes[1, 0], X_nonsep, y_nonsep, perceptron_nonsep, \n",
        "                      'XOR Problem\\n(Non-separable)')\n",
        "\n",
        "# Show actual XOR pattern\n",
        "axes[1, 1].scatter(X_nonsep[y_nonsep == 0, 0], X_nonsep[y_nonsep == 0, 1], \n",
        "                   c='blue', s=20, alpha=0.5, label='Class 0')\n",
        "axes[1, 1].scatter(X_nonsep[y_nonsep == 1, 0], X_nonsep[y_nonsep == 1, 1], \n",
        "                   c='red', s=20, alpha=0.5, label='Class 1')\n",
        "axes[1, 1].set_title('XOR Pattern\\n(Cannot be separated linearly)')\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Non-convergence plot\n",
        "axes[1, 2].plot(perceptron_nonsep.errors_per_iteration, 'r-o', markersize=4)\n",
        "axes[1, 2].set_xlabel('Iteration')\n",
        "axes[1, 2].set_ylabel('Number of Errors')\n",
        "axes[1, 2].set_title('No Convergence (Non-separable)')\n",
        "axes[1, 2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nKey Observations:\")\n",
        "print(\"1. Perceptron converges perfectly on linearly separable data\")\n",
        "print(\"2. Cannot learn XOR pattern (non-linearly separable)\")\n",
        "print(\"3. This limitation led to the development of multi-layer perceptrons (neural networks)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Exercise 7: Implementing Logistic Regression from Scratch\n",
        "\n",
        "### Concept\n",
        "Logistic Regression uses:\n",
        "- Maximum Likelihood Estimation (MLE)\n",
        "- Binary Cross-Entropy Loss: $L = -[y\\log(\\hat{y}) + (1-y)\\log(1-\\hat{y})]$\n",
        "- Gradient Descent optimization\n",
        "\n",
        "### Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LogisticRegressionFromScratch:\n",
        "    \"\"\"Logistic Regression implementation using gradient descent\"\"\"\n",
        "    \n",
        "    def __init__(self, learning_rate=0.01, n_iterations=1000, regularization=None, lambda_reg=0.01):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_iterations = n_iterations\n",
        "        self.regularization = regularization  # None, 'l2', or 'l1'\n",
        "        self.lambda_reg = lambda_reg\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "        self.losses = []\n",
        "    \n",
        "    def sigmoid(self, z):\n",
        "        \"\"\"Sigmoid activation function\"\"\"\n",
        "        # Clip to prevent overflow\n",
        "        z = np.clip(z, -500, 500)\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "    \n",
        "    def binary_cross_entropy(self, y_true, y_pred):\n",
        "        \"\"\"Binary cross-entropy loss\"\"\"\n",
        "        # Add small epsilon to prevent log(0)\n",
        "        epsilon = 1e-7\n",
        "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
        "        \n",
        "        loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
        "        \n",
        "        # Add regularization term\n",
        "        if self.regularization == 'l2':\n",
        "            loss += self.lambda_reg * np.sum(self.weights ** 2) / (2 * len(y_true))\n",
        "        elif self.regularization == 'l1':\n",
        "            loss += self.lambda_reg * np.sum(np.abs(self.weights)) / len(y_true)\n",
        "        \n",
        "        return loss\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        \n",
        "        # Initialize parameters\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "        \n",
        "        # Gradient descent\n",
        "        for i in range(self.n_iterations):\n",
        "            # Forward pass\n",
        "            z = np.dot(X, self.weights) + self.bias\n",
        "            y_pred = self.sigmoid(z)\n",
        "            \n",
        "            # Calculate loss\n",
        "            loss = self.binary_cross_entropy(y, y_pred)\n",
        "            self.losses.append(loss)\n",
        "            \n",
        "            # Backward pass (gradients)\n",
        "            dw = np.dot(X.T, (y_pred - y)) / n_samples\n",
        "            db = np.sum(y_pred - y) / n_samples\n",
        "            \n",
        "            # Add regularization gradient\n",
        "            if self.regularization == 'l2':\n",
        "                dw += self.lambda_reg * self.weights / n_samples\n",
        "            elif self.regularization == 'l1':\n",
        "                dw += self.lambda_reg * np.sign(self.weights) / n_samples\n",
        "            \n",
        "            # Update parameters\n",
        "            self.weights -= self.learning_rate * dw\n",
        "            self.bias -= self.learning_rate * db\n",
        "            \n",
        "            # Print progress\n",
        "            if i % 100 == 0:\n",
        "                print(f'Iteration {i}, Loss: {loss:.4f}')\n",
        "    \n",
        "    def predict_proba(self, X):\n",
        "        z = np.dot(X, self.weights) + self.bias\n",
        "        return self.sigmoid(z)\n",
        "    \n",
        "    def predict(self, X, threshold=0.5):\n",
        "        return (self.predict_proba(X) >= threshold).astype(int)\n",
        "\n",
        "# Generate dataset\n",
        "X, y = make_classification(n_samples=500, n_features=2, n_redundant=0,\n",
        "                          n_informative=2, random_state=42,\n",
        "                          n_clusters_per_class=1, flip_y=0.1)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Standardize\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train models with different regularization\n",
        "models = {\n",
        "    'No Regularization': LogisticRegressionFromScratch(learning_rate=0.1, n_iterations=500),\n",
        "    'L2 Regularization': LogisticRegressionFromScratch(learning_rate=0.1, n_iterations=500, \n",
        "                                                       regularization='l2', lambda_reg=0.1),\n",
        "    'L1 Regularization': LogisticRegressionFromScratch(learning_rate=0.1, n_iterations=500, \n",
        "                                                       regularization='l1', lambda_reg=0.1)\n",
        "}\n",
        "\n",
        "# Train and evaluate\n",
        "results = {}\n",
        "for name, model in models.items():\n",
        "    print(f\"\\n{name}:\")\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    \n",
        "    # Predictions\n",
        "    train_pred = model.predict(X_train_scaled)\n",
        "    test_pred = model.predict(X_test_scaled)\n",
        "    \n",
        "    # Store results\n",
        "    results[name] = {\n",
        "        'model': model,\n",
        "        'train_acc': accuracy_score(y_train, train_pred),\n",
        "        'test_acc': accuracy_score(y_test, test_pred)\n",
        "    }\n",
        "\n",
        "# Visualization\n",
        "fig = plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Plot 1: Loss curves\n",
        "ax1 = plt.subplot(2, 3, 1)\n",
        "for name, data in results.items():\n",
        "    ax1.plot(data['model'].losses, label=name, linewidth=2)\n",
        "ax1.set_xlabel('Iteration')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.set_title('Training Loss Curves')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Plots 2-4: Decision boundaries\n",
        "for idx, (name, data) in enumerate(results.items()):\n",
        "    ax = plt.subplot(2, 3, idx + 2)\n",
        "    \n",
        "    # Create mesh\n",
        "    h = 0.02\n",
        "    x_min, x_max = X_test_scaled[:, 0].min() - 1, X_test_scaled[:, 0].max() + 1\n",
        "    y_min, y_max = X_test_scaled[:, 1].min() - 1, X_test_scaled[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "    \n",
        "    Z = data['model'].predict_proba(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    \n",
        "    # Plot\n",
        "    ax.contourf(xx, yy, Z, levels=20, cmap='RdBu', alpha=0.6)\n",
        "    ax.scatter(X_test_scaled[y_test == 0, 0], X_test_scaled[y_test == 0, 1], \n",
        "              c='blue', edgecolor='black', s=50)\n",
        "    ax.scatter(X_test_scaled[y_test == 1, 0], X_test_scaled[y_test == 1, 1], \n",
        "              c='red', edgecolor='black', s=50)\n",
        "    ax.contour(xx, yy, Z, levels=[0.5], colors='green', linewidths=2)\n",
        "    ax.set_title(f'{name}\\nTest Acc: {data[\"test_acc\"]:.3f}')\n",
        "\n",
        "# Plot 5: Coefficient comparison\n",
        "ax5 = plt.subplot(2, 3, 5)\n",
        "width = 0.25\n",
        "x = np.arange(len(results[list(results.keys())[0]]['model'].weights))\n",
        "\n",
        "for idx, (name, data) in enumerate(results.items()):\n",
        "    ax5.bar(x + idx * width, np.abs(data['model'].weights), width, label=name)\n",
        "\n",
        "ax5.set_xlabel('Feature Index')\n",
        "ax5.set_ylabel('|Coefficient|')\n",
        "ax5.set_title('Coefficient Magnitudes')\n",
        "ax5.legend()\n",
        "ax5.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Summary\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Model Performance Summary:\")\n",
        "print(\"=\"*60)\n",
        "for name, data in results.items():\n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"  Train Accuracy: {data['train_acc']:.3f}\")\n",
        "    print(f\"  Test Accuracy:  {data['test_acc']:.3f}\")\n",
        "    print(f\"  Weights: {data['model'].weights}\")\n",
        "    print(f\"  Bias: {data['model'].bias:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Exercise 8: Model Evaluation and Threshold Optimization\n",
        "\n",
        "### Concept\n",
        "Classification metrics:\n",
        "- **ROC Curve**: True Positive Rate vs False Positive Rate\n",
        "- **AUC**: Area Under the Curve (higher is better)\n",
        "- **Threshold tuning**: Balance precision and recall\n",
        "\n",
        "### Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate imbalanced dataset\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15,\n",
        "                          n_redundant=5, weights=[0.9, 0.1], flip_y=0.05,\n",
        "                          random_state=42)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, \n",
        "                                                   random_state=42, stratify=y)\n",
        "\n",
        "# Standardize\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train logistic regression\n",
        "log_reg = LogisticRegression(random_state=42, max_iter=1000)\n",
        "log_reg.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Get predicted probabilities\n",
        "y_train_proba = log_reg.predict_proba(X_train_scaled)[:, 1]\n",
        "y_test_proba = log_reg.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "# Calculate ROC curves\n",
        "fpr_train, tpr_train, thresholds_train = roc_curve(y_train, y_train_proba)\n",
        "fpr_test, tpr_test, thresholds_test = roc_curve(y_test, y_test_proba)\n",
        "\n",
        "auc_train = roc_auc_score(y_train, y_train_proba)\n",
        "auc_test = roc_auc_score(y_test, y_test_proba)\n",
        "\n",
        "# Find optimal threshold using Youden's J statistic\n",
        "j_scores = tpr_test - fpr_test\n",
        "optimal_idx = np.argmax(j_scores)\n",
        "optimal_threshold = thresholds_test[optimal_idx]\n",
        "\n",
        "# Create interactive ROC curve\n",
        "fig = make_subplots(\n",
        "    rows=2, cols=2,\n",
        "    subplot_titles=('ROC Curves', 'Precision-Recall Curve',\n",
        "                   'Threshold Effects', 'Confusion Matrices')\n",
        ")\n",
        "\n",
        "# ROC Curves\n",
        "fig.add_trace(\n",
        "    go.Scatter(x=fpr_train, y=tpr_train, \n",
        "               name=f'Train (AUC={auc_train:.3f})',\n",
        "               line=dict(color='blue', width=2)),\n",
        "    row=1, col=1\n",
        ")\n",
        "fig.add_trace(\n",
        "    go.Scatter(x=fpr_test, y=tpr_test, \n",
        "               name=f'Test (AUC={auc_test:.3f})',\n",
        "               line=dict(color='red', width=2)),\n",
        "    row=1, col=1\n",
        ")\n",
        "fig.add_trace(\n",
        "    go.Scatter(x=[0, 1], y=[0, 1], \n",
        "               name='Random',\n",
        "               line=dict(color='gray', width=1, dash='dash')),\n",
        "    row=1, col=1\n",
        ")\n",
        "fig.add_trace(\n",
        "    go.Scatter(x=[fpr_test[optimal_idx]], y=[tpr_test[optimal_idx]],\n",
        "               mode='markers', name=f'Optimal (t={optimal_threshold:.3f})',\n",
        "               marker=dict(size=12, color='green')),\n",
        "    row=1, col=1\n",
        ")\n",
        "\n",
        "# Precision-Recall Curve\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "precision, recall, pr_thresholds = precision_recall_curve(y_test, y_test_proba)\n",
        "\n",
        "fig.add_trace(\n",
        "    go.Scatter(x=recall, y=precision, \n",
        "               name='PR Curve',\n",
        "               line=dict(color='purple', width=2)),\n",
        "    row=1, col=2\n",
        ")\n",
        "\n",
        "# Threshold effects\n",
        "thresholds_to_test = np.linspace(0, 1, 50)\n",
        "accuracies = []\n",
        "precisions = []\n",
        "recalls = []\n",
        "f1_scores = []\n",
        "\n",
        "for t in thresholds_to_test:\n",
        "    y_pred_t = (y_test_proba >= t).astype(int)\n",
        "    \n",
        "    accuracies.append(accuracy_score(y_test, y_pred_t))\n",
        "    \n",
        "    if y_pred_t.sum() > 0:  # Avoid division by zero\n",
        "        precisions.append(precision_score(y_test, y_pred_t, zero_division=0))\n",
        "        recalls.append(recall_score(y_test, y_pred_t, zero_division=0))\n",
        "        f1_scores.append(f1_score(y_test, y_pred_t, zero_division=0))\n",
        "    else:\n",
        "        precisions.append(0)\n",
        "        recalls.append(0)\n",
        "        f1_scores.append(0)\n",
        "\n",
        "fig.add_trace(\n",
        "    go.Scatter(x=thresholds_to_test, y=accuracies, \n",
        "               name='Accuracy', line=dict(color='blue', width=2)),\n",
        "    row=2, col=1\n",
        ")\n",
        "fig.add_trace(\n",
        "    go.Scatter(x=thresholds_to_test, y=precisions, \n",
        "               name='Precision', line=dict(color='green', width=2)),\n",
        "    row=2, col=1\n",
        ")\n",
        "fig.add_trace(\n",
        "    go.Scatter(x=thresholds_to_test, y=recalls, \n",
        "               name='Recall', line=dict(color='red', width=2)),\n",
        "    row=2, col=1\n",
        ")\n",
        "fig.add_trace(\n",
        "    go.Scatter(x=thresholds_to_test, y=f1_scores, \n",
        "               name='F1-Score', line=dict(color='purple', width=2)),\n",
        "    row=2, col=1\n",
        ")\n",
        "\n",
        "# Update layout\n",
        "fig.update_xaxes(title_text=\"False Positive Rate\", row=1, col=1)\n",
        "fig.update_yaxes(title_text=\"True Positive Rate\", row=1, col=1)\n",
        "fig.update_xaxes(title_text=\"Recall\", row=1, col=2)\n",
        "fig.update_yaxes(title_text=\"Precision\", row=1, col=2)\n",
        "fig.update_xaxes(title_text=\"Threshold\", row=2, col=1)\n",
        "fig.update_yaxes(title_text=\"Score\", row=2, col=1)\n",
        "\n",
        "fig.update_layout(height=800, title_text=\"Comprehensive Model Evaluation\")\n",
        "fig.show()\n",
        "\n",
        "# Confusion matrices for different thresholds\n",
        "fig2, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "thresholds_compare = [0.3, 0.5, optimal_threshold]\n",
        "titles = ['Low Threshold (0.3)', 'Default (0.5)', f'Optimal ({optimal_threshold:.3f})']\n",
        "\n",
        "for ax, t, title in zip(axes, thresholds_compare, titles):\n",
        "    y_pred = (y_test_proba >= t).astype(int)\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    \n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax, cbar=False)\n",
        "    ax.set_title(title)\n",
        "    ax.set_xlabel('Predicted')\n",
        "    ax.set_ylabel('Actual')\n",
        "    \n",
        "    # Add metrics\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    prec = precision_score(y_test, y_pred, zero_division=0)\n",
        "    rec = recall_score(y_test, y_pred, zero_division=0)\n",
        "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
        "    \n",
        "    ax.text(0.5, -0.15, f'Acc: {acc:.3f}, Prec: {prec:.3f}\\nRec: {rec:.3f}, F1: {f1:.3f}',\n",
        "           transform=ax.transAxes, ha='center')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nClass Distribution:\")\n",
        "print(f\"Training set: {np.bincount(y_train)}\")\n",
        "print(f\"Test set: {np.bincount(y_test)}\")\n",
        "print(f\"\\nClass imbalance ratio: {np.bincount(y_test)[0]/np.bincount(y_test)[1]:.2f}:1\")\n",
        "print(f\"\\nOptimal threshold (Youden's J): {optimal_threshold:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Exercise 9: Multiclass Classification - OvR vs Softmax\n",
        "\n",
        "### Concept\n",
        "Two main strategies for multiclass:\n",
        "1. **One-vs-Rest (OvR)**: Train K binary classifiers\n",
        "2. **Softmax (Multinomial)**: Native multiclass with probability distribution\n",
        "\n",
        "Softmax function: $P(y=k|x) = \\frac{e^{w_k^Tx}}{\\sum_j e^{w_j^Tx}}$\n",
        "\n",
        "### Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Iris dataset for multiclass\n",
        "iris = load_iris()\n",
        "X_iris = iris.data\n",
        "y_iris = iris.target\n",
        "\n",
        "# Use only 2 features for visualization\n",
        "X_iris_2d = X_iris[:, [0, 2]]  # Sepal length and petal length\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_iris_2d, y_iris, test_size=0.3, random_state=42, stratify=y_iris\n",
        ")\n",
        "\n",
        "# Standardize\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train different multiclass strategies\n",
        "models = {\n",
        "    'One-vs-Rest': OneVsRestClassifier(LogisticRegression(random_state=42)),\n",
        "    'Softmax (Multinomial)': LogisticRegression(multi_class='multinomial', random_state=42),\n",
        "    'Auto (Default)': LogisticRegression(random_state=42)\n",
        "}\n",
        "\n",
        "# Train models\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Create visualization\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "\n",
        "# Helper function for decision regions\n",
        "def plot_decision_regions_multi(ax, X, y, model, title):\n",
        "    h = 0.02\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "    \n",
        "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    \n",
        "    ax.contourf(xx, yy, Z, alpha=0.3, cmap='viridis')\n",
        "    \n",
        "    # Plot data points\n",
        "    colors = ['blue', 'red', 'green']\n",
        "    markers = ['o', 's', '^']\n",
        "    for i in range(3):\n",
        "        idx = y == i\n",
        "        ax.scatter(X[idx, 0], X[idx, 1], c=colors[i], \n",
        "                  marker=markers[i], s=50, edgecolor='black',\n",
        "                  label=iris.target_names[i])\n",
        "    \n",
        "    ax.set_title(title)\n",
        "    ax.set_xlabel('Sepal Length (scaled)')\n",
        "    ax.set_ylabel('Petal Length (scaled)')\n",
        "    ax.legend()\n",
        "\n",
        "# Row 1: Decision boundaries\n",
        "for idx, (name, model) in enumerate(models.items()):\n",
        "    plot_decision_regions_multi(axes[0, idx], X_test_scaled, y_test, model, \n",
        "                               f'{name}\\nTest Acc: {model.score(X_test_scaled, y_test):.3f}')\n",
        "\n",
        "# Row 2: Probability contours for Softmax model\n",
        "softmax_model = models['Softmax (Multinomial)']\n",
        "\n",
        "h = 0.02\n",
        "x_min, x_max = X_test_scaled[:, 0].min() - 1, X_test_scaled[:, 0].max() + 1\n",
        "y_min, y_max = X_test_scaled[:, 1].min() - 1, X_test_scaled[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "\n",
        "# Get probabilities for each class\n",
        "Z_proba = softmax_model.predict_proba(np.c_[xx.ravel(), yy.ravel()])\n",
        "\n",
        "for i in range(3):\n",
        "    Z_class = Z_proba[:, i].reshape(xx.shape)\n",
        "    \n",
        "    im = axes[1, i].contourf(xx, yy, Z_class, levels=20, cmap='RdBu_r', alpha=0.7)\n",
        "    axes[1, i].scatter(X_test_scaled[y_test == i, 0], \n",
        "                      X_test_scaled[y_test == i, 1],\n",
        "                      c='black', s=50, edgecolor='white')\n",
        "    axes[1, i].set_title(f'P(y={iris.target_names[i]}|x)')\n",
        "    axes[1, i].set_xlabel('Sepal Length (scaled)')\n",
        "    axes[1, i].set_ylabel('Petal Length (scaled)')\n",
        "    plt.colorbar(im, ax=axes[1, i])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Compare predictions and probabilities\n",
        "print(\"\\nSample Predictions Comparison:\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "sample_idx = [0, 10, 20, 30, 40]\n",
        "X_samples = X_test_scaled[sample_idx]\n",
        "y_true_samples = y_test[sample_idx]\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\n{name}:\")\n",
        "    predictions = model.predict(X_samples)\n",
        "    probabilities = model.predict_proba(X_samples)\n",
        "    \n",
        "    for i in range(len(sample_idx)):\n",
        "        print(f\"  Sample {i}: True={iris.target_names[y_true_samples[i]]}, \"\n",
        "              f\"Pred={iris.target_names[predictions[i]]}, \"\n",
        "              f\"Probs={probabilities[i].round(3)}\")\n",
        "\n",
        "# Classification reports\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Classification Reports:\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\n{name}:\")\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "    print(classification_report(y_test, y_pred, target_names=iris.target_names))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Exercise 10: Softmax Regression Implementation\n",
        "\n",
        "### Concept\n",
        "Softmax regression generalizes logistic regression to K classes:\n",
        "- Uses categorical cross-entropy loss\n",
        "- Outputs probability distribution over all classes\n",
        "- Each class has its own weight vector\n",
        "\n",
        "### Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SoftmaxRegression:\n",
        "    \"\"\"Softmax Regression (Multinomial Logistic Regression) from scratch\"\"\"\n",
        "    \n",
        "    def __init__(self, learning_rate=0.01, n_iterations=1000, reg_lambda=0.01):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_iterations = n_iterations\n",
        "        self.reg_lambda = reg_lambda\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "        self.losses = []\n",
        "    \n",
        "    def softmax(self, z):\n",
        "        \"\"\"Softmax function for multi-class probabilities\"\"\"\n",
        "        # Subtract max for numerical stability\n",
        "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
        "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
        "    \n",
        "    def one_hot_encode(self, y, n_classes):\n",
        "        \"\"\"Convert labels to one-hot encoding\"\"\"\n",
        "        one_hot = np.zeros((len(y), n_classes))\n",
        "        one_hot[np.arange(len(y)), y] = 1\n",
        "        return one_hot\n",
        "    \n",
        "    def categorical_cross_entropy(self, y_true, y_pred):\n",
        "        \"\"\"Categorical cross-entropy loss\"\"\"\n",
        "        n_samples = len(y_true)\n",
        "        # Add small epsilon to prevent log(0)\n",
        "        epsilon = 1e-7\n",
        "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
        "        \n",
        "        # Calculate loss\n",
        "        loss = -np.sum(y_true * np.log(y_pred)) / n_samples\n",
        "        \n",
        "        # Add L2 regularization\n",
        "        loss += self.reg_lambda * np.sum(self.weights ** 2) / (2 * n_samples)\n",
        "        \n",
        "        return loss\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        self.n_classes = len(np.unique(y))\n",
        "        \n",
        "        # Initialize weights and bias\n",
        "        self.weights = np.random.randn(n_features, self.n_classes) * 0.01\n",
        "        self.bias = np.zeros(self.n_classes)\n",
        "        \n",
        "        # One-hot encode labels\n",
        "        y_one_hot = self.one_hot_encode(y, self.n_classes)\n",
        "        \n",
        "        # Gradient descent\n",
        "        for i in range(self.n_iterations):\n",
        "            # Forward pass\n",
        "            z = np.dot(X, self.weights) + self.bias\n",
        "            y_pred = self.softmax(z)\n",
        "            \n",
        "            # Calculate loss\n",
        "            loss = self.categorical_cross_entropy(y_one_hot, y_pred)\n",
        "            self.losses.append(loss)\n",
        "            \n",
        "            # Backward pass\n",
        "            dz = (y_pred - y_one_hot) / n_samples\n",
        "            dw = np.dot(X.T, dz) + self.reg_lambda * self.weights / n_samples\n",
        "            db = np.sum(dz, axis=0)\n",
        "            \n",
        "            # Update parameters\n",
        "            self.weights -= self.learning_rate * dw\n",
        "            self.bias -= self.learning_rate * db\n",
        "            \n",
        "            if i % 100 == 0:\n",
        "                print(f'Iteration {i}, Loss: {loss:.4f}')\n",
        "    \n",
        "    def predict_proba(self, X):\n",
        "        z = np.dot(X, self.weights) + self.bias\n",
        "        return self.softmax(z)\n",
        "    \n",
        "    def predict(self, X):\n",
        "        return np.argmax(self.predict_proba(X), axis=1)\n",
        "    \n",
        "    def score(self, X, y):\n",
        "        return np.mean(self.predict(X) == y)\n",
        "\n",
        "# Create synthetic multiclass dataset\n",
        "X, y = make_classification(n_samples=600, n_features=20, n_informative=15,\n",
        "                          n_redundant=5, n_classes=4, random_state=42)\n",
        "\n",
        "# Split and scale\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, \n",
        "                                                   random_state=42, stratify=y)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train our softmax regression\n",
        "print(\"Training Softmax Regression from scratch...\")\n",
        "softmax_reg = SoftmaxRegression(learning_rate=0.1, n_iterations=500, reg_lambda=0.01)\n",
        "softmax_reg.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Compare with sklearn\n",
        "print(\"\\nTraining sklearn's Logistic Regression (multinomial)...\")\n",
        "sklearn_softmax = LogisticRegression(multi_class='multinomial', max_iter=500)\n",
        "sklearn_softmax.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Evaluate models\n",
        "our_train_acc = softmax_reg.score(X_train_scaled, y_train)\n",
        "our_test_acc = softmax_reg.score(X_test_scaled, y_test)\n",
        "sklearn_train_acc = sklearn_softmax.score(X_train_scaled, y_train)\n",
        "sklearn_test_acc = sklearn_softmax.score(X_test_scaled, y_test)\n",
        "\n",
        "# Visualization\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "\n",
        "# Plot 1: Loss curve\n",
        "axes[0, 0].plot(softmax_reg.losses, 'b-', linewidth=2)\n",
        "axes[0, 0].set_xlabel('Iteration')\n",
        "axes[0, 0].set_ylabel('Loss')\n",
        "axes[0, 0].set_title('Training Loss (Categorical Cross-Entropy)')\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Confusion matrix (our implementation)\n",
        "from sklearn.metrics import confusion_matrix\n",
        "y_pred_our = softmax_reg.predict(X_test_scaled)\n",
        "cm_our = confusion_matrix(y_test, y_pred_our)\n",
        "sns.heatmap(cm_our, annot=True, fmt='d', cmap='Blues', ax=axes[0, 1])\n",
        "axes[0, 1].set_title(f'Our Implementation\\nTest Acc: {our_test_acc:.3f}')\n",
        "axes[0, 1].set_xlabel('Predicted')\n",
        "axes[0, 1].set_ylabel('Actual')\n",
        "\n",
        "# Plot 3: Confusion matrix (sklearn)\n",
        "y_pred_sklearn = sklearn_softmax.predict(X_test_scaled)\n",
        "cm_sklearn = confusion_matrix(y_test, y_pred_sklearn)\n",
        "sns.heatmap(cm_sklearn, annot=True, fmt='d', cmap='Greens', ax=axes[1, 0])\n",
        "axes[1, 0].set_title(f'Sklearn Implementation\\nTest Acc: {sklearn_test_acc:.3f}')\n",
        "axes[1, 0].set_xlabel('Predicted')\n",
        "axes[1, 0].set_ylabel('Actual')\n",
        "\n",
        "# Plot 4: Probability distributions for sample points\n",
        "n_samples_plot = 10\n",
        "sample_probs = softmax_reg.predict_proba(X_test_scaled[:n_samples_plot])\n",
        "\n",
        "x_pos = np.arange(n_samples_plot)\n",
        "width = 0.2\n",
        "colors = ['blue', 'green', 'red', 'orange']\n",
        "colors = ['blue', 'green', 'red', 'orange']\n",
        "\n",
        "for i in range(4):\n",
        "    axes[1, 1].bar(x_pos + i * width, sample_probs[:, i], width,\n",
        "                   label=f'Class {i}', color=colors[i])\n",
        "\n",
        "axes[1, 1].set_xlabel('Sample Index')\n",
        "axes[1, 1].set_ylabel('Probability')\n",
        "axes[1, 1].set_title('Predicted Probability Distribution\\n(First 10 test samples)')\n",
        "axes[1, 1].set_xticks(x_pos + 1.5 * width)\n",
        "axes[1, 1].set_xticklabels([f'S{i}' for i in range(n_samples_plot)])\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nModel Comparison:\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Our Implementation:\")\n",
        "print(f\"  Train Accuracy: {our_train_acc:.3f}\")\n",
        "print(f\"  Test Accuracy:  {our_test_acc:.3f}\")\n",
        "print(f\"\\nSklearn Implementation:\")\n",
        "print(f\"  Train Accuracy: {sklearn_train_acc:.3f}\")\n",
        "print(f\"  Test Accuracy:  {sklearn_test_acc:.3f}\")\n",
        "print(\"\\nOur implementation achieves comparable performance!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Turn\n",
        "1. Implement early stopping based on validation loss\n",
        "2. Add dropout regularization to prevent overfitting\n",
        "3. Compare different optimization algorithms (SGD, Adam, etc.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Congratulations! 🎉\n",
        "\n",
        "You've completed a comprehensive journey from Linear to Logistic Regression!\n",
        "\n",
        "### What you've learned:\n",
        "✅ Advanced linear regression techniques (polynomial, regularization)  \n",
        "✅ Why and how to transition to classification  \n",
        "✅ Logistic regression theory and implementation  \n",
        "✅ Multiclass classification strategies  \n",
        "✅ Model evaluation and optimization  \n",
        "\n",
        "### Next Steps:\n",
        "1. **Deep Learning**: Neural networks extend these concepts\n",
        "2. **Tree-based Methods**: Decision trees, Random Forests, XGBoost\n",
        "3. **Support Vector Machines**: Another approach to classification\n",
        "4. **Ensemble Methods**: Combining multiple models\n",
        "\n",
        "Happy modeling! 🚀"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}