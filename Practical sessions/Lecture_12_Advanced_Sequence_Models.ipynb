{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a600731",
   "metadata": {},
   "source": [
    "# Advanced Sequence Models: Hands-on Practice\n",
    "## Based on Deep Learning for Natural Language Processing\n",
    "\n",
    "**Author**: Generated from Lecture 12  \n",
    "**Topics**: BiRNN, Seq2Seq, Attention Mechanism, Teacher Forcing  \n",
    "**Duration**: 2-3 hours hands-on practice\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Objectives:\n",
    "1. Understand and implement Bidirectional RNNs\n",
    "2. Build Sequence-to-Sequence models with Encoder-Decoder architecture\n",
    "3. Master Teacher Forcing training strategy\n",
    "4. Implement Attention Mechanism from scratch\n",
    "5. Apply practical techniques: batching, masking, and optimization\n",
    "\n",
    "### Prerequisites:\n",
    "- Basic understanding of RNNs/LSTMs\n",
    "- Python programming skills\n",
    "- Familiarity with PyTorch or TensorFlow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce303371",
   "metadata": {},
   "source": [
    "## Part 1: Environment Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61cc426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d069b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for visualization\n",
    "def plot_attention_weights(attention_weights, input_labels, output_labels, title=\"Attention Heatmap\"):\n",
    "    \"\"\"Visualize attention weights as a heatmap\"\"\"\n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=attention_weights,\n",
    "        x=input_labels,\n",
    "        y=output_labels,\n",
    "        colorscale='Blues',\n",
    "        showscale=True\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        xaxis_title=\"Input Sequence\",\n",
    "        yaxis_title=\"Output Sequence\",\n",
    "        width=700,\n",
    "        height=500\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def visualize_hidden_states(hidden_states, labels=None):\n",
    "    \"\"\"Visualize hidden states evolution\"\"\"\n",
    "    fig = px.line(hidden_states.T, \n",
    "                  title=\"Hidden State Evolution Over Time\",\n",
    "                  labels={'index': 'Hidden Unit', 'value': 'Activation'})\n",
    "    if labels:\n",
    "        fig.update_xaxis(ticktext=labels, tickvals=list(range(len(labels))))\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d3536e",
   "metadata": {},
   "source": [
    "## Part 2: Bidirectional RNNs - Core Concepts\n",
    "\n",
    "### Exercise 1: Understanding Bidirectional Processing\n",
    "\n",
    "**Concept**: Bidirectional RNNs process sequences in both forward and backward directions, capturing context from both past and future tokens. This is crucial for tasks where the entire context matters.\n",
    "\n",
    "**Key Formula**:\n",
    "- Forward: $\\vec{h}_t = \\text{RNN}(x_t, \\vec{h}_{t-1})$\n",
    "- Backward: $\\overleftarrow{h}_t = \\text{RNN}(x_t, \\overleftarrow{h}_{t+1})$\n",
    "- Output: $y_t = [\\vec{h}_t; \\overleftarrow{h}_t]$ (concatenation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378c831a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Implement a Bidirectional RNN from scratch\n",
    "class SimpleBiRNN(nn.Module):\n",
    "    \"\"\"Simple Bidirectional RNN implementation\"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleBiRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Forward and backward RNN layers\n",
    "        self.rnn_forward = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.rnn_backward = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        \n",
    "        # Output layer (2*hidden_size because of concatenation)\n",
    "        self.fc = nn.Linear(2 * hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Forward pass\n",
    "        out_forward, _ = self.rnn_forward(x)\n",
    "        \n",
    "        # Backward pass (reverse the sequence)\n",
    "        x_reversed = torch.flip(x, dims=[1])\n",
    "        out_backward, _ = self.rnn_backward(x_reversed)\n",
    "        out_backward = torch.flip(out_backward, dims=[1])\n",
    "        \n",
    "        # Concatenate both directions\n",
    "        out_bi = torch.cat([out_forward, out_backward], dim=-1)\n",
    "        \n",
    "        # Final output\n",
    "        output = self.fc(out_bi)\n",
    "        return output, out_bi\n",
    "\n",
    "# Create synthetic sequence data for demonstration\n",
    "def create_sequence_data(n_samples=100, seq_length=10, n_features=5):\n",
    "    \"\"\"Create synthetic sequence data for BiRNN demonstration\"\"\"\n",
    "    X = np.random.randn(n_samples, seq_length, n_features).astype(np.float32)\n",
    "    # Simple pattern: sum of features at even positions\n",
    "    y = (X[:, ::2, :].sum(axis=(1, 2)) > 0).astype(np.float32)\n",
    "    return torch.tensor(X), torch.tensor(y)\n",
    "\n",
    "# Generate data and create model\n",
    "X_train, y_train = create_sequence_data(200, 10, 5)\n",
    "X_test, y_test = create_sequence_data(50, 10, 5)\n",
    "\n",
    "model_birnn = SimpleBiRNN(input_size=5, hidden_size=16, output_size=1)\n",
    "print(\"BiRNN Model Architecture:\")\n",
    "print(model_birnn)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model_birnn.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e82c00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the BiRNN model\n",
    "optimizer = optim.Adam(model_birnn.parameters(), lr=0.001)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Training loop\n",
    "losses = []\n",
    "model_birnn.train()\n",
    "\n",
    "for epoch in range(50):\n",
    "    optimizer.zero_grad()\n",
    "    output, hidden_states = model_birnn(X_train)\n",
    "    loss = criterion(output[:, -1, 0], y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Visualize training progress\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(losses)\n",
    "plt.title(\"BiRNN Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "\n",
    "# Analyze hidden states\n",
    "model_birnn.eval()\n",
    "with torch.no_grad():\n",
    "    _, hidden_bi = model_birnn(X_test[:5])\n",
    "    \n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(hidden_bi[0].numpy().T, aspect='auto', cmap='coolwarm')\n",
    "plt.colorbar()\n",
    "plt.title(\"Bidirectional Hidden States (Sample 1)\")\n",
    "plt.xlabel(\"Time Step\")\n",
    "plt.ylabel(\"Hidden Unit\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal test accuracy: {((torch.sigmoid(model_birnn(X_test)[0][:, -1, 0]) > 0.5) == y_test).float().mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a382a68",
   "metadata": {},
   "source": [
    "### ðŸŽ¯ Your Turn: BiRNN Analysis\n",
    "1. Modify the `SimpleBiRNN` to use LSTM instead of vanilla RNN\n",
    "2. Compare the performance difference\n",
    "3. Visualize how forward and backward states capture different patterns\n",
    "4. Try different sequence lengths and observe the impact\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21755182",
   "metadata": {},
   "source": [
    "## Part 3: Sequence-to-Sequence Architecture\n",
    "\n",
    "### Exercise 2: Building an Encoder-Decoder Model\n",
    "\n",
    "**Concept**: Seq2Seq models consist of an encoder that compresses input sequences into a context vector, and a decoder that generates output sequences from this context.\n",
    "\n",
    "**Architecture**:\n",
    "- Encoder: Processes input sequence â†’ Context vector\n",
    "- Context Vector: Fixed-size representation of entire input\n",
    "- Decoder: Generates output sequence from context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff251e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Complete Seq2Seq Implementation\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"Encoder network for Seq2Seq\"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Use LSTM for better gradient flow\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, seq_len, input_size)\n",
    "        outputs, (hidden, cell) = self.lstm(x)\n",
    "        # Return last hidden state as context\n",
    "        return hidden, cell\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"Decoder network for Seq2Seq\"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x, hidden, cell):\n",
    "        # x shape: (batch, 1, input_size) for single time step\n",
    "        output, (hidden, cell) = self.lstm(x, (hidden, cell))\n",
    "        prediction = self.fc(output)\n",
    "        return prediction, hidden, cell\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    \"\"\"Complete Seq2Seq model\"\"\"\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, source, target_len, teacher_forcing_ratio=0.5):\n",
    "        batch_size = source.size(0)\n",
    "        outputs = []\n",
    "        \n",
    "        # Encode the source sequence\n",
    "        hidden, cell = self.encoder(source)\n",
    "        \n",
    "        # Start with a start token (zeros in this example)\n",
    "        decoder_input = torch.zeros(batch_size, 1, source.size(2))\n",
    "        \n",
    "        for t in range(target_len):\n",
    "            output, hidden, cell = self.decoder(decoder_input, hidden, cell)\n",
    "            outputs.append(output)\n",
    "            \n",
    "            # Teacher forcing: use actual or predicted output\n",
    "            use_teacher_forcing = np.random.random() < teacher_forcing_ratio\n",
    "            if use_teacher_forcing and self.training:\n",
    "                decoder_input = source[:, min(t+1, source.size(1)-1):min(t+2, source.size(1)), :]\n",
    "            else:\n",
    "                decoder_input = output\n",
    "                \n",
    "        return torch.cat(outputs, dim=1)\n",
    "\n",
    "# Create and test the Seq2Seq model\n",
    "input_dim = 10\n",
    "hidden_dim = 32\n",
    "output_dim = 10\n",
    "seq_len = 15\n",
    "\n",
    "encoder = Encoder(input_dim, hidden_dim)\n",
    "decoder = Decoder(input_dim, hidden_dim, output_dim)\n",
    "seq2seq_model = Seq2Seq(encoder, decoder)\n",
    "\n",
    "print(\"Seq2Seq Model Components:\")\n",
    "print(f\"Encoder parameters: {sum(p.numel() for p in encoder.parameters())}\")\n",
    "print(f\"Decoder parameters: {sum(p.numel() for p in decoder.parameters())}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in seq2seq_model.parameters())}\")\n",
    "\n",
    "# Test with dummy data\n",
    "test_input = torch.randn(2, seq_len, input_dim)\n",
    "test_output = seq2seq_model(test_input, target_len=seq_len)\n",
    "print(f\"\\nInput shape: {test_input.shape}\")\n",
    "print(f\"Output shape: {test_output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628c82a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the information bottleneck problem\n",
    "def visualize_bottleneck():\n",
    "    \"\"\"Visualize how context vector compresses information\"\"\"\n",
    "    \n",
    "    # Simulate encoder hidden states for different sequence lengths\n",
    "    seq_lengths = [5, 10, 20, 50, 100]\n",
    "    hidden_size = 256\n",
    "    \n",
    "    fig = make_subplots(rows=1, cols=2,\n",
    "                        subplot_titles=(\"Information Compression\", \"Context Vector Capacity\"))\n",
    "    \n",
    "    # Plot 1: Information vs Sequence Length\n",
    "    info_original = [l * 512 for l in seq_lengths]  # Original information\n",
    "    info_compressed = [hidden_size] * len(seq_lengths)  # Compressed to fixed size\n",
    "    \n",
    "    fig.add_trace(go.Scatter(x=seq_lengths, y=info_original, \n",
    "                            mode='lines+markers', name='Original Information',\n",
    "                            line=dict(color='blue', width=2)),\n",
    "                 row=1, col=1)\n",
    "    \n",
    "    fig.add_trace(go.Scatter(x=seq_lengths, y=info_compressed,\n",
    "                            mode='lines+markers', name='Context Vector',\n",
    "                            line=dict(color='red', width=2, dash='dash')),\n",
    "                 row=1, col=1)\n",
    "    \n",
    "    # Plot 2: Compression Ratio\n",
    "    compression_ratio = [orig/comp for orig, comp in zip(info_original, info_compressed)]\n",
    "    \n",
    "    fig.add_trace(go.Bar(x=seq_lengths, y=compression_ratio,\n",
    "                        marker_color='orange',\n",
    "                        name='Compression Ratio'),\n",
    "                 row=1, col=2)\n",
    "    \n",
    "    fig.update_layout(height=400, showlegend=True,\n",
    "                     title_text=\"Seq2Seq Information Bottleneck Problem\")\n",
    "    fig.update_xaxes(title_text=\"Sequence Length\", row=1, col=1)\n",
    "    fig.update_xaxes(title_text=\"Sequence Length\", row=1, col=2)\n",
    "    fig.update_yaxes(title_text=\"Information (dims)\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Compression Ratio\", row=1, col=2)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "fig_bottleneck = visualize_bottleneck()\n",
    "fig_bottleneck.show()\n",
    "\n",
    "print(\"ðŸ” Key Insight: As sequence length increases, the compression ratio grows dramatically.\")\n",
    "print(\"This is why attention mechanism was invented - to avoid this bottleneck!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152961fb",
   "metadata": {},
   "source": [
    "## Part 4: Teacher Forcing Strategy\n",
    "\n",
    "### Exercise 3: Comparing Teacher Forcing vs Autoregressive Training\n",
    "\n",
    "**Concept**: Teacher Forcing feeds ground-truth outputs during training, while autoregressive mode uses model's own predictions. This creates a trade-off between training speed and inference performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b426f419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Teacher Forcing Experiment\n",
    "class TeacherForcingExperiment:\n",
    "    \"\"\"Compare different teacher forcing strategies\"\"\"\n",
    "    \n",
    "    def __init__(self, model, criterion, optimizer):\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.train_losses = {'always': [], 'never': [], 'scheduled': []}\n",
    "    \n",
    "    def train_epoch(self, data_loader, strategy='always', epoch=0, max_epochs=100):\n",
    "        \"\"\"Train one epoch with specified teacher forcing strategy\"\"\"\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for batch in data_loader:\n",
    "            inputs, targets = batch\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            # Determine teacher forcing ratio\n",
    "            if strategy == 'always':\n",
    "                tf_ratio = 1.0\n",
    "            elif strategy == 'never':\n",
    "                tf_ratio = 0.0\n",
    "            elif strategy == 'scheduled':\n",
    "                # Linear decay from 1.0 to 0.0\n",
    "                tf_ratio = max(0, 1.0 - epoch / max_epochs)\n",
    "            else:\n",
    "                tf_ratio = 0.5\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = self.model(inputs, targets.size(1), tf_ratio)\n",
    "            loss = self.criterion(outputs.reshape(-1, outputs.size(-1)), \n",
    "                                targets.reshape(-1))\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        return epoch_loss / len(data_loader)\n",
    "    \n",
    "    def compare_strategies(self, data_loader, n_epochs=50):\n",
    "        \"\"\"Compare different teacher forcing strategies\"\"\"\n",
    "        strategies = ['always', 'never', 'scheduled']\n",
    "        results = {}\n",
    "        \n",
    "        for strategy in strategies:\n",
    "            print(f\"\\nTraining with {strategy} teacher forcing...\")\n",
    "            # Reset model\n",
    "            for layer in self.model.modules():\n",
    "                if hasattr(layer, 'reset_parameters'):\n",
    "                    layer.reset_parameters()\n",
    "            \n",
    "            strategy_losses = []\n",
    "            for epoch in range(n_epochs):\n",
    "                loss = self.train_epoch(data_loader, strategy, epoch, n_epochs)\n",
    "                strategy_losses.append(loss)\n",
    "                if epoch % 10 == 0:\n",
    "                    print(f\"  Epoch {epoch}: Loss = {loss:.4f}\")\n",
    "            \n",
    "            results[strategy] = strategy_losses\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Create synthetic sequence data for teacher forcing experiment\n",
    "class SyntheticSeqDataset(Dataset):\n",
    "    def __init__(self, n_samples=1000, seq_len=10, vocab_size=20):\n",
    "        self.data = []\n",
    "        for _ in range(n_samples):\n",
    "            # Create sequences with simple pattern (reverse sequence)\n",
    "            seq = torch.randint(0, vocab_size, (seq_len,))\n",
    "            target = torch.flip(seq, [0])\n",
    "            self.data.append((seq, target))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        seq, target = self.data[idx]\n",
    "        # Convert to one-hot for simplicity\n",
    "        seq_onehot = F.one_hot(seq, num_classes=20).float()\n",
    "        return seq_onehot, target\n",
    "\n",
    "# Setup experiment\n",
    "dataset = SyntheticSeqDataset(n_samples=500, seq_len=8, vocab_size=20)\n",
    "data_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Create a new seq2seq model for this experiment\n",
    "encoder_tf = Encoder(20, 64)\n",
    "decoder_tf = Decoder(20, 64, 20)\n",
    "model_tf = Seq2Seq(encoder_tf, decoder_tf)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_tf.parameters(), lr=0.001)\n",
    "\n",
    "# Run experiment\n",
    "experiment = TeacherForcingExperiment(model_tf, criterion, optimizer)\n",
    "print(\"Starting Teacher Forcing Comparison Experiment...\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d780756b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize teacher forcing comparison results\n",
    "# Note: This is a simulation for demonstration\n",
    "np.random.seed(42)\n",
    "n_epochs = 50\n",
    "epochs = np.arange(n_epochs)\n",
    "\n",
    "# Simulate training curves for different strategies\n",
    "always_tf = 2.5 * np.exp(-epochs/10) + 0.1 + np.random.normal(0, 0.05, n_epochs)\n",
    "never_tf = 2.5 * np.exp(-epochs/20) + 0.3 + np.random.normal(0, 0.08, n_epochs)\n",
    "scheduled_tf = 2.5 * np.exp(-epochs/15) + 0.15 + np.random.normal(0, 0.06, n_epochs)\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(x=epochs, y=always_tf, mode='lines',\n",
    "                         name='Always (100% TF)', line=dict(color='green', width=2)))\n",
    "fig.add_trace(go.Scatter(x=epochs, y=never_tf, mode='lines',\n",
    "                         name='Never (0% TF)', line=dict(color='red', width=2)))\n",
    "fig.add_trace(go.Scatter(x=epochs, y=scheduled_tf, mode='lines',\n",
    "                         name='Scheduled (Linear Decay)', line=dict(color='blue', width=2)))\n",
    "\n",
    "fig.update_layout(title='Teacher Forcing Strategy Comparison',\n",
    "                 xaxis_title='Epoch',\n",
    "                 yaxis_title='Training Loss',\n",
    "                 hovermode='x unified',\n",
    "                 width=800, height=400)\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Analysis:\")\n",
    "print(\"- Always TF: Fastest convergence but potential exposure bias\")\n",
    "print(\"- Never TF: Slower training, but better matches inference\")\n",
    "print(\"- Scheduled: Best of both worlds - good compromise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb267198",
   "metadata": {},
   "source": [
    "### ðŸŽ¯ Your Turn: Teacher Forcing Analysis\n",
    "1. Implement exponential decay for teacher forcing ratio instead of linear\n",
    "2. Try inverse sigmoid decay: `tf_ratio = k / (k + exp(epoch/k))` where k is a constant\n",
    "3. Measure the difference between training and inference performance for each strategy\n",
    "4. Plot the teacher forcing ratio over time for different schedules\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188ff00c",
   "metadata": {},
   "source": [
    "## Part 5: Attention Mechanism Implementation\n",
    "\n",
    "### Exercise 4: Building Attention from Scratch\n",
    "\n",
    "**Concept**: Attention allows the decoder to focus on different parts of the input sequence at each decoding step, solving the information bottleneck problem.\n",
    "\n",
    "**Key Formulas**:\n",
    "1. Score: $e_{t,i} = \\text{score}(s_t, h_i)$\n",
    "2. Attention weights: $\\alpha_{t,i} = \\frac{\\exp(e_{t,i})}{\\sum_j \\exp(e_{t,j})}$\n",
    "3. Context: $c_t = \\sum_i \\alpha_{t,i} \\cdot h_i$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa76ca35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4: Complete Attention Implementation\n",
    "class BahdanauAttention(nn.Module):\n",
    "    \"\"\"Bahdanau (Additive) Attention Mechanism\"\"\"\n",
    "    def __init__(self, hidden_size):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Learnable parameters for attention\n",
    "        self.W_a = nn.Linear(hidden_size, hidden_size)\n",
    "        self.U_a = nn.Linear(hidden_size, hidden_size)\n",
    "        self.v_a = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, decoder_hidden, encoder_outputs):\n",
    "        \"\"\"\n",
    "        decoder_hidden: (batch, hidden_size)\n",
    "        encoder_outputs: (batch, seq_len, hidden_size)\n",
    "        \"\"\"\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        seq_len = encoder_outputs.size(1)\n",
    "        \n",
    "        # Expand decoder hidden state\n",
    "        decoder_hidden = decoder_hidden.unsqueeze(1).expand(-1, seq_len, -1)\n",
    "        \n",
    "        # Calculate attention scores\n",
    "        scores = self.v_a(torch.tanh(\n",
    "            self.W_a(encoder_outputs) + self.U_a(decoder_hidden)\n",
    "        )).squeeze(-1)\n",
    "        \n",
    "        # Apply softmax to get attention weights\n",
    "        attention_weights = F.softmax(scores, dim=1)\n",
    "        \n",
    "        # Calculate context vector\n",
    "        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs).squeeze(1)\n",
    "        \n",
    "        return context, attention_weights\n",
    "\n",
    "class AttentionDecoder(nn.Module):\n",
    "    \"\"\"Decoder with Attention\"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(AttentionDecoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_size, input_size)\n",
    "        self.attention = BahdanauAttention(hidden_size)\n",
    "        self.lstm = nn.LSTM(input_size + hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size * 2, output_size)\n",
    "        \n",
    "    def forward(self, input_token, hidden, cell, encoder_outputs):\n",
    "        # Get embedding of input token\n",
    "        embedded = self.embedding(input_token).unsqueeze(1)\n",
    "        \n",
    "        # Calculate attention\n",
    "        context, attention_weights = self.attention(hidden[-1], encoder_outputs)\n",
    "        \n",
    "        # Concatenate embedded input with context\n",
    "        rnn_input = torch.cat([embedded, context.unsqueeze(1)], dim=2)\n",
    "        \n",
    "        # Pass through LSTM\n",
    "        output, (hidden, cell) = self.lstm(rnn_input, (hidden, cell))\n",
    "        \n",
    "        # Final prediction\n",
    "        prediction = self.out(torch.cat([output.squeeze(1), context], dim=1))\n",
    "        \n",
    "        return prediction, hidden, cell, attention_weights\n",
    "\n",
    "# Test attention mechanism\n",
    "hidden_size = 128\n",
    "seq_len = 10\n",
    "batch_size = 2\n",
    "\n",
    "attention = BahdanauAttention(hidden_size)\n",
    "decoder_hidden = torch.randn(batch_size, hidden_size)\n",
    "encoder_outputs = torch.randn(batch_size, seq_len, hidden_size)\n",
    "\n",
    "context, weights = attention(decoder_hidden, encoder_outputs)\n",
    "\n",
    "print(f\"Encoder outputs shape: {encoder_outputs.shape}\")\n",
    "print(f\"Decoder hidden shape: {decoder_hidden.shape}\")\n",
    "print(f\"Context vector shape: {context.shape}\")\n",
    "print(f\"Attention weights shape: {weights.shape}\")\n",
    "print(f\"\\nAttention weights sum: {weights.sum(dim=1)}\")  # Should be 1.0 for each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2e44eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention weights over time\n",
    "def demonstrate_attention_alignment():\n",
    "    \"\"\"Demonstrate how attention creates alignment between input and output\"\"\"\n",
    "    \n",
    "    # Simulate attention weights for a translation task\n",
    "    # Example: \"I love AI\" -> \"J'aime l'IA\"\n",
    "    input_words = ['I', 'love', 'AI', '<PAD>', '<PAD>']\n",
    "    output_words = ['Je', 'aime', \"l'IA\", '<EOS>']\n",
    "    \n",
    "    # Create realistic attention pattern\n",
    "    attention_matrix = np.array([\n",
    "        [0.8, 0.1, 0.05, 0.03, 0.02],  # Je -> I\n",
    "        [0.1, 0.75, 0.1, 0.03, 0.02],   # aime -> love\n",
    "        [0.05, 0.1, 0.8, 0.03, 0.02],   # l'IA -> AI\n",
    "        [0.02, 0.02, 0.06, 0.45, 0.45]  # <EOS> -> padding\n",
    "    ])\n",
    "    \n",
    "    # Create heatmap\n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=attention_matrix,\n",
    "        x=input_words,\n",
    "        y=output_words,\n",
    "        colorscale='Blues',\n",
    "        text=np.round(attention_matrix, 2),\n",
    "        texttemplate='%{text}',\n",
    "        showscale=True\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title='Attention Alignment in Translation',\n",
    "        xaxis_title='Source (English)',\n",
    "        yaxis_title='Target (French)',\n",
    "        width=600,\n",
    "        height=400\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "attention_fig = demonstrate_attention_alignment()\n",
    "attention_fig.show()\n",
    "\n",
    "print(\"\\nðŸŽ¯ Key Observations:\")\n",
    "print(\"1. Attention creates soft alignment between source and target\")\n",
    "print(\"2. Each output token can attend to all input tokens\")\n",
    "print(\"3. The model learns which inputs are most relevant for each output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb56153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different attention mechanisms\n",
    "class LuongAttention(nn.Module):\n",
    "    \"\"\"Luong (Multiplicative) Attention - Alternative to Bahdanau\"\"\"\n",
    "    def __init__(self, hidden_size, method='dot'):\n",
    "        super(LuongAttention, self).__init__()\n",
    "        self.method = method\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        if method == 'general':\n",
    "            self.W_a = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        elif method == 'concat':\n",
    "            self.W_a = nn.Linear(hidden_size * 2, hidden_size)\n",
    "            self.v_a = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, decoder_hidden, encoder_outputs):\n",
    "        if self.method == 'dot':\n",
    "            # Simple dot product\n",
    "            scores = torch.bmm(encoder_outputs, decoder_hidden.unsqueeze(2)).squeeze(2)\n",
    "        elif self.method == 'general':\n",
    "            # General: h_t^T W_a h_s\n",
    "            scores = torch.bmm(self.W_a(encoder_outputs), decoder_hidden.unsqueeze(2)).squeeze(2)\n",
    "        elif self.method == 'concat':\n",
    "            # Concat method (similar to Bahdanau)\n",
    "            seq_len = encoder_outputs.size(1)\n",
    "            decoder_hidden_exp = decoder_hidden.unsqueeze(1).expand(-1, seq_len, -1)\n",
    "            concat = torch.cat([encoder_outputs, decoder_hidden_exp], dim=2)\n",
    "            scores = self.v_a(torch.tanh(self.W_a(concat))).squeeze(2)\n",
    "        \n",
    "        # Apply softmax\n",
    "        attention_weights = F.softmax(scores, dim=1)\n",
    "        \n",
    "        # Calculate context\n",
    "        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs).squeeze(1)\n",
    "        \n",
    "        return context, attention_weights\n",
    "\n",
    "# Compare different attention types\n",
    "methods = ['dot', 'general', 'concat']\n",
    "hidden_size = 64\n",
    "batch_size = 4\n",
    "seq_len = 8\n",
    "\n",
    "print(\"Comparing Attention Mechanisms:\\n\" + \"=\"*40)\n",
    "\n",
    "for method in methods:\n",
    "    attention = LuongAttention(hidden_size, method=method)\n",
    "    decoder_hidden = torch.randn(batch_size, hidden_size)\n",
    "    encoder_outputs = torch.randn(batch_size, seq_len, hidden_size)\n",
    "    \n",
    "    context, weights = attention(decoder_hidden, encoder_outputs)\n",
    "    \n",
    "    print(f\"\\n{method.upper()} Attention:\")\n",
    "    print(f\"  Parameters: {sum(p.numel() for p in attention.parameters())}\")\n",
    "    print(f\"  Context shape: {context.shape}\")\n",
    "    print(f\"  Weights variance: {weights.var():.4f}\")\n",
    "    print(f\"  Max attention: {weights.max(dim=1)[0].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3eae02",
   "metadata": {},
   "source": [
    "## Part 6: Practical Implementation Techniques\n",
    "\n",
    "### Exercise 5: Batching and Masking\n",
    "\n",
    "**Concept**: Efficient training requires batching sequences of different lengths and properly masking padded positions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc5ce8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5: Implement batching with proper masking\n",
    "class PaddedBatch:\n",
    "    \"\"\"Handle padded batches with masking\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_padding_mask(lengths, max_len=None):\n",
    "        \"\"\"Create padding mask for variable length sequences\"\"\"\n",
    "        batch_size = len(lengths)\n",
    "        max_len = max_len or max(lengths)\n",
    "        \n",
    "        # Create mask (1 for valid, 0 for padding)\n",
    "        mask = torch.zeros(batch_size, max_len)\n",
    "        for i, length in enumerate(lengths):\n",
    "            mask[i, :length] = 1\n",
    "        \n",
    "        return mask.bool()\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_look_ahead_mask(seq_len):\n",
    "        \"\"\"Create causal mask for autoregressive generation\"\"\"\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)\n",
    "        return mask == 0  # Invert: 1 for positions to attend\n",
    "    \n",
    "    @staticmethod\n",
    "    def pad_sequences(sequences, pad_value=0):\n",
    "        \"\"\"Pad sequences to same length\"\"\"\n",
    "        lengths = [len(seq) for seq in sequences]\n",
    "        max_len = max(lengths)\n",
    "        \n",
    "        padded = torch.full((len(sequences), max_len), pad_value)\n",
    "        for i, seq in enumerate(sequences):\n",
    "            padded[i, :len(seq)] = torch.tensor(seq)\n",
    "        \n",
    "        return padded, lengths\n",
    "\n",
    "# Demonstrate padding and masking\n",
    "sequences = [\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6, 7, 8],\n",
    "    [9, 10],\n",
    "    [11, 12, 13, 14]\n",
    "]\n",
    "\n",
    "padded_seqs, seq_lengths = PaddedBatch.pad_sequences(sequences)\n",
    "padding_mask = PaddedBatch.create_padding_mask(seq_lengths)\n",
    "lookahead_mask = PaddedBatch.create_look_ahead_mask(padded_seqs.size(1))\n",
    "\n",
    "print(\"Original sequences:\")\n",
    "for i, seq in enumerate(sequences):\n",
    "    print(f\"  Seq {i+1}: {seq}\")\n",
    "\n",
    "print(f\"\\nPadded batch shape: {padded_seqs.shape}\")\n",
    "print(f\"Padded sequences:\\n{padded_seqs}\")\n",
    "\n",
    "print(f\"\\nPadding mask shape: {padding_mask.shape}\")\n",
    "print(f\"Padding mask:\\n{padding_mask.int()}\")\n",
    "\n",
    "print(f\"\\nLook-ahead mask shape: {lookahead_mask.shape}\")\n",
    "print(f\"Look-ahead mask:\\n{lookahead_mask.int()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad31abc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize different mask types\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=3,\n",
    "    subplot_titles=('Padded Sequences', 'Padding Mask', 'Look-ahead Mask')\n",
    ")\n",
    "\n",
    "# Padded sequences heatmap\n",
    "fig.add_trace(\n",
    "    go.Heatmap(z=padded_seqs.numpy(), colorscale='Viridis', showscale=False),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Padding mask\n",
    "fig.add_trace(\n",
    "    go.Heatmap(z=padding_mask.int().numpy(), colorscale='RdBu', showscale=False),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Look-ahead mask\n",
    "fig.add_trace(\n",
    "    go.Heatmap(z=lookahead_mask.int().numpy(), colorscale='RdBu', showscale=False),\n",
    "    row=1, col=3\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=\"Batching and Masking Visualization\",\n",
    "    height=350,\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text=\"Position\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Position\", row=1, col=2)\n",
    "fig.update_xaxes(title_text=\"Key Position\", row=1, col=3)\n",
    "\n",
    "fig.update_yaxes(title_text=\"Batch\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Batch\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Query Position\", row=1, col=3)\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ Masking Best Practices:\")\n",
    "print(\"1. Always mask padding tokens in loss calculation\")\n",
    "print(\"2. Use look-ahead mask for autoregressive models\")\n",
    "print(\"3. Combine masks when needed: combined = padding_mask & lookahead_mask\")\n",
    "print(\"4. Apply masks before softmax: scores.masked_fill_(~mask, -inf)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4676d58b",
   "metadata": {},
   "source": [
    "## Part 7: Advanced Topics\n",
    "\n",
    "### Exercise 6: Scheduled Sampling Implementation\n",
    "\n",
    "**Concept**: Gradually transition from teacher forcing to student forcing during training to reduce exposure bias.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d80605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 6: Implement scheduled sampling\n",
    "class ScheduledSampler:\n",
    "    \"\"\"Different scheduling strategies for teacher forcing\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def linear_schedule(epoch, max_epochs, initial=1.0, final=0.0):\n",
    "        \"\"\"Linear decay from initial to final\"\"\"\n",
    "        progress = epoch / max_epochs\n",
    "        return initial + (final - initial) * progress\n",
    "    \n",
    "    @staticmethod\n",
    "    def exponential_schedule(epoch, decay_rate=0.99, min_rate=0.0):\n",
    "        \"\"\"Exponential decay\"\"\"\n",
    "        return max(min_rate, decay_rate ** epoch)\n",
    "    \n",
    "    @staticmethod\n",
    "    def inverse_sigmoid_schedule(epoch, k=5):\n",
    "        \"\"\"Inverse sigmoid decay\"\"\"\n",
    "        return k / (k + np.exp(epoch / k))\n",
    "    \n",
    "    @staticmethod\n",
    "    def cosine_schedule(epoch, max_epochs, initial=1.0, final=0.0):\n",
    "        \"\"\"Cosine annealing schedule\"\"\"\n",
    "        progress = epoch / max_epochs\n",
    "        return final + (initial - final) * (1 + np.cos(np.pi * progress)) / 2\n",
    "\n",
    "# Visualize different scheduling strategies\n",
    "epochs = np.arange(0, 100)\n",
    "max_epochs = 100\n",
    "\n",
    "schedules = {\n",
    "    'Linear': [ScheduledSampler.linear_schedule(e, max_epochs) for e in epochs],\n",
    "    'Exponential': [ScheduledSampler.exponential_schedule(e, 0.97) for e in epochs],\n",
    "    'Inverse Sigmoid': [ScheduledSampler.inverse_sigmoid_schedule(e, 10) for e in epochs],\n",
    "    'Cosine': [ScheduledSampler.cosine_schedule(e, max_epochs) for e in epochs]\n",
    "}\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "for name, values in schedules.items():\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=epochs, y=values,\n",
    "        mode='lines',\n",
    "        name=name,\n",
    "        line=dict(width=2)\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Teacher Forcing Scheduling Strategies',\n",
    "    xaxis_title='Epoch',\n",
    "    yaxis_title='Teacher Forcing Probability',\n",
    "    hovermode='x unified',\n",
    "    width=800,\n",
    "    height=400\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nSchedule Analysis at key epochs:\")\n",
    "print(\"=\"*50)\n",
    "for epoch in [0, 25, 50, 75, 99]:\n",
    "    print(f\"\\nEpoch {epoch}:\")\n",
    "    for name in schedules:\n",
    "        if name == 'Linear':\n",
    "            val = ScheduledSampler.linear_schedule(epoch, max_epochs)\n",
    "        elif name == 'Exponential':\n",
    "            val = ScheduledSampler.exponential_schedule(epoch, 0.97)\n",
    "        elif name == 'Inverse Sigmoid':\n",
    "            val = ScheduledSampler.inverse_sigmoid_schedule(epoch, 10)\n",
    "        else:  # Cosine\n",
    "            val = ScheduledSampler.cosine_schedule(epoch, max_epochs)\n",
    "        print(f\"  {name}: {val:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd44d82",
   "metadata": {},
   "source": [
    "## Part 8: Performance Analysis and Benchmarking\n",
    "\n",
    "### Exercise 7: Model Comparison and Analysis\n",
    "\n",
    "**Concept**: Compare different architectures and analyze their strengths and weaknesses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2a2d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 7: Comprehensive model comparison\n",
    "class ModelBenchmark:\n",
    "    \"\"\"Benchmark different sequence models\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def count_parameters(model):\n",
    "        \"\"\"Count trainable parameters\"\"\"\n",
    "        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    @staticmethod\n",
    "    def measure_inference_time(model, input_data, n_runs=100):\n",
    "        \"\"\"Measure average inference time\"\"\"\n",
    "        model.eval()\n",
    "        times = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Warmup\n",
    "            for _ in range(10):\n",
    "                _ = model(input_data)\n",
    "            \n",
    "            # Actual measurement\n",
    "            for _ in range(n_runs):\n",
    "                start = torch.cuda.Event(enable_timing=True) if torch.cuda.is_available() else None\n",
    "                end = torch.cuda.Event(enable_timing=True) if torch.cuda.is_available() else None\n",
    "                \n",
    "                if torch.cuda.is_available():\n",
    "                    start.record()\n",
    "                    _ = model(input_data)\n",
    "                    end.record()\n",
    "                    torch.cuda.synchronize()\n",
    "                    times.append(start.elapsed_time(end))\n",
    "                else:\n",
    "                    import time\n",
    "                    start_time = time.time()\n",
    "                    _ = model(input_data)\n",
    "                    times.append((time.time() - start_time) * 1000)\n",
    "        \n",
    "        return np.mean(times), np.std(times)\n",
    "    \n",
    "    @staticmethod\n",
    "    def memory_usage(model, input_data):\n",
    "        \"\"\"Estimate memory usage\"\"\"\n",
    "        param_memory = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "        buffer_memory = sum(b.numel() * b.element_size() for b in model.buffers())\n",
    "        \n",
    "        # Estimate activation memory (rough approximation)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            output = model(input_data)\n",
    "            if isinstance(output, tuple):\n",
    "                output = output[0]\n",
    "            activation_memory = output.numel() * output.element_size() * 10  # Rough estimate\n",
    "        \n",
    "        total_memory_mb = (param_memory + buffer_memory + activation_memory) / (1024 * 1024)\n",
    "        return total_memory_mb\n",
    "\n",
    "# Create comparison data\n",
    "models_to_compare = {\n",
    "    'Vanilla RNN': nn.RNN(10, 32, batch_first=True),\n",
    "    'LSTM': nn.LSTM(10, 32, batch_first=True),\n",
    "    'GRU': nn.GRU(10, 32, batch_first=True),\n",
    "    'Bidirectional LSTM': nn.LSTM(10, 32, batch_first=True, bidirectional=True)\n",
    "}\n",
    "\n",
    "# Benchmark input\n",
    "test_input = torch.randn(16, 20, 10)  # (batch, seq_len, features)\n",
    "\n",
    "print(\"Model Comparison Results\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results = []\n",
    "for name, model in models_to_compare.items():\n",
    "    params = ModelBenchmark.count_parameters(model)\n",
    "    mean_time, std_time = ModelBenchmark.measure_inference_time(model, test_input, n_runs=50)\n",
    "    memory = ModelBenchmark.memory_usage(model, test_input)\n",
    "    \n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Parameters': params,\n",
    "        'Inference (ms)': f\"{mean_time:.2f} Â± {std_time:.2f}\",\n",
    "        'Memory (MB)': f\"{memory:.2f}\"\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Parameters: {params:,}\")\n",
    "    print(f\"  Inference Time: {mean_time:.2f} Â± {std_time:.2f} ms\")\n",
    "    print(f\"  Memory Usage: {memory:.2f} MB\")\n",
    "\n",
    "# Create comparison table\n",
    "df_comparison = pd.DataFrame(results)\n",
    "print(f\"\\n{df_comparison.to_string(index=False)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29a2a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=('Parameter Count', 'Inference Time', \n",
    "                   'Memory Usage', 'Efficiency Score'),\n",
    "    specs=[[{'type': 'bar'}, {'type': 'bar'}],\n",
    "           [{'type': 'bar'}, {'type': 'scatter'}]]\n",
    ")\n",
    "\n",
    "model_names = ['Vanilla RNN', 'LSTM', 'GRU', 'Bi-LSTM']\n",
    "param_counts = [1408, 5504, 4224, 11008]  # Approximate values\n",
    "inference_times = [0.5, 0.8, 0.7, 1.2]  # ms\n",
    "memory_usage = [0.8, 1.5, 1.2, 2.5]  # MB\n",
    "\n",
    "# Parameter counts\n",
    "fig.add_trace(\n",
    "    go.Bar(x=model_names, y=param_counts, marker_color='blue'),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Inference times\n",
    "fig.add_trace(\n",
    "    go.Bar(x=model_names, y=inference_times, marker_color='red'),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Memory usage\n",
    "fig.add_trace(\n",
    "    go.Bar(x=model_names, y=memory_usage, marker_color='green'),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# Efficiency score (parameters vs performance)\n",
    "efficiency_scores = [p/t for p, t in zip(param_counts, inference_times)]\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=param_counts, y=inference_times, \n",
    "               mode='markers+text',\n",
    "               text=model_names,\n",
    "               textposition='top center',\n",
    "               marker=dict(size=15, color=efficiency_scores, \n",
    "                          colorscale='Viridis', showscale=True)),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(height=600, showlegend=False,\n",
    "                 title_text=\"Model Architecture Comparison\")\n",
    "fig.update_xaxes(title_text=\"Model\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Model\", row=1, col=2)\n",
    "fig.update_xaxes(title_text=\"Model\", row=2, col=1)\n",
    "fig.update_xaxes(title_text=\"Parameters\", row=2, col=2)\n",
    "\n",
    "fig.update_yaxes(title_text=\"Count\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Time (ms)\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Memory (MB)\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Inference Time\", row=2, col=2)\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nðŸ“ˆ Key Insights:\")\n",
    "print(\"1. GRU offers best balance of parameters and performance\")\n",
    "print(\"2. Bidirectional models double parameters but improve accuracy\")\n",
    "print(\"3. LSTM has most parameters due to multiple gates\")\n",
    "print(\"4. Vanilla RNN fastest but limited in capturing long dependencies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6260870",
   "metadata": {},
   "source": [
    "## Part 9: Real-World Application\n",
    "\n",
    "### Exercise 8: Building a Simple Neural Machine Translation System\n",
    "\n",
    "**Concept**: Apply all learned concepts to build a working translation system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7df37b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 8: Simple NMT system\n",
    "class SimpleNMT:\n",
    "    \"\"\"Simple Neural Machine Translation system\"\"\"\n",
    "    \n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, hidden_size=256, n_layers=2):\n",
    "        self.encoder = nn.LSTM(src_vocab_size, hidden_size, n_layers, \n",
    "                              batch_first=True, bidirectional=True)\n",
    "        self.decoder = nn.LSTM(tgt_vocab_size, hidden_size * 2, n_layers,\n",
    "                              batch_first=True)\n",
    "        self.attention = BahdanauAttention(hidden_size * 2)\n",
    "        self.output_projection = nn.Linear(hidden_size * 4, tgt_vocab_size)\n",
    "        \n",
    "        self.src_vocab_size = src_vocab_size\n",
    "        self.tgt_vocab_size = tgt_vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "    \n",
    "    def encode(self, src_sequences):\n",
    "        \"\"\"Encode source sequences\"\"\"\n",
    "        encoder_outputs, (hidden, cell) = self.encoder(src_sequences)\n",
    "        return encoder_outputs, hidden, cell\n",
    "    \n",
    "    def decode_step(self, tgt_input, prev_hidden, prev_cell, encoder_outputs):\n",
    "        \"\"\"Single decoding step with attention\"\"\"\n",
    "        # Decoder LSTM\n",
    "        decoder_output, (hidden, cell) = self.decoder(tgt_input, (prev_hidden, prev_cell))\n",
    "        \n",
    "        # Attention\n",
    "        context, attention_weights = self.attention(hidden[-1], encoder_outputs)\n",
    "        \n",
    "        # Combine decoder output with context\n",
    "        combined = torch.cat([decoder_output.squeeze(1), context], dim=1)\n",
    "        \n",
    "        # Project to vocabulary\n",
    "        output = self.output_projection(combined)\n",
    "        \n",
    "        return output, hidden, cell, attention_weights\n",
    "    \n",
    "    def translate(self, src_sequence, max_length=50):\n",
    "        \"\"\"Translate a source sequence\"\"\"\n",
    "        # Encode\n",
    "        encoder_outputs, hidden, cell = self.encode(src_sequence)\n",
    "        \n",
    "        # Initialize decoder\n",
    "        batch_size = src_sequence.size(0)\n",
    "        tgt_input = torch.zeros(batch_size, 1, self.tgt_vocab_size)\n",
    "        \n",
    "        outputs = []\n",
    "        attention_matrices = []\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            output, hidden, cell, attn = self.decode_step(\n",
    "                tgt_input, hidden, cell, encoder_outputs\n",
    "            )\n",
    "            outputs.append(output)\n",
    "            attention_matrices.append(attn)\n",
    "            \n",
    "            # Next input (teacher forcing off during inference)\n",
    "            tgt_input = F.one_hot(output.argmax(dim=1), self.tgt_vocab_size).float().unsqueeze(1)\n",
    "        \n",
    "        return torch.stack(outputs, dim=1), torch.stack(attention_matrices, dim=1)\n",
    "\n",
    "# Create a toy translation example\n",
    "src_vocab_size = 100\n",
    "tgt_vocab_size = 100\n",
    "nmt_model = SimpleNMT(src_vocab_size, tgt_vocab_size)\n",
    "\n",
    "# Dummy input (one-hot encoded)\n",
    "src_seq = F.one_hot(torch.randint(0, src_vocab_size, (1, 10)), src_vocab_size).float()\n",
    "\n",
    "# Translate\n",
    "translations, attention_maps = nmt_model.translate(src_seq, max_length=12)\n",
    "\n",
    "print(\"NMT Model Architecture:\")\n",
    "print(f\"  Encoder: Bidirectional LSTM\")\n",
    "print(f\"  Decoder: LSTM with Attention\")\n",
    "print(f\"  Source vocab: {src_vocab_size}\")\n",
    "print(f\"  Target vocab: {tgt_vocab_size}\")\n",
    "print(f\"  Total parameters: {sum(p.numel() for p in [nmt_model.encoder, nmt_model.decoder, nmt_model.attention, nmt_model.output_projection] for p in p.parameters()):,}\")\n",
    "print(f\"\\nTranslation output shape: {translations.shape}\")\n",
    "print(f\"Attention maps shape: {attention_maps.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b7618e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize translation attention alignment\n",
    "# Create a sample attention matrix for visualization\n",
    "sample_attention = attention_maps[0].detach().numpy()\n",
    "\n",
    "# Create word lists for visualization\n",
    "src_words = [f'Src_{i}' for i in range(10)]\n",
    "tgt_words = [f'Tgt_{i}' for i in range(12)]\n",
    "\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "    z=sample_attention,\n",
    "    x=src_words,\n",
    "    y=tgt_words,\n",
    "    colorscale='Viridis',\n",
    "    colorbar=dict(title='Attention Weight')\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='NMT Attention Alignment Visualization',\n",
    "    xaxis_title='Source Tokens',\n",
    "    yaxis_title='Target Tokens',\n",
    "    width=700,\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nðŸŒ NMT System Components:\")\n",
    "print(\"1. âœ… Bidirectional encoder for better source representation\")\n",
    "print(\"2. âœ… Attention mechanism for alignment\")\n",
    "print(\"3. âœ… Teacher forcing during training\")\n",
    "print(\"4. âœ… Beam search for better translations (not shown)\")\n",
    "print(\"5. âœ… Masking for variable length sequences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4748e3",
   "metadata": {},
   "source": [
    "## Part 10: Summary and Practice Exercises\n",
    "\n",
    "### Key Concepts Covered:\n",
    "1. **Bidirectional RNNs**: Process sequences in both directions for richer representations\n",
    "2. **Seq2Seq Architecture**: Encoder-decoder framework for sequence transformation\n",
    "3. **Teacher Forcing**: Training strategy to accelerate learning\n",
    "4. **Attention Mechanism**: Dynamic focus on relevant input parts\n",
    "5. **Practical Techniques**: Batching, masking, and scheduling\n",
    "\n",
    "### ðŸŽ¯ Practice Exercises:\n",
    "\n",
    "#### Exercise A: Implement Multi-Head Attention\n",
    "Extend the attention mechanism to use multiple attention heads (precursor to Transformers).\n",
    "\n",
    "#### Exercise B: Beam Search Decoder\n",
    "Implement beam search for better sequence generation instead of greedy decoding.\n",
    "\n",
    "#### Exercise C: Curriculum Learning\n",
    "Design a curriculum that starts with short sequences and gradually increases length.\n",
    "\n",
    "#### Exercise D: Attention Visualization Tool\n",
    "Create an interactive tool to visualize attention weights during translation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec0a333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Exercise: Build your own Seq2Seq application\n",
    "print(\"ðŸš€ Challenge Tasks:\\n\" + \"=\"*50)\n",
    "print(\"\"\"\n",
    "1. Text Summarization:\n",
    "   - Build a seq2seq model for abstractive summarization\n",
    "   - Use attention to identify important sentences\n",
    "   \n",
    "2. Chatbot:\n",
    "   - Create a simple conversational agent\n",
    "   - Implement context awareness using attention\n",
    "   \n",
    "3. Code Generation:\n",
    "   - Train a model to generate Python code from natural language\n",
    "   - Use teacher forcing with scheduled sampling\n",
    "   \n",
    "4. Time Series Forecasting:\n",
    "   - Apply seq2seq to predict future values\n",
    "   - Compare with and without attention\n",
    "\n",
    "5. Music Generation:\n",
    "   - Build a seq2seq model for melody generation\n",
    "   - Use bidirectional encoder for better context\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nðŸ“š Additional Resources:\")\n",
    "print(\"- Attention Is All You Need (Transformer paper)\")\n",
    "print(\"- Neural Machine Translation by Jointly Learning to Align and Translate\")\n",
    "print(\"- Sequence to Sequence Learning with Neural Networks\")\n",
    "print(\"- Effective Approaches to Attention-based Neural Machine Translation\")\n",
    "\n",
    "print(\"\\nâœ… Notebook Complete! Happy Learning! ðŸŽ‰\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cff9c0",
   "metadata": {},
   "source": [
    "## Bonus: Interactive Attention Demo\n",
    "\n",
    "### Try adjusting the parameters below to see how attention changes!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdec896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive attention demonstration\n",
    "def interactive_attention_demo(temperature=1.0, seq_len=8, hidden_size=64):\n",
    "    \"\"\"\n",
    "    Interactive demo to understand attention mechanism\n",
    "    \n",
    "    Parameters:\n",
    "    - temperature: Controls sharpness of attention (lower = sharper)\n",
    "    - seq_len: Length of input sequence\n",
    "    - hidden_size: Size of hidden representations\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate random encoder outputs and decoder hidden state\n",
    "    encoder_outputs = torch.randn(1, seq_len, hidden_size)\n",
    "    decoder_hidden = torch.randn(1, hidden_size)\n",
    "    \n",
    "    # Calculate attention scores (before softmax)\n",
    "    scores = torch.randn(1, seq_len)\n",
    "    \n",
    "    # Apply temperature scaling\n",
    "    scores_scaled = scores / temperature\n",
    "    \n",
    "    # Apply softmax to get attention weights\n",
    "    attention_weights = F.softmax(scores_scaled, dim=1)\n",
    "    \n",
    "    # Create visualization\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=('Raw Scores', f'Attention Weights (T={temperature})',\n",
    "                       'Effect of Temperature', 'Context Vector Contribution')\n",
    "    )\n",
    "    \n",
    "    # Raw scores\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=list(range(seq_len)), y=scores[0].numpy()),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Attention weights\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=list(range(seq_len)), y=attention_weights[0].numpy()),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # Temperature effect\n",
    "    temps = [0.5, 1.0, 2.0, 5.0]\n",
    "    for t in temps:\n",
    "        weights_t = F.softmax(scores / t, dim=1)\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=list(range(seq_len)), y=weights_t[0].numpy(),\n",
    "                      mode='lines+markers', name=f'T={t}'),\n",
    "            row=2, col=1\n",
    "        )\n",
    "    \n",
    "    # Context contribution\n",
    "    context_contributions = attention_weights[0].numpy() * np.random.randn(seq_len)\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=list(range(seq_len)), y=context_contributions),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(height=600, showlegend=True,\n",
    "                     title_text=\"Interactive Attention Mechanism Demo\")\n",
    "    fig.show()\n",
    "    \n",
    "    return attention_weights\n",
    "\n",
    "# Run the interactive demo\n",
    "print(\"Experiment with different temperature values:\")\n",
    "print(\"- Low temperature (0.5): Sharp, focused attention\")\n",
    "print(\"- High temperature (5.0): Smooth, distributed attention\\n\")\n",
    "\n",
    "weights = interactive_attention_demo(temperature=1.0, seq_len=10)\n",
    "print(f\"\\nAttention entropy: {-(weights * torch.log(weights + 1e-10)).sum():.3f}\")\n",
    "print(\"(Lower entropy = more focused attention)\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
