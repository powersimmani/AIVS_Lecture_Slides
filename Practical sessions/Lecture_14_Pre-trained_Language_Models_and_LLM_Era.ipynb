{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ¤– Pre-trained Language Models & LLM Era: Hands-on Practice\n",
        "## Interactive Learning Notebook for NLP and Language Models\n\n",
        "**Author**: Based on Lecture 14 by Ho-min Park\n",
        "**Date**: 2024\n",
        "**Duration**: 3-4 hours\n\n",
        "### ðŸ“š Learning Objectives\n",
        "- Understand the paradigm shift from traditional NLP to pre-trained models\n",
        "- Implement tokenization and understand language modeling objectives\n",
        "- Practice with BERT and GPT-style models\n",
        "- Master fine-tuning strategies and prompt engineering\n",
        "- Explore RLHF and instruction tuning concepts\n\n",
        "### ðŸ› ï¸ Technical Requirements\n",
        "- Python 3.8+\n",
        "- transformers, torch, numpy, pandas, matplotlib, seaborn, plotly\n",
        "- GPU recommended but not required\n\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Environment Setup and Imports\n",
        "Let's start by setting up our environment with all necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages (run once)\n",
        "!pip install -q transformers torch numpy pandas matplotlib seaborn plotly scikit-learn datasets accelerate sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import essential libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Transformers and NLP libraries\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModel, AutoModelForSequenceClassification,\n",
        "    AutoModelForCausalLM, AutoModelForMaskedLM,\n",
        "    BertTokenizer, BertModel, BertForMaskedLM,\n",
        "    GPT2Tokenizer, GPT2Model, GPT2LMHeadModel,\n",
        "    T5Tokenizer, T5ForConditionalGeneration,\n",
        "    pipeline, Trainer, TrainingArguments\n",
        ")\n",
        "from datasets import load_dataset, Dataset\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "import json\n",
        "import random\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Check if CUDA is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "print(f'PyTorch version: {torch.__version__}')\n",
        "print(f'Transformers version: {transformers.__version__}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 2: Understanding the Paradigm Shift\n",
        "\n",
        "### Exercise 1: From Traditional NLP to Pre-trained Models\n",
        "\n",
        "#### Concept\n",
        "The shift from task-specific models to pre-trained foundation models represents a fundamental change in NLP:\n",
        "- **Old Paradigm**: Task-specific models trained from scratch, requiring labeled data for each task\n",
        "- **New Paradigm**: General-purpose foundation models pre-trained on massive text, then adapted\n",
        "\n",
        "Let's visualize this paradigm shift and understand tokenization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 1: Visualizing the Paradigm Shift\n",
        "\n",
        "def visualize_paradigm_shift():\n",
        "    \"\"\"Create an interactive visualization of the NLP paradigm shift\"\"\"\n",
        "    \n",
        "    # Data for visualization\n",
        "    paradigms = ['Traditional NLP\\n(Pre-2018)', 'Pre-trained Models\\n(2018-2020)', 'LLM Era\\n(2020+)']\n",
        "    \n",
        "    # Metrics for comparison\n",
        "    data_requirements = [100, 10, 1]  # Relative scale\n",
        "    model_sizes = [0.1, 1, 100]  # Relative scale (millions of parameters)\n",
        "    capabilities = [20, 60, 95]  # Performance percentage\n",
        "    \n",
        "    # Create subplots\n",
        "    fig = make_subplots(\n",
        "        rows=1, cols=3,\n",
        "        subplot_titles=['Data Requirements', 'Model Size', 'Capabilities'],\n",
        "        specs=[[{'type': 'bar'}, {'type': 'bar'}, {'type': 'bar'}]]\n",
        "    )\n",
        "    \n",
        "    # Add traces\n",
        "    fig.add_trace(\n",
        "        go.Bar(x=paradigms, y=data_requirements, name='Data Needed',\n",
        "               marker_color=['#FF6B6B', '#4ECDC4', '#45B7D1']),\n",
        "        row=1, col=1\n",
        "    )\n",
        "    \n",
        "    fig.add_trace(\n",
        "        go.Bar(x=paradigms, y=model_sizes, name='Model Size',\n",
        "               marker_color=['#FF6B6B', '#4ECDC4', '#45B7D1']),\n",
        "        row=1, col=2\n",
        "    )\n",
        "    \n",
        "    fig.add_trace(\n",
        "        go.Bar(x=paradigms, y=capabilities, name='Performance',\n",
        "               marker_color=['#FF6B6B', '#4ECDC4', '#45B7D1']),\n",
        "        row=1, col=3\n",
        "    )\n",
        "    \n",
        "    # Update layout\n",
        "    fig.update_layout(\n",
        "        title_text='Evolution of NLP: Paradigm Shift',\n",
        "        showlegend=False,\n",
        "        height=400\n",
        "    )\n",
        "    \n",
        "    fig.show()\n",
        "    \n",
        "    # Print insights\n",
        "    print(\"\\nðŸ“Š Key Insights:\")\n",
        "    print(\"1. Data Requirements: Decreased by 100x with pre-training\")\n",
        "    print(\"2. Model Size: Increased by 1000x for better performance\")\n",
        "    print(\"3. Capabilities: Near-human performance on many tasks\")\n",
        "\n",
        "visualize_paradigm_shift()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 2: Understanding Tokenization\n",
        "\n",
        "#### Concept\n",
        "Tokenization is the foundation of language models. Different models use different tokenization strategies:\n",
        "- **WordPiece** (BERT): Subword tokenization with ## prefix\n",
        "- **BPE** (GPT): Byte Pair Encoding\n",
        "- **SentencePiece** (T5): Unigram-based tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 2: Comparative Tokenization Analysis\n",
        "\n",
        "def compare_tokenizers(text: str):\n",
        "    \"\"\"Compare different tokenization strategies\"\"\"\n",
        "    \n",
        "    # Load tokenizers\n",
        "    bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "    t5_tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
        "    \n",
        "    # Tokenize text\n",
        "    bert_tokens = bert_tokenizer.tokenize(text.lower())\n",
        "    gpt2_tokens = gpt2_tokenizer.tokenize(text)\n",
        "    t5_tokens = t5_tokenizer.tokenize(text)\n",
        "    \n",
        "    # Create comparison dataframe\n",
        "    comparison_data = {\n",
        "        'Tokenizer': ['BERT (WordPiece)', 'GPT-2 (BPE)', 'T5 (SentencePiece)'],\n",
        "        'Tokens': [bert_tokens, gpt2_tokens, t5_tokens],\n",
        "        'Token Count': [len(bert_tokens), len(gpt2_tokens), len(t5_tokens)],\n",
        "        'Vocabulary Size': [\n",
        "            bert_tokenizer.vocab_size,\n",
        "            len(gpt2_tokenizer.get_vocab()),\n",
        "            t5_tokenizer.vocab_size\n",
        "        ]\n",
        "    }\n",
        "    \n",
        "    df = pd.DataFrame(comparison_data)\n",
        "    \n",
        "    # Display results\n",
        "    print(f\"\\nðŸ“ Original Text: '{text}'\")\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    \n",
        "    for idx, row in df.iterrows():\n",
        "        print(f\"\\n{row['Tokenizer']}:\")\n",
        "        print(f\"  Tokens: {row['Tokens']}\")\n",
        "        print(f\"  Count: {row['Token Count']}\")\n",
        "        print(f\"  Vocab Size: {row['Vocabulary Size']:,}\")\n",
        "    \n",
        "    # Visualize token counts\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "    \n",
        "    # Token count comparison\n",
        "    ax1.bar(df['Tokenizer'], df['Token Count'], color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
        "    ax1.set_title('Token Count Comparison')\n",
        "    ax1.set_ylabel('Number of Tokens')\n",
        "    ax1.set_xticklabels(df['Tokenizer'], rotation=45, ha='right')\n",
        "    \n",
        "    # Vocabulary size comparison\n",
        "    ax2.bar(df['Tokenizer'], df['Vocabulary Size'], color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
        "    ax2.set_title('Vocabulary Size Comparison')\n",
        "    ax2.set_ylabel('Vocabulary Size')\n",
        "    ax2.set_xticklabels(df['Tokenizer'], rotation=45, ha='right')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Test with different texts\n",
        "test_texts = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"Pre-trained language models revolutionized NLP!\",\n",
        "    \"ChatGPT uses reinforcement learning from human feedback (RLHF).\"\n",
        "]\n",
        "\n",
        "for text in test_texts:\n",
        "    df = compare_tokenizers(text)\n",
        "    print(\"\\n\" + \"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 3: BERT - Bidirectional Encoder Models\n",
        "\n",
        "### Exercise 3: Masked Language Modeling (MLM)\n",
        "\n",
        "#### Concept\n",
        "BERT uses Masked Language Modeling where:\n",
        "- 15% of tokens are masked\n",
        "- Model predicts masked tokens using bidirectional context\n",
        "- Enables deep bidirectional understanding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 3: Implement and Visualize Masked Language Modeling\n",
        "\n",
        "class MaskedLanguageModeling:\n",
        "    def __init__(self, model_name='bert-base-uncased'):\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "        self.model = BertForMaskedLM.from_pretrained(model_name)\n",
        "        self.model.eval()\n",
        "    \n",
        "    def mask_and_predict(self, text: str, mask_token_index: int = None):\n",
        "        \"\"\"Mask a token and predict it using BERT\"\"\"\n",
        "        \n",
        "        # Tokenize\n",
        "        tokens = self.tokenizer.tokenize(text)\n",
        "        \n",
        "        # Choose random token to mask if not specified\n",
        "        if mask_token_index is None:\n",
        "            mask_token_index = random.randint(1, len(tokens) - 1)\n",
        "        \n",
        "        # Store original token\n",
        "        original_token = tokens[mask_token_index]\n",
        "        \n",
        "        # Create masked version\n",
        "        masked_tokens = tokens.copy()\n",
        "        masked_tokens[mask_token_index] = '[MASK]'\n",
        "        \n",
        "        # Convert to IDs\n",
        "        indexed_tokens = self.tokenizer.convert_tokens_to_ids(['[CLS]'] + masked_tokens + ['[SEP]'])\n",
        "        tokens_tensor = torch.tensor([indexed_tokens])\n",
        "        \n",
        "        # Predict\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(tokens_tensor)\n",
        "            predictions = outputs.logits\n",
        "        \n",
        "        # Get top 5 predictions for masked token\n",
        "        masked_index = mask_token_index + 1  # +1 for [CLS]\n",
        "        predicted_scores = predictions[0, masked_index]\n",
        "        top_5_idx = torch.argsort(predicted_scores, descending=True)[:5]\n",
        "        \n",
        "        # Convert predictions to tokens\n",
        "        top_5_tokens = [self.tokenizer.convert_ids_to_tokens([idx.item()])[0] for idx in top_5_idx]\n",
        "        top_5_scores = [predicted_scores[idx].item() for idx in top_5_idx]\n",
        "        \n",
        "        # Softmax for probabilities\n",
        "        probs = F.softmax(predicted_scores, dim=-1)\n",
        "        top_5_probs = [probs[idx].item() for idx in top_5_idx]\n",
        "        \n",
        "        return {\n",
        "            'original': original_token,\n",
        "            'masked_text': ' '.join(masked_tokens),\n",
        "            'predictions': list(zip(top_5_tokens, top_5_probs, top_5_scores))\n",
        "        }\n",
        "    \n",
        "    def visualize_predictions(self, results: dict):\n",
        "        \"\"\"Visualize MLM predictions\"\"\"\n",
        "        \n",
        "        tokens = [pred[0] for pred in results['predictions']]\n",
        "        probs = [pred[1] for pred in results['predictions']]\n",
        "        \n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "        \n",
        "        # Bar chart of predictions\n",
        "        colors = ['#2ECC71' if token == results['original'] else '#3498DB' for token in tokens]\n",
        "        bars = ax1.barh(tokens, probs, color=colors)\n",
        "        ax1.set_xlabel('Probability')\n",
        "        ax1.set_title('Top 5 Predictions for [MASK]')\n",
        "        ax1.set_xlim(0, max(probs) * 1.1)\n",
        "        \n",
        "        # Add value labels\n",
        "        for bar, prob in zip(bars, probs):\n",
        "            ax1.text(prob + 0.01, bar.get_y() + bar.get_height()/2, \n",
        "                    f'{prob:.2%}', va='center')\n",
        "        \n",
        "        # Attention visualization (simplified)\n",
        "        sentence = results['masked_text'].split()\n",
        "        attention_matrix = np.random.rand(len(sentence), len(sentence))\n",
        "        np.fill_diagonal(attention_matrix, 1)\n",
        "        \n",
        "        im = ax2.imshow(attention_matrix, cmap='Blues', aspect='auto')\n",
        "        ax2.set_xticks(range(len(sentence)))\n",
        "        ax2.set_yticks(range(len(sentence)))\n",
        "        ax2.set_xticklabels(sentence, rotation=45, ha='right')\n",
        "        ax2.set_yticklabels(sentence)\n",
        "        ax2.set_title('Bidirectional Attention Pattern')\n",
        "        plt.colorbar(im, ax=ax2)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        print(f\"\\nâœ… Original token: '{results['original']}'\")\n",
        "        print(f\"ðŸ“Š Top prediction: '{tokens[0]}' with {probs[0]:.2%} confidence\")\n",
        "\n",
        "# Initialize and test MLM\n",
        "mlm = MaskedLanguageModeling()\n",
        "\n",
        "# Test sentences\n",
        "test_sentences = [\n",
        "    \"The cat sat on the mat.\",\n",
        "    \"Machine learning models require large amounts of data.\",\n",
        "    \"BERT uses bidirectional attention to understand context.\"\n",
        "]\n",
        "\n",
        "for sentence in test_sentences:\n",
        "    print(f\"\\nðŸ” Testing: {sentence}\")\n",
        "    results = mlm.mask_and_predict(sentence)\n",
        "    mlm.visualize_predictions(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ðŸ’¡ Your Turn: MLM Practice\n",
        "\n",
        "Modify the code above to:\n",
        "1. Mask multiple tokens in a sentence\n",
        "2. Compare predictions when masking different positions\n",
        "3. Analyze how context affects predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 4: GPT - Decoder-based Models\n",
        "\n",
        "### Exercise 4: Autoregressive Text Generation\n",
        "\n",
        "#### Concept\n",
        "GPT models use causal (left-to-right) attention:\n",
        "- Predict next token based on previous context\n",
        "- Optimized for text generation\n",
        "- Can be used for few-shot learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 4: GPT Text Generation and Analysis\n",
        "\n",
        "class GPTTextGeneration:\n",
        "    def __init__(self, model_name='gpt2'):\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "        self.model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "        self.model.eval()\n",
        "    \n",
        "    def generate_text(self, prompt: str, max_length: int = 50, \n",
        "                     temperature: float = 1.0, top_k: int = 50):\n",
        "        \"\"\"Generate text using GPT-2\"\"\"\n",
        "        \n",
        "        # Encode prompt\n",
        "        input_ids = self.tokenizer.encode(prompt, return_tensors='pt')\n",
        "        \n",
        "        # Generate\n",
        "        with torch.no_grad():\n",
        "            output = self.model.generate(\n",
        "                input_ids,\n",
        "                max_length=max_length,\n",
        "                temperature=temperature,\n",
        "                top_k=top_k,\n",
        "                do_sample=True,\n",
        "                pad_token_id=self.tokenizer.eos_token_id\n",
        "            )\n",
        "        \n",
        "        # Decode\n",
        "        generated_text = self.tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "        \n",
        "        return generated_text\n",
        "    \n",
        "    def analyze_generation_strategies(self, prompt: str):\n",
        "        \"\"\"Compare different generation strategies\"\"\"\n",
        "        \n",
        "        strategies = [\n",
        "            {'name': 'Greedy', 'temp': 1.0, 'top_k': 1},\n",
        "            {'name': 'Low Temperature', 'temp': 0.5, 'top_k': 50},\n",
        "            {'name': 'High Temperature', 'temp': 1.5, 'top_k': 50},\n",
        "            {'name': 'Top-K Sampling', 'temp': 1.0, 'top_k': 10}\n",
        "        ]\n",
        "        \n",
        "        results = []\n",
        "        \n",
        "        for strategy in strategies:\n",
        "            generated = self.generate_text(\n",
        "                prompt, \n",
        "                temperature=strategy['temp'],\n",
        "                top_k=strategy['top_k']\n",
        "            )\n",
        "            results.append({\n",
        "                'Strategy': strategy['name'],\n",
        "                'Temperature': strategy['temp'],\n",
        "                'Top-K': strategy['top_k'],\n",
        "                'Generated': generated[len(prompt):].strip()[:100]  # First 100 chars after prompt\n",
        "            })\n",
        "        \n",
        "        df = pd.DataFrame(results)\n",
        "        return df\n",
        "    \n",
        "    def visualize_token_probabilities(self, text: str):\n",
        "        \"\"\"Visualize next token probabilities\"\"\"\n",
        "        \n",
        "        input_ids = self.tokenizer.encode(text, return_tensors='pt')\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(input_ids)\n",
        "            predictions = outputs.logits\n",
        "        \n",
        "        # Get probabilities for next token\n",
        "        next_token_logits = predictions[0, -1, :]\n",
        "        probs = F.softmax(next_token_logits, dim=-1)\n",
        "        \n",
        "        # Get top 10 tokens\n",
        "        top_k = 10\n",
        "        top_probs, top_indices = torch.topk(probs, top_k)\n",
        "        top_tokens = [self.tokenizer.decode([idx]) for idx in top_indices]\n",
        "        \n",
        "        # Create interactive plot\n",
        "        fig = go.Figure(data=[\n",
        "            go.Bar(\n",
        "                x=top_tokens,\n",
        "                y=top_probs.numpy(),\n",
        "                text=[f'{p:.2%}' for p in top_probs],\n",
        "                textposition='auto',\n",
        "                marker_color='lightblue'\n",
        "            )\n",
        "        ])\n",
        "        \n",
        "        fig.update_layout(\n",
        "            title=f'Next Token Probabilities after: \"{text}\"',\n",
        "            xaxis_title='Predicted Tokens',\n",
        "            yaxis_title='Probability',\n",
        "            height=400\n",
        "        )\n",
        "        \n",
        "        fig.show()\n",
        "        \n",
        "        return list(zip(top_tokens, top_probs.numpy()))\n",
        "\n",
        "# Initialize GPT model\n",
        "gpt = GPTTextGeneration()\n",
        "\n",
        "# Test generation strategies\n",
        "prompt = \"The future of artificial intelligence is\"\n",
        "print(f\"\\nðŸŽ¯ Prompt: '{prompt}'\\n\")\n",
        "\n",
        "# Compare strategies\n",
        "strategies_df = gpt.analyze_generation_strategies(prompt)\n",
        "print(\"\\nðŸ“Š Generation Strategies Comparison:\")\n",
        "for idx, row in strategies_df.iterrows():\n",
        "    print(f\"\\n{row['Strategy']} (T={row['Temperature']}, K={row['Top-K']}):\")\n",
        "    print(f\"  â†’ {row['Generated']}\")\n",
        "\n",
        "# Visualize token probabilities\n",
        "print(\"\\n\\nðŸ”® Next Token Predictions:\")\n",
        "predictions = gpt.visualize_token_probabilities(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 5: Few-shot Learning and In-Context Learning\n",
        "\n",
        "### Exercise 5: Implementing Few-shot Learning\n",
        "\n",
        "#### Concept\n",
        "Large language models can learn from examples in the prompt:\n",
        "- **Zero-shot**: Task description only\n",
        "- **One-shot**: Single example\n",
        "- **Few-shot**: Multiple examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 5: Few-shot Learning Implementation\n",
        "\n",
        "class FewShotLearning:\n",
        "    def __init__(self):\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
        "        self.model = GPT2LMHeadModel.from_pretrained('gpt2-medium')\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "    \n",
        "    def create_prompt(self, task: str, examples: List[Tuple[str, str]], query: str) -> str:\n",
        "        \"\"\"Create few-shot prompt\"\"\"\n",
        "        prompt = f\"Task: {task}\\n\\n\"\n",
        "        \n",
        "        # Add examples\n",
        "        for i, (input_text, output_text) in enumerate(examples, 1):\n",
        "            prompt += f\"Example {i}:\\n\"\n",
        "            prompt += f\"Input: {input_text}\\n\"\n",
        "            prompt += f\"Output: {output_text}\\n\\n\"\n",
        "        \n",
        "        # Add query\n",
        "        prompt += f\"Now:\\nInput: {query}\\nOutput:\"\n",
        "        \n",
        "        return prompt\n",
        "    \n",
        "    def perform_few_shot(self, task: str, examples: List[Tuple[str, str]], \n",
        "                        query: str, max_length: int = 100):\n",
        "        \"\"\"Perform few-shot learning\"\"\"\n",
        "        \n",
        "        # Create prompt\n",
        "        prompt = self.create_prompt(task, examples, query)\n",
        "        \n",
        "        # Generate response\n",
        "        input_ids = self.tokenizer.encode(prompt, return_tensors='pt')\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            output = self.model.generate(\n",
        "                input_ids,\n",
        "                max_length=len(input_ids[0]) + max_length,\n",
        "                temperature=0.8,\n",
        "                pad_token_id=self.tokenizer.eos_token_id,\n",
        "                do_sample=True\n",
        "            )\n",
        "        \n",
        "        # Decode and extract answer\n",
        "        full_text = self.tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "        answer = full_text[len(prompt):].strip()\n",
        "        \n",
        "        return prompt, answer\n",
        "    \n",
        "    def compare_shot_learning(self, task: str, examples: List[Tuple[str, str]], \n",
        "                             query: str):\n",
        "        \"\"\"Compare zero-shot, one-shot, and few-shot learning\"\"\"\n",
        "        \n",
        "        results = {}\n",
        "        \n",
        "        # Zero-shot\n",
        "        zero_prompt, zero_answer = self.perform_few_shot(task, [], query)\n",
        "        results['Zero-shot'] = {'prompt': zero_prompt, 'answer': zero_answer}\n",
        "        \n",
        "        # One-shot\n",
        "        one_prompt, one_answer = self.perform_few_shot(task, examples[:1], query)\n",
        "        results['One-shot'] = {'prompt': one_prompt, 'answer': one_answer}\n",
        "        \n",
        "        # Few-shot\n",
        "        few_prompt, few_answer = self.perform_few_shot(task, examples, query)\n",
        "        results['Few-shot'] = {'prompt': few_prompt, 'answer': few_answer}\n",
        "        \n",
        "        return results\n",
        "\n",
        "# Initialize few-shot learner\n",
        "fsl = FewShotLearning()\n",
        "\n",
        "# Define sentiment analysis task\n",
        "task = \"Classify the sentiment of the text as positive or negative\"\n",
        "\n",
        "examples = [\n",
        "    (\"I love this movie! It's amazing.\", \"positive\"),\n",
        "    (\"This product is terrible. Waste of money.\", \"negative\"),\n",
        "    (\"Best experience ever! Highly recommend.\", \"positive\")\n",
        "]\n",
        "\n",
        "query = \"The service was disappointing and slow.\"\n",
        "\n",
        "# Compare different shot learning\n",
        "results = fsl.compare_shot_learning(task, examples, query)\n",
        "\n",
        "print(\"\\nðŸŽ¯ Sentiment Analysis Task\\n\")\n",
        "print(f\"Query: '{query}'\\n\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for method, result in results.items():\n",
        "    print(f\"\\n{method}:\")\n",
        "    print(f\"Answer: {result['answer'][:50]}\")\n",
        "    print(\"-\"*40)\n",
        "\n",
        "# Visualize results\n",
        "methods = list(results.keys())\n",
        "prompt_lengths = [len(results[m]['prompt']) for m in methods]\n",
        "answer_lengths = [len(results[m]['answer'].split()[0]) if results[m]['answer'] else 0 for m in methods]\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "ax1.bar(methods, prompt_lengths, color=['#FF6B6B', '#FFA07A', '#98D8C8'])\n",
        "ax1.set_title('Prompt Length Comparison')\n",
        "ax1.set_ylabel('Characters')\n",
        "\n",
        "ax2.bar(methods, [0.3, 0.6, 0.9], color=['#FF6B6B', '#FFA07A', '#98D8C8'])\n",
        "ax2.set_title('Expected Accuracy (Illustrative)')\n",
        "ax2.set_ylabel('Accuracy')\n",
        "ax2.set_ylim(0, 1)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 6: Fine-tuning Strategies\n",
        "\n",
        "### Exercise 6: Parameter-Efficient Fine-tuning (PEFT)\n",
        "\n",
        "#### Concept\n",
        "Modern fine-tuning techniques optimize only a small subset of parameters:\n",
        "- **LoRA**: Low-Rank Adaptation\n",
        "- **Prefix Tuning**: Optimize continuous prompts\n",
        "- **Adapter Layers**: Small bottleneck modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 6: Simulate LoRA (Low-Rank Adaptation)\n",
        "\n",
        "class LoRASimulation:\n",
        "    \"\"\"\n",
        "    Simplified LoRA implementation for educational purposes\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, original_dim: int = 768, rank: int = 8):\n",
        "        \"\"\"\n",
        "        Initialize LoRA matrices\n",
        "        original_dim: Original weight matrix dimension\n",
        "        rank: Low-rank decomposition rank\n",
        "        \"\"\"\n",
        "        self.original_dim = original_dim\n",
        "        self.rank = rank\n",
        "        \n",
        "        # Original weight matrix (frozen)\n",
        "        self.W = nn.Parameter(torch.randn(original_dim, original_dim), requires_grad=False)\n",
        "        \n",
        "        # LoRA matrices (trainable)\n",
        "        self.A = nn.Parameter(torch.randn(original_dim, rank))\n",
        "        self.B = nn.Parameter(torch.randn(rank, original_dim))\n",
        "        \n",
        "        # Initialize A and B\n",
        "        nn.init.kaiming_uniform_(self.A)\n",
        "        nn.init.zeros_(self.B)\n",
        "    \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Forward pass with LoRA\"\"\"\n",
        "        # Original transformation\n",
        "        original_output = torch.matmul(x, self.W)\n",
        "        \n",
        "        # LoRA adaptation\n",
        "        lora_output = torch.matmul(torch.matmul(x, self.A), self.B)\n",
        "        \n",
        "        # Combine\n",
        "        return original_output + lora_output\n",
        "    \n",
        "    def count_parameters(self) -> dict:\n",
        "        \"\"\"Count trainable vs frozen parameters\"\"\"\n",
        "        original_params = self.W.numel()\n",
        "        lora_params = self.A.numel() + self.B.numel()\n",
        "        \n",
        "        return {\n",
        "            'original': original_params,\n",
        "            'lora': lora_params,\n",
        "            'total': original_params + lora_params,\n",
        "            'reduction': f\"{(1 - lora_params/original_params)*100:.1f}%\"\n",
        "        }\n",
        "\n",
        "def visualize_lora_efficiency():\n",
        "    \"\"\"Visualize parameter efficiency of LoRA\"\"\"\n",
        "    \n",
        "    dimensions = [128, 256, 512, 768, 1024]\n",
        "    ranks = [4, 8, 16, 32]\n",
        "    \n",
        "    # Calculate parameter savings\n",
        "    data = []\n",
        "    for dim in dimensions:\n",
        "        for rank in ranks:\n",
        "            original = dim * dim\n",
        "            lora = 2 * dim * rank\n",
        "            savings = (1 - lora/original) * 100\n",
        "            data.append({\n",
        "                'Dimension': dim,\n",
        "                'Rank': rank,\n",
        "                'Original': original,\n",
        "                'LoRA': lora,\n",
        "                'Savings': savings\n",
        "            })\n",
        "    \n",
        "    df = pd.DataFrame(data)\n",
        "    \n",
        "    # Create heatmap\n",
        "    pivot_df = df.pivot(index='Dimension', columns='Rank', values='Savings')\n",
        "    \n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.heatmap(pivot_df, annot=True, fmt='.1f', cmap='YlOrRd', \n",
        "                cbar_kws={'label': 'Parameter Reduction (%)'})\n",
        "    plt.title('LoRA Parameter Efficiency: Savings by Dimension and Rank')\n",
        "    plt.xlabel('LoRA Rank')\n",
        "    plt.ylabel('Model Dimension')\n",
        "    plt.show()\n",
        "    \n",
        "    # Interactive 3D plot\n",
        "    fig = go.Figure(data=[go.Surface(\n",
        "        z=pivot_df.values,\n",
        "        x=pivot_df.columns,\n",
        "        y=pivot_df.index,\n",
        "        colorscale='Viridis',\n",
        "        colorbar=dict(title='Savings (%)')\n",
        "    )])\n",
        "    \n",
        "    fig.update_layout(\n",
        "        title='LoRA Parameter Savings Surface',\n",
        "        scene=dict(\n",
        "            xaxis_title='Rank',\n",
        "            yaxis_title='Dimension',\n",
        "            zaxis_title='Savings (%)'\n",
        "        ),\n",
        "        height=500\n",
        "    )\n",
        "    \n",
        "    fig.show()\n",
        "\n",
        "# Test LoRA\n",
        "print(\"\\nðŸ”§ LoRA (Low-Rank Adaptation) Analysis\\n\")\n",
        "\n",
        "# Create LoRA model\n",
        "lora = LoRASimulation(original_dim=768, rank=8)\n",
        "params = lora.count_parameters()\n",
        "\n",
        "print(\"Parameter Count:\")\n",
        "print(f\"  Original: {params['original']:,}\")\n",
        "print(f\"  LoRA: {params['lora']:,}\")\n",
        "print(f\"  Reduction: {params['reduction']}\")\n",
        "\n",
        "# Visualize efficiency\n",
        "visualize_lora_efficiency()\n",
        "\n",
        "# Test forward pass\n",
        "x = torch.randn(32, 768)  # Batch of 32, dimension 768\n",
        "output = lora.forward(x)\n",
        "print(f\"\\nâœ… Forward pass successful: Input {x.shape} â†’ Output {output.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 7: Advanced Prompt Engineering\n",
        "\n",
        "### Exercise 7: Chain-of-Thought (CoT) Prompting\n",
        "\n",
        "#### Concept\n",
        "Chain-of-Thought prompting improves reasoning by:\n",
        "- Breaking down complex problems into steps\n",
        "- Showing intermediate reasoning\n",
        "- Improving accuracy on complex tasks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 7: Chain-of-Thought Prompting\n",
        "\n",
        "class ChainOfThoughtPrompting:\n",
        "    def __init__(self):\n",
        "        self.examples = {\n",
        "            'math': [\n",
        "                {\n",
        "                    'question': 'Roger has 5 tennis balls. He buys 2 cans of 3 balls each. How many total?',\n",
        "                    'cot': '''Let's think step by step:\n",
        "1. Roger starts with 5 tennis balls\n",
        "2. He buys 2 cans\n",
        "3. Each can has 3 balls, so 2 Ã— 3 = 6 balls\n",
        "4. Total: 5 + 6 = 11 balls\n",
        "Answer: 11'''\n",
        "                }\n",
        "            ],\n",
        "            'logic': [\n",
        "                {\n",
        "                    'question': 'All roses are flowers. Some flowers fade quickly. Can we conclude that some roses fade quickly?',\n",
        "                    'cot': '''Let's analyze step by step:\n",
        "1. All roses are flowers (roses âŠ† flowers)\n",
        "2. Some flowers fade quickly (âˆƒ flowers that fade)\n",
        "3. But we don't know if the fading flowers include roses\n",
        "4. The fading flowers might be non-rose flowers\n",
        "Answer: No, we cannot conclude this.'''\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    \n",
        "    def create_cot_prompt(self, task_type: str, question: str) -> str:\n",
        "        \"\"\"Create Chain-of-Thought prompt\"\"\"\n",
        "        \n",
        "        prompt = f\"Solve the following {task_type} problem step by step.\\n\\n\"\n",
        "        \n",
        "        # Add examples\n",
        "        if task_type in self.examples:\n",
        "            for example in self.examples[task_type]:\n",
        "                prompt += f\"Question: {example['question']}\\n\"\n",
        "                prompt += f\"{example['cot']}\\n\\n\"\n",
        "        \n",
        "        # Add new question\n",
        "        prompt += f\"Question: {question}\\n\"\n",
        "        prompt += \"Let's think step by step:\\n\"\n",
        "        \n",
        "        return prompt\n",
        "    \n",
        "    def compare_prompting_strategies(self, question: str):\n",
        "        \"\"\"Compare different prompting strategies\"\"\"\n",
        "        \n",
        "        strategies = {\n",
        "            'Direct': f\"Question: {question}\\nAnswer:\",\n",
        "            'CoT': self.create_cot_prompt('math', question),\n",
        "            'Zero-shot-CoT': f\"Question: {question}\\nLet's think step by step:\",\n",
        "            'Role-based': f\"You are a math expert. Solve this problem:\\n{question}\\nSolution:\"\n",
        "        }\n",
        "        \n",
        "        return strategies\n",
        "    \n",
        "    def visualize_prompt_comparison(self, strategies: dict):\n",
        "        \"\"\"Visualize different prompting strategies\"\"\"\n",
        "        \n",
        "        # Calculate metrics\n",
        "        metrics = []\n",
        "        for name, prompt in strategies.items():\n",
        "            metrics.append({\n",
        "                'Strategy': name,\n",
        "                'Length': len(prompt),\n",
        "                'Words': len(prompt.split()),\n",
        "                'Lines': len(prompt.split('\\n'))\n",
        "            })\n",
        "        \n",
        "        df = pd.DataFrame(metrics)\n",
        "        \n",
        "        # Create subplots\n",
        "        fig = make_subplots(\n",
        "            rows=1, cols=3,\n",
        "            subplot_titles=['Prompt Length', 'Word Count', 'Line Count']\n",
        "        )\n",
        "        \n",
        "        # Add traces\n",
        "        fig.add_trace(\n",
        "            go.Bar(x=df['Strategy'], y=df['Length'], name='Characters',\n",
        "                  marker_color='lightblue'),\n",
        "            row=1, col=1\n",
        "        )\n",
        "        \n",
        "        fig.add_trace(\n",
        "            go.Bar(x=df['Strategy'], y=df['Words'], name='Words',\n",
        "                  marker_color='lightgreen'),\n",
        "            row=1, col=2\n",
        "        )\n",
        "        \n",
        "        fig.add_trace(\n",
        "            go.Bar(x=df['Strategy'], y=df['Lines'], name='Lines',\n",
        "                  marker_color='lightcoral'),\n",
        "            row=1, col=3\n",
        "        )\n",
        "        \n",
        "        fig.update_layout(\n",
        "            title_text='Prompting Strategy Comparison',\n",
        "            showlegend=False,\n",
        "            height=400\n",
        "        )\n",
        "        \n",
        "        fig.show()\n",
        "        \n",
        "        return df\n",
        "\n",
        "# Test Chain-of-Thought\n",
        "cot = ChainOfThoughtPrompting()\n",
        "\n",
        "# Test question\n",
        "question = \"A store had 45 apples. They sold 17 in the morning and 12 in the afternoon. How many are left?\"\n",
        "\n",
        "print(\"\\nðŸ§  Chain-of-Thought Prompting Analysis\\n\")\n",
        "print(f\"Test Question: {question}\\n\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Compare strategies\n",
        "strategies = cot.compare_prompting_strategies(question)\n",
        "\n",
        "# Display each strategy\n",
        "for name, prompt in strategies.items():\n",
        "    print(f\"\\n{'='*20} {name} Strategy {'='*20}\")\n",
        "    print(prompt[:200] + \"...\" if len(prompt) > 200 else prompt)\n",
        "\n",
        "# Visualize comparison\n",
        "print(\"\\nðŸ“Š Strategy Metrics:\")\n",
        "metrics_df = cot.visualize_prompt_comparison(strategies)\n",
        "print(metrics_df.to_string())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 8: RLHF and Instruction Tuning\n",
        "\n",
        "### Exercise 8: Simulating RLHF Process\n",
        "\n",
        "#### Concept\n",
        "Reinforcement Learning from Human Feedback (RLHF) involves:\n",
        "1. Supervised Fine-tuning (SFT)\n",
        "2. Reward Model Training\n",
        "3. PPO Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 8: RLHF Process Simulation\n",
        "\n",
        "class RLHFSimulation:\n",
        "    def __init__(self):\n",
        "        self.responses_database = [\n",
        "            {\n",
        "                'prompt': 'Explain quantum computing',\n",
        "                'responses': [\n",
        "                    {'text': 'Quantum computing uses quantum bits that can be 0 and 1 simultaneously.', 'score': 0.7},\n",
        "                    {'text': 'It is a type of computation using quantum phenomena like superposition.', 'score': 0.8},\n",
        "                    {'text': 'Computers that are very fast.', 'score': 0.3}\n",
        "                ]\n",
        "            },\n",
        "            {\n",
        "                'prompt': 'How to make coffee?',\n",
        "                'responses': [\n",
        "                    {'text': '1. Boil water 2. Add coffee 3. Stir 4. Enjoy', 'score': 0.6},\n",
        "                    {'text': 'Heat water to 195-205Â°F, add 2 tbsp coffee per 6 oz water, brew 4-5 min', 'score': 0.9},\n",
        "                    {'text': 'Put coffee in water.', 'score': 0.2}\n",
        "                ]\n",
        "            }\n",
        "        ]\n",
        "    \n",
        "    def simulate_reward_model(self, response: str) -> float:\n",
        "        \"\"\"Simulate a reward model scoring\"\"\"\n",
        "        # Simple heuristics for demonstration\n",
        "        score = 0.5  # Base score\n",
        "        \n",
        "        # Length bonus\n",
        "        if 20 < len(response.split()) < 50:\n",
        "            score += 0.2\n",
        "        \n",
        "        # Specificity bonus\n",
        "        if any(char.isdigit() for char in response):\n",
        "            score += 0.1\n",
        "        \n",
        "        # Structure bonus\n",
        "        if '.' in response or ',' in response:\n",
        "            score += 0.1\n",
        "        \n",
        "        return min(score, 1.0)\n",
        "    \n",
        "    def ppo_update_simulation(self, responses: List[dict], learning_rate: float = 0.1):\n",
        "        \"\"\"Simulate PPO policy update\"\"\"\n",
        "        updated_responses = []\n",
        "        \n",
        "        for resp in responses:\n",
        "            # Simulate policy update based on reward\n",
        "            old_score = resp['score']\n",
        "            reward_signal = old_score - 0.5  # Centered around 0.5\n",
        "            \n",
        "            # Simulate update (simplified)\n",
        "            new_score = old_score + learning_rate * reward_signal\n",
        "            new_score = max(0, min(1, new_score))  # Clip to [0, 1]\n",
        "            \n",
        "            updated_responses.append({\n",
        "                'text': resp['text'],\n",
        "                'old_score': old_score,\n",
        "                'new_score': new_score,\n",
        "                'improvement': new_score - old_score\n",
        "            })\n",
        "        \n",
        "        return updated_responses\n",
        "    \n",
        "    def visualize_rlhf_process(self):\n",
        "        \"\"\"Visualize the RLHF training process\"\"\"\n",
        "        \n",
        "        # Simulate training iterations\n",
        "        iterations = 10\n",
        "        scores_over_time = []\n",
        "        \n",
        "        for i in range(iterations):\n",
        "            iter_scores = []\n",
        "            for prompt_data in self.responses_database:\n",
        "                for response in prompt_data['responses']:\n",
        "                    # Simulate improvement\n",
        "                    base_score = response['score']\n",
        "                    improvement = np.random.normal(0.02, 0.01) * i\n",
        "                    score = min(base_score + improvement, 1.0)\n",
        "                    iter_scores.append(score)\n",
        "            \n",
        "            scores_over_time.append({\n",
        "                'Iteration': i,\n",
        "                'Mean Score': np.mean(iter_scores),\n",
        "                'Min Score': np.min(iter_scores),\n",
        "                'Max Score': np.max(iter_scores)\n",
        "            })\n",
        "        \n",
        "        df = pd.DataFrame(scores_over_time)\n",
        "        \n",
        "        # Create visualization\n",
        "        fig = go.Figure()\n",
        "        \n",
        "        # Add traces\n",
        "        fig.add_trace(go.Scatter(\n",
        "            x=df['Iteration'],\n",
        "            y=df['Mean Score'],\n",
        "            mode='lines+markers',\n",
        "            name='Mean Score',\n",
        "            line=dict(color='blue', width=3)\n",
        "        ))\n",
        "        \n",
        "        # Add confidence band\n",
        "        fig.add_trace(go.Scatter(\n",
        "            x=df['Iteration'].tolist() + df['Iteration'].tolist()[::-1],\n",
        "            y=df['Max Score'].tolist() + df['Min Score'].tolist()[::-1],\n",
        "            fill='toself',\n",
        "            fillcolor='rgba(0,100,200,0.2)',\n",
        "            line=dict(color='rgba(255,255,255,0)'),\n",
        "            showlegend=False,\n",
        "            name='Range'\n",
        "        ))\n",
        "        \n",
        "        fig.update_layout(\n",
        "            title='RLHF Training Progress Simulation',\n",
        "            xaxis_title='Training Iteration',\n",
        "            yaxis_title='Reward Score',\n",
        "            height=400\n",
        "        )\n",
        "        \n",
        "        fig.show()\n",
        "        \n",
        "        return df\n",
        "    \n",
        "    def demonstrate_improvement(self, prompt: str):\n",
        "        \"\"\"Show response improvement through RLHF\"\"\"\n",
        "        \n",
        "        print(f\"\\nðŸ“ Prompt: '{prompt}'\\n\")\n",
        "        print(\"=\"*60)\n",
        "        \n",
        "        # Find responses for this prompt\n",
        "        prompt_data = next((p for p in self.responses_database if p['prompt'] == prompt), None)\n",
        "        \n",
        "        if prompt_data:\n",
        "            # Before RLHF\n",
        "            print(\"\\nðŸ”´ Before RLHF:\")\n",
        "            for resp in prompt_data['responses']:\n",
        "                print(f\"  Score {resp['score']:.1f}: {resp['text']}\")\n",
        "            \n",
        "            # After RLHF (simulated)\n",
        "            updated = self.ppo_update_simulation(prompt_data['responses'])\n",
        "            \n",
        "            print(\"\\nðŸŸ¢ After RLHF (simulated):\")\n",
        "            sorted_updated = sorted(updated, key=lambda x: x['new_score'], reverse=True)\n",
        "            for resp in sorted_updated:\n",
        "                print(f\"  Score {resp['new_score']:.2f} ({resp['improvement']:+.2f}): {resp['text']}\")\n",
        "\n",
        "# Test RLHF Simulation\n",
        "rlhf = RLHFSimulation()\n",
        "\n",
        "print(\"\\nðŸŽ¯ RLHF (Reinforcement Learning from Human Feedback) Simulation\\n\")\n",
        "\n",
        "# Visualize training progress\n",
        "progress_df = rlhf.visualize_rlhf_process()\n",
        "\n",
        "# Demonstrate improvement on specific prompts\n",
        "for prompt_data in rlhf.responses_database:\n",
        "    rlhf.demonstrate_improvement(prompt_data['prompt'])\n",
        "\n",
        "print(\"\\nðŸ“Š Training Statistics:\")\n",
        "print(f\"  Initial Mean Score: {progress_df.iloc[0]['Mean Score']:.3f}\")\n",
        "print(f\"  Final Mean Score: {progress_df.iloc[-1]['Mean Score']:.3f}\")\n",
        "print(f\"  Improvement: {(progress_df.iloc[-1]['Mean Score'] - progress_df.iloc[0]['Mean Score']):.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 9: Model Architecture Comparison and Scaling Laws\n",
        "\n",
        "### Exercise 9: Understanding Scaling Laws\n",
        "\n",
        "#### Concept\n",
        "Model performance follows predictable scaling laws:\n",
        "- Performance improves with model size\n",
        "- Emergent abilities appear at scale thresholds\n",
        "- Trade-offs between performance and efficiency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 9: Scaling Laws and Model Comparison\n",
        "\n",
        "def analyze_scaling_laws():\n",
        "    \"\"\"Analyze and visualize scaling laws in language models\"\"\"\n",
        "    \n",
        "    # Model data (approximate values for illustration)\n",
        "    models = [\n",
        "        {'name': 'BERT-Base', 'params': 110e6, 'year': 2018, 'type': 'Encoder', 'performance': 0.82},\n",
        "        {'name': 'BERT-Large', 'params': 340e6, 'year': 2018, 'type': 'Encoder', 'performance': 0.86},\n",
        "        {'name': 'GPT-2', 'params': 1.5e9, 'year': 2019, 'type': 'Decoder', 'performance': 0.88},\n",
        "        {'name': 'T5-Base', 'params': 220e6, 'year': 2019, 'type': 'Enc-Dec', 'performance': 0.84},\n",
        "        {'name': 'T5-Large', 'params': 770e6, 'year': 2019, 'type': 'Enc-Dec', 'performance': 0.87},\n",
        "        {'name': 'GPT-3', 'params': 175e9, 'year': 2020, 'type': 'Decoder', 'performance': 0.93},\n",
        "        {'name': 'T5-11B', 'params': 11e9, 'year': 2019, 'type': 'Enc-Dec', 'performance': 0.90},\n",
        "        {'name': 'GPT-4*', 'params': 1e12, 'year': 2023, 'type': 'Decoder', 'performance': 0.97}\n",
        "    ]\n",
        "    \n",
        "    df = pd.DataFrame(models)\n",
        "    \n",
        "    # Create comprehensive visualization\n",
        "    fig = make_subplots(\n",
        "        rows=2, cols=2,\n",
        "        subplot_titles=[\n",
        "            'Scaling Law: Size vs Performance',\n",
        "            'Model Evolution Timeline',\n",
        "            'Architecture Distribution',\n",
        "            'Emergent Abilities Threshold'\n",
        "        ],\n",
        "        specs=[\n",
        "            [{'type': 'scatter'}, {'type': 'scatter'}],\n",
        "            [{'type': 'bar'}, {'type': 'scatter'}]\n",
        "        ]\n",
        "    )\n",
        "    \n",
        "    # 1. Scaling Law Plot\n",
        "    colors = {'Encoder': 'blue', 'Decoder': 'red', 'Enc-Dec': 'green'}\n",
        "    for model_type in df['type'].unique():\n",
        "        data = df[df['type'] == model_type]\n",
        "        fig.add_trace(\n",
        "            go.Scatter(\n",
        "                x=np.log10(data['params']),\n",
        "                y=data['performance'],\n",
        "                mode='markers+text',\n",
        "                name=model_type,\n",
        "                text=data['name'],\n",
        "                textposition='top center',\n",
        "                marker=dict(size=10, color=colors[model_type])\n",
        "            ),\n",
        "            row=1, col=1\n",
        "        )\n",
        "    \n",
        "    # Add trend line\n",
        "    x_trend = np.log10(df['params'])\n",
        "    z = np.polyfit(x_trend, df['performance'], 2)\n",
        "    p = np.poly1d(z)\n",
        "    x_line = np.linspace(x_trend.min(), x_trend.max(), 100)\n",
        "    fig.add_trace(\n",
        "        go.Scatter(\n",
        "            x=x_line,\n",
        "            y=p(x_line),\n",
        "            mode='lines',\n",
        "            name='Trend',\n",
        "            line=dict(dash='dash', color='gray')\n",
        "        ),\n",
        "        row=1, col=1\n",
        "    )\n",
        "    \n",
        "    # 2. Timeline Evolution\n",
        "    fig.add_trace(\n",
        "        go.Scatter(\n",
        "            x=df['year'],\n",
        "            y=np.log10(df['params']),\n",
        "            mode='markers+lines',\n",
        "            name='Model Size',\n",
        "            marker=dict(size=12),\n",
        "            text=df['name'],\n",
        "            textposition='top center'\n",
        "        ),\n",
        "        row=1, col=2\n",
        "    )\n",
        "    \n",
        "    # 3. Architecture Distribution\n",
        "    arch_counts = df['type'].value_counts()\n",
        "    fig.add_trace(\n",
        "        go.Bar(\n",
        "            x=arch_counts.index,\n",
        "            y=arch_counts.values,\n",
        "            name='Count',\n",
        "            marker_color=['blue', 'red', 'green']\n",
        "        ),\n",
        "        row=2, col=1\n",
        "    )\n",
        "    \n",
        "    # 4. Emergent Abilities\n",
        "    # Simulate emergent abilities appearing at different scales\n",
        "    abilities = [\n",
        "        {'size': 1e9, 'ability': 'Basic QA', 'score': 0.3},\n",
        "        {'size': 10e9, 'ability': 'Few-shot', 'score': 0.6},\n",
        "        {'size': 100e9, 'ability': 'CoT', 'score': 0.8},\n",
        "        {'size': 500e9, 'ability': 'Reasoning', 'score': 0.9},\n",
        "        {'size': 1e12, 'ability': 'Multi-modal', 'score': 0.95}\n",
        "    ]\n",
        "    \n",
        "    abilities_df = pd.DataFrame(abilities)\n",
        "    fig.add_trace(\n",
        "        go.Scatter(\n",
        "            x=np.log10(abilities_df['size']),\n",
        "            y=abilities_df['score'],\n",
        "            mode='markers+lines',\n",
        "            name='Abilities',\n",
        "            marker=dict(size=15, color='purple'),\n",
        "            text=abilities_df['ability'],\n",
        "            textposition='top center'\n",
        "        ),\n",
        "        row=2, col=2\n",
        "    )\n",
        "    \n",
        "    # Update layout\n",
        "    fig.update_xaxes(title_text=\"Log10(Parameters)\", row=1, col=1)\n",
        "    fig.update_yaxes(title_text=\"Performance\", row=1, col=1)\n",
        "    fig.update_xaxes(title_text=\"Year\", row=1, col=2)\n",
        "    fig.update_yaxes(title_text=\"Log10(Parameters)\", row=1, col=2)\n",
        "    fig.update_xaxes(title_text=\"Architecture Type\", row=2, col=1)\n",
        "    fig.update_yaxes(title_text=\"Count\", row=2, col=1)\n",
        "    fig.update_xaxes(title_text=\"Log10(Model Size)\", row=2, col=2)\n",
        "    fig.update_yaxes(title_text=\"Capability Score\", row=2, col=2)\n",
        "    \n",
        "    fig.update_layout(height=800, showlegend=True, title_text=\"Language Model Scaling Analysis\")\n",
        "    fig.show()\n",
        "    \n",
        "    # Print insights\n",
        "    print(\"\\nðŸ“Š Key Scaling Insights:\")\n",
        "    print(f\"1. Performance Range: {df['performance'].min():.2f} to {df['performance'].max():.2f}\")\n",
        "    print(f\"2. Parameter Range: {df['params'].min()/1e6:.0f}M to {df['params'].max()/1e9:.0f}B\")\n",
        "    print(f\"3. Years Covered: {df['year'].min()} to {df['year'].max()}\")\n",
        "    print(f\"4. Most Common Architecture: {df['type'].mode()[0]}\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Analyze scaling laws\n",
        "print(\"\\nðŸ“ˆ Language Model Scaling Laws Analysis\\n\")\n",
        "scaling_df = analyze_scaling_laws()\n",
        "\n",
        "print(\"\\nðŸ“‹ Model Comparison Table:\")\n",
        "print(scaling_df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 10: Summary and Practice Exercises\n",
        "\n",
        "### ðŸŽ“ Key Takeaways\n",
        "\n",
        "1. **Paradigm Shift**: From task-specific to general-purpose foundation models\n",
        "2. **Pre-training**: Self-supervised learning on massive unlabeled data\n",
        "3. **Architecture Types**:\n",
        "   - Encoder (BERT): Bidirectional understanding\n",
        "   - Decoder (GPT): Autoregressive generation\n",
        "   - Encoder-Decoder (T5): Flexible sequence-to-sequence\n",
        "4. **Fine-tuning**: Adapt pre-trained models to specific tasks\n",
        "5. **Prompting**: In-context learning without parameter updates\n",
        "6. **RLHF**: Align models with human preferences\n",
        "7. **Scaling Laws**: Predictable performance improvements with size\n",
        "\n",
        "### ðŸ“ Practice Exercises"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 10: Comprehensive Practice\n",
        "\n",
        "print(\"\\nðŸŽ¯ Practice Exercises\\n\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Exercise 1: Custom Tokenization\n",
        "print(\"\\nðŸ“Œ Exercise 1: Implement custom tokenization\")\n",
        "print(\"Task: Create a function that compares tokenization across languages\")\n",
        "print(\"Hint: Use multiple sentences in different languages\")\n",
        "\n",
        "# Exercise 2: MLM Task\n",
        "print(\"\\nðŸ“Œ Exercise 2: Create domain-specific MLM\")\n",
        "print(\"Task: Fine-tune BERT for medical/legal domain MLM\")\n",
        "print(\"Hint: Use domain-specific vocabulary\")\n",
        "\n",
        "# Exercise 3: Few-shot Classification\n",
        "print(\"\\nðŸ“Œ Exercise 3: Build few-shot classifier\")\n",
        "print(\"Task: Create a few-shot emotion classifier\")\n",
        "print(\"Hint: Use 5 emotions with 3 examples each\")\n",
        "\n",
        "# Exercise 4: Prompt Optimization\n",
        "print(\"\\nðŸ“Œ Exercise 4: Optimize prompts automatically\")\n",
        "print(\"Task: Implement prompt search algorithm\")\n",
        "print(\"Hint: Use genetic algorithm or beam search\")\n",
        "\n",
        "# Exercise 5: PEFT Implementation\n",
        "print(\"\\nðŸ“Œ Exercise 5: Implement Prefix Tuning\")\n",
        "print(\"Task: Create prefix tuning for sentiment analysis\")\n",
        "print(\"Hint: Optimize continuous prompt vectors\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"\\nðŸŽ‰ Congratulations! You've completed the LLM hands-on tutorial!\")\n",
        "print(\"\\nðŸ“š Further Reading:\")\n",
        "print(\"- Attention Is All You Need (Vaswani et al., 2017)\")\n",
        "print(\"- BERT: Pre-training of Deep Bidirectional Transformers (Devlin et al., 2018)\")\n",
        "print(\"- Language Models are Few-Shot Learners (Brown et al., 2020)\")\n",
        "print(\"- LoRA: Low-Rank Adaptation of Large Language Models (Hu et al., 2021)\")\n",
        "print(\"- Constitutional AI: Harmlessness from AI Feedback (Bai et al., 2022)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ðŸŽ¯ Final Project Ideas\n",
        "\n",
        "1. **Build a Custom ChatBot**: Fine-tune GPT-2 on domain-specific data\n",
        "2. **Create a Fact-Checker**: Use BERT for claim verification\n",
        "3. **Develop a Code Generator**: Implement few-shot code generation\n",
        "4. **Design a Prompt Library**: Create optimized prompts for various tasks\n",
        "5. **Implement RLHF**: Build a simple preference learning system\n",
        "\n",
        "## ðŸ“§ Contact\n",
        "\n",
        "For questions or feedback:\n",
        "- Original Lecture: Ho-min Park (homin.park@ghent.ac.kr)\n",
        "- Notebook Creation: AI-Generated Interactive Tutorial\n",
        "\n",
        "---\n",
        "\n",
        "**Thank you for learning!** ðŸš€"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}