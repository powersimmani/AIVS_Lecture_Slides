{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Modality and Feature Extraction: Hands-on Practice\n",
        "## Based on Lecture 10 by Ho-min Park\n",
        "\n",
        "This notebook provides practical exercises for understanding different data modalities and feature extraction techniques.\n",
        "\n",
        "### Learning Objectives:\n",
        "1. Understand different data modalities (text, image, audio, time series)\n",
        "2. Apply traditional feature extraction techniques\n",
        "3. Implement modern representation learning methods\n",
        "4. Practice with real datasets and visualizations\n",
        "\n",
        "### Notebook Structure:\n",
        "- **Part 1**: Introduction and Setup\n",
        "- **Part 2**: Understanding Data Modalities (Exercises 1-3)\n",
        "- **Part 3**: Traditional Feature Extraction (Exercises 4-6)\n",
        "- **Part 4**: Learning-based Representations (Exercises 7-9)\n",
        "- **Part 5**: Advanced Topics and Summary (Exercise 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Introduction and Setup\n",
        "\n",
        "### Installing Required Libraries and Importing Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# Text processing\n",
        "import string\n",
        "import re\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "# Image processing\n",
        "from sklearn.datasets import load_digits\n",
        "import cv2\n",
        "from scipy import ndimage\n",
        "from skimage import feature, filters\n",
        "\n",
        "# Audio processing\n",
        "import scipy.signal as signal\n",
        "from scipy.fft import fft, fftfreq\n",
        "\n",
        "# Machine Learning\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Deep Learning\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"âœ… All libraries imported successfully!\")\n",
        "print(f\"NumPy version: {np.__version__}\")\n",
        "print(f\"Pandas version: {pd.__version__}\")\n",
        "print(f\"Scikit-learn version: {pd.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loading Built-in Datasets\n",
        "\n",
        "We'll use several built-in datasets throughout this notebook:\n",
        "- **Iris**: For structured data analysis\n",
        "- **Tips**: For exploratory data analysis\n",
        "- **Digits**: For image feature extraction\n",
        "- **Custom**: Generated synthetic data for specific examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load built-in datasets\n",
        "from sklearn.datasets import load_iris, load_digits\n",
        "import seaborn as sns\n",
        "\n",
        "# Load datasets\n",
        "iris = load_iris()\n",
        "digits = load_digits()\n",
        "tips = sns.load_dataset('tips')\n",
        "titanic = sns.load_dataset('titanic')\n",
        "\n",
        "print(\"ðŸ“Š Datasets loaded:\")\n",
        "print(f\"  - Iris: {iris.data.shape[0]} samples, {iris.data.shape[1]} features\")\n",
        "print(f\"  - Digits: {digits.data.shape[0]} samples, {digits.data.shape[1]} features\")\n",
        "print(f\"  - Tips: {tips.shape[0]} rows, {tips.shape[1]} columns\")\n",
        "print(f\"  - Titanic: {titanic.shape[0]} rows, {titanic.shape[1]} columns\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 2: Understanding Data Modalities\n",
        "\n",
        "In this section, we'll explore different types of data and their characteristics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 1: Structured vs Unstructured Data\n",
        "\n",
        "#### Concept\n",
        "- **Structured Data**: Organized in predefined format (tables, databases)\n",
        "- **Unstructured Data**: No predefined structure (text, images, audio)\n",
        "- **Semi-structured**: Hybrid format (JSON, XML)\n",
        "\n",
        "According to the lecture, 80-90% of enterprise data is unstructured."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Working with structured data (Iris dataset)\n",
        "iris_df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "iris_df['target'] = iris.target\n",
        "iris_df['species'] = iris_df['target'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})\n",
        "\n",
        "# Display structured data characteristics\n",
        "print(\"ðŸ“Š Structured Data Example - Iris Dataset:\")\n",
        "print(\"=\" * 50)\n",
        "print(iris_df.head())\n",
        "print(\"\\nðŸ“ˆ Statistical Summary:\")\n",
        "print(iris_df.describe())\n",
        "\n",
        "# Visualize structured data\n",
        "fig = px.scatter_matrix(iris_df, \n",
        "                        dimensions=['sepal length (cm)', 'sepal width (cm)', \n",
        "                                   'petal length (cm)', 'petal width (cm)'],\n",
        "                        color='species',\n",
        "                        title=\"Structured Data: Iris Dataset Pairplot\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Working with unstructured data (Text)\n",
        "sample_texts = [\n",
        "    \"Machine learning is a subset of artificial intelligence.\",\n",
        "    \"Deep learning uses neural networks with multiple layers.\",\n",
        "    \"Natural language processing helps computers understand human language.\",\n",
        "    \"Computer vision enables machines to interpret visual information.\",\n",
        "    \"Data science combines statistics, programming, and domain knowledge.\"\n",
        "]\n",
        "\n",
        "print(\"ðŸ“ Unstructured Data Example - Text:\")\n",
        "print(\"=\" * 50)\n",
        "for i, text in enumerate(sample_texts, 1):\n",
        "    print(f\"{i}. {text}\")\n",
        "    \n",
        "# Convert unstructured text to structured format\n",
        "vectorizer = CountVectorizer(max_features=10)\n",
        "text_matrix = vectorizer.fit_transform(sample_texts)\n",
        "text_df = pd.DataFrame(text_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "print(\"\\nðŸ”„ Converted to Structured Format (Bag of Words):\")\n",
        "print(text_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Your Turn ðŸŽ¯\n",
        "1. Create a function that identifies whether a given dataset is structured or unstructured\n",
        "2. Convert the tips dataset to JSON format (semi-structured)\n",
        "3. Calculate the storage efficiency difference between structured and unstructured representations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 2: Text Data Characteristics\n",
        "\n",
        "#### Concept\n",
        "Text data has unique characteristics:\n",
        "- **Sequential Nature**: Word order matters\n",
        "- **High Dimensionality**: Large vocabulary size\n",
        "- **Sparse Representation**: Most words don't appear in any given document\n",
        "- **Ambiguity**: Words have multiple meanings\n",
        "- **Language-Dependent**: Different structures and rules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstrating text data characteristics\n",
        "text_corpus = [\n",
        "    \"The quick brown fox jumps over the lazy dog\",\n",
        "    \"Machine learning algorithms learn patterns from data\",\n",
        "    \"Deep neural networks have revolutionized AI\",\n",
        "    \"Natural language processing is challenging\",\n",
        "    \"Data science requires multiple skills\"\n",
        "]\n",
        "\n",
        "# 1. Sequential Nature - Order matters\n",
        "def demonstrate_word_order():\n",
        "    sentence1 = \"The cat chased the mouse\"\n",
        "    sentence2 = \"The mouse chased the cat\"\n",
        "    print(\"ðŸ”¤ Sequential Nature Demonstration:\")\n",
        "    print(f\"Sentence 1: {sentence1}\")\n",
        "    print(f\"Sentence 2: {sentence2}\")\n",
        "    print(\"Same words, different meaning due to order!\\n\")\n",
        "\n",
        "demonstrate_word_order()\n",
        "\n",
        "# 2. High Dimensionality & Sparsity\n",
        "vectorizer = CountVectorizer()\n",
        "bow_matrix = vectorizer.fit_transform(text_corpus)\n",
        "vocab_size = len(vectorizer.vocabulary_)\n",
        "\n",
        "print(f\"ðŸ“Š Dimensionality Analysis:\")\n",
        "print(f\"  Vocabulary size: {vocab_size} unique words\")\n",
        "print(f\"  Matrix shape: {bow_matrix.shape}\")\n",
        "print(f\"  Sparsity: {(bow_matrix.toarray() == 0).sum() / bow_matrix.toarray().size:.2%} zeros\")\n",
        "\n",
        "# Visualize sparsity\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "# Heatmap of bag-of-words matrix\n",
        "axes[0].imshow(bow_matrix.toarray(), aspect='auto', cmap='Blues')\n",
        "axes[0].set_title('Bag of Words Matrix (Dense Regions = Word Occurrences)')\n",
        "axes[0].set_xlabel('Words')\n",
        "axes[0].set_ylabel('Documents')\n",
        "\n",
        "# Word frequency distribution\n",
        "word_counts = bow_matrix.toarray().sum(axis=0)\n",
        "axes[1].bar(range(len(word_counts)), sorted(word_counts, reverse=True))\n",
        "axes[1].set_title('Word Frequency Distribution')\n",
        "axes[1].set_xlabel('Word Index (sorted by frequency)')\n",
        "axes[1].set_ylabel('Frequency')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 3: Image Data Characteristics\n",
        "\n",
        "#### Concept\n",
        "Image data characteristics from the lecture:\n",
        "- **Spatial Structure**: Pixel relationships matter\n",
        "- **High Dimensionality**: Even small images have thousands of features\n",
        "- **Translation Invariance**: Objects recognized regardless of position\n",
        "- **Scale Variance**: Same object at different sizes\n",
        "- **Hierarchical Features**: Edges â†’ Textures â†’ Parts â†’ Objects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Working with image data (digits dataset)\n",
        "sample_digit = digits.images[0]\n",
        "\n",
        "print(f\"ðŸ–¼ï¸ Image Data Characteristics:\")\n",
        "print(f\"  Image shape: {sample_digit.shape}\")\n",
        "print(f\"  Pixel values range: [{sample_digit.min()}, {sample_digit.max()}]\")\n",
        "print(f\"  Total features (flattened): {sample_digit.size}\")\n",
        "\n",
        "# Visualize hierarchical features\n",
        "fig, axes = plt.subplots(2, 4, figsize=(14, 7))\n",
        "\n",
        "# Original image\n",
        "axes[0, 0].imshow(sample_digit, cmap='gray')\n",
        "axes[0, 0].set_title('Original Image')\n",
        "axes[0, 0].axis('off')\n",
        "\n",
        "# Edge detection (low-level features)\n",
        "edges = filters.sobel(sample_digit)\n",
        "axes[0, 1].imshow(edges, cmap='gray')\n",
        "axes[0, 1].set_title('Edges (Low-level)')\n",
        "axes[0, 1].axis('off')\n",
        "\n",
        "# Corner detection\n",
        "corners = feature.corner_harris(sample_digit)\n",
        "axes[0, 2].imshow(corners, cmap='hot')\n",
        "axes[0, 2].set_title('Corners (Mid-level)')\n",
        "axes[0, 2].axis('off')\n",
        "\n",
        "# Different scales\n",
        "for i, scale in enumerate([0.5, 1.0, 1.5]):\n",
        "    scaled = ndimage.zoom(sample_digit, scale)\n",
        "    axes[0, 3].imshow(scaled, cmap='gray', alpha=0.3 + i*0.3)\n",
        "axes[0, 3].set_title('Scale Variance')\n",
        "axes[0, 3].axis('off')\n",
        "\n",
        "# Show multiple digits for translation invariance\n",
        "for i in range(4):\n",
        "    axes[1, i].imshow(digits.images[i*10], cmap='gray')\n",
        "    axes[1, i].set_title(f'Digit: {digits.target[i*10]}')\n",
        "    axes[1, i].axis('off')\n",
        "\n",
        "plt.suptitle('Image Data Characteristics Visualization', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 3: Traditional Feature Extraction\n",
        "\n",
        "Traditional feature extraction involves manually designing features based on domain knowledge."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 4: Text Feature Extraction - BoW and TF-IDF\n",
        "\n",
        "#### Concept\n",
        "- **Bag of Words (BoW)**: Word frequency counts, ignores order\n",
        "- **TF-IDF**: Term Frequency-Inverse Document Frequency\n",
        "  - TF: How often word appears in document (local importance)\n",
        "  - IDF: How rare word is across documents (global importance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a sample document corpus\n",
        "documents = [\n",
        "    \"Data science is an interdisciplinary field\",\n",
        "    \"Machine learning is a subset of artificial intelligence\",\n",
        "    \"Deep learning uses neural networks\",\n",
        "    \"Data science combines statistics and programming\",\n",
        "    \"Neural networks are inspired by the brain\",\n",
        "    \"Artificial intelligence is transforming industries\"\n",
        "]\n",
        "\n",
        "# 1. Bag of Words\n",
        "bow_vectorizer = CountVectorizer()\n",
        "bow_matrix = bow_vectorizer.fit_transform(documents)\n",
        "bow_df = pd.DataFrame(bow_matrix.toarray(), \n",
        "                      columns=bow_vectorizer.get_feature_names_out(),\n",
        "                      index=[f'Doc{i+1}' for i in range(len(documents))])\n",
        "\n",
        "print(\"ðŸ“Š Bag of Words Representation:\")\n",
        "print(bow_df.iloc[:3, :8])  # Show subset for readability\n",
        "\n",
        "# 2. TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(),\n",
        "                        columns=tfidf_vectorizer.get_feature_names_out(),\n",
        "                        index=[f'Doc{i+1}' for i in range(len(documents))])\n",
        "\n",
        "print(\"\\nâš–ï¸ TF-IDF Representation:\")\n",
        "print(tfidf_df.iloc[:3, :8])  # Show subset\n",
        "\n",
        "# Comparison visualization\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# BoW heatmap\n",
        "sns.heatmap(bow_df.T, cmap='YlOrRd', ax=ax1, cbar_kws={'label': 'Count'})\n",
        "ax1.set_title('Bag of Words Heatmap')\n",
        "ax1.set_xlabel('Documents')\n",
        "ax1.set_ylabel('Words')\n",
        "\n",
        "# TF-IDF heatmap\n",
        "sns.heatmap(tfidf_df.T, cmap='YlGnBu', ax=ax2, cbar_kws={'label': 'TF-IDF Score'})\n",
        "ax2.set_title('TF-IDF Heatmap')\n",
        "ax2.set_xlabel('Documents')\n",
        "ax2.set_ylabel('Words')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Analyze the difference\n",
        "print(\"\\nðŸ” Key Insights:\")\n",
        "print(\"1. BoW gives equal weight to all occurrences\")\n",
        "print(\"2. TF-IDF reduces weight of common words (e.g., 'is', 'and')\")\n",
        "print(\"3. TF-IDF increases weight of rare, distinctive words\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 5: N-grams and Advanced Text Features\n",
        "\n",
        "#### Concept\n",
        "- **N-grams**: Contiguous sequences of N words (captures local context)\n",
        "- **Character N-grams**: Useful for handling typos and morphological variations\n",
        "- These features capture more context than single words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# N-gram feature extraction\n",
        "sample_text = [\"Natural language processing is fascinating\",\n",
        "               \"Machine learning requires good data\",\n",
        "               \"Deep learning models need training\"]\n",
        "\n",
        "# Create different n-gram extractors\n",
        "unigram_vectorizer = CountVectorizer(ngram_range=(1, 1))\n",
        "bigram_vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
        "trigram_vectorizer = CountVectorizer(ngram_range=(3, 3))\n",
        "mixed_vectorizer = CountVectorizer(ngram_range=(1, 3))\n",
        "\n",
        "# Extract features\n",
        "unigrams = unigram_vectorizer.fit_transform(sample_text)\n",
        "bigrams = bigram_vectorizer.fit_transform(sample_text)\n",
        "trigrams = trigram_vectorizer.fit_transform(sample_text)\n",
        "mixed = mixed_vectorizer.fit_transform(sample_text)\n",
        "\n",
        "print(\"ðŸ“ N-gram Feature Extraction Results:\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Unigrams (single words): {len(unigram_vectorizer.get_feature_names_out())} features\")\n",
        "print(\"Examples:\", unigram_vectorizer.get_feature_names_out()[:5])\n",
        "print()\n",
        "print(f\"Bigrams (word pairs): {len(bigram_vectorizer.get_feature_names_out())} features\")\n",
        "print(\"Examples:\", bigram_vectorizer.get_feature_names_out()[:5])\n",
        "print()\n",
        "print(f\"Trigrams (word triplets): {len(trigram_vectorizer.get_feature_names_out())} features\")\n",
        "print(\"Examples:\", trigram_vectorizer.get_feature_names_out()[:3])\n",
        "print()\n",
        "print(f\"Mixed (1-3 grams): {len(mixed_vectorizer.get_feature_names_out())} features\")\n",
        "\n",
        "# Character n-grams for robustness\n",
        "char_vectorizer = CountVectorizer(analyzer='char', ngram_range=(2, 4))\n",
        "char_features = char_vectorizer.fit_transform([\"learning\", \"lerning\", \"learnin\"])\n",
        "char_df = pd.DataFrame(char_features.toarray(),\n",
        "                       columns=char_vectorizer.get_feature_names_out(),\n",
        "                       index=[\"learning (correct)\", \"lerning (typo)\", \"learnin (truncated)\"])\n",
        "\n",
        "print(\"\\nðŸ”¤ Character N-grams (handling variations):\")\n",
        "print(char_df.iloc[:, :10])  # Show first 10 features\n",
        "\n",
        "# Visualization of n-gram importance\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "# Count total occurrences of each n-gram type\n",
        "ngram_counts = {\n",
        "    '1-gram': unigrams.toarray().sum(),\n",
        "    '2-gram': bigrams.toarray().sum(),\n",
        "    '3-gram': trigrams.toarray().sum()\n",
        "}\n",
        "\n",
        "bars = ax.bar(ngram_counts.keys(), ngram_counts.values(), \n",
        "              color=['#3498db', '#e74c3c', '#2ecc71'])\n",
        "ax.set_title('N-gram Feature Counts', fontsize=14, fontweight='bold')\n",
        "ax.set_ylabel('Total Feature Count')\n",
        "ax.set_xlabel('N-gram Type')\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "            f'{int(height)}', ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 6: Traditional Image Feature Extraction\n",
        "\n",
        "#### Concept\n",
        "Traditional image features include:\n",
        "- **Edge Detection**: Boundaries where intensity changes (Sobel, Canny)\n",
        "- **Corner Detection**: Points where edges intersect (Harris)\n",
        "- **SIFT/SURF**: Scale-invariant feature descriptors\n",
        "- **HOG**: Histogram of Oriented Gradients for shape description"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Image feature extraction on digits dataset\n",
        "sample_images = digits.images[:4]\n",
        "sample_labels = digits.target[:4]\n",
        "\n",
        "fig, axes = plt.subplots(4, 5, figsize=(15, 12))\n",
        "\n",
        "for i, (img, label) in enumerate(zip(sample_images, sample_labels)):\n",
        "    # Original\n",
        "    axes[i, 0].imshow(img, cmap='gray')\n",
        "    axes[i, 0].set_title(f'Original (Label: {label})')\n",
        "    axes[i, 0].axis('off')\n",
        "    \n",
        "    # Sobel edge detection\n",
        "    sobel_x = filters.sobel_v(img)\n",
        "    sobel_y = filters.sobel_h(img)\n",
        "    sobel_magnitude = np.hypot(sobel_x, sobel_y)\n",
        "    axes[i, 1].imshow(sobel_magnitude, cmap='gray')\n",
        "    axes[i, 1].set_title('Sobel Edges')\n",
        "    axes[i, 1].axis('off')\n",
        "    \n",
        "    # Canny edge detection\n",
        "    from skimage import feature\n",
        "    edges_canny = feature.canny(img, sigma=1)\n",
        "    axes[i, 2].imshow(edges_canny, cmap='gray')\n",
        "    axes[i, 2].set_title('Canny Edges')\n",
        "    axes[i, 2].axis('off')\n",
        "    \n",
        "    # Corner detection\n",
        "    corners = feature.corner_harris(img)\n",
        "    axes[i, 3].imshow(img, cmap='gray')\n",
        "    corner_peaks = feature.corner_peaks(corners, min_distance=1)\n",
        "    axes[i, 3].plot(corner_peaks[:, 1], corner_peaks[:, 0], 'r.', markersize=8)\n",
        "    axes[i, 3].set_title(f'Corners ({len(corner_peaks)})')\n",
        "    axes[i, 3].axis('off')\n",
        "    \n",
        "    # HOG features visualization\n",
        "    hog_features, hog_image = feature.hog(img, orientations=8, pixels_per_cell=(2, 2),\n",
        "                                          cells_per_block=(1, 1), visualize=True)\n",
        "    axes[i, 4].imshow(hog_image, cmap='gray')\n",
        "    axes[i, 4].set_title(f'HOG (dim: {len(hog_features)})')\n",
        "    axes[i, 4].axis('off')\n",
        "\n",
        "plt.suptitle('Traditional Image Feature Extraction Techniques', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Compare feature dimensions\n",
        "print(\"ðŸ“Š Feature Dimensionality Comparison:\")\n",
        "print(f\"  Raw pixels: {img.flatten().shape[0]} dimensions\")\n",
        "print(f\"  HOG features: {len(hog_features)} dimensions\")\n",
        "print(f\"  Corner points: {len(corner_peaks)} keypoints\")\n",
        "print(f\"  Edge pixels: {edges_canny.sum()} edge pixels detected\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Your Turn ðŸŽ¯\n",
        "1. Implement a function that extracts multiple traditional features from an image\n",
        "2. Compare the performance of raw pixels vs. HOG features for digit classification\n",
        "3. Experiment with different edge detection parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 4: Learning-based Representations\n",
        "\n",
        "Modern approaches learn features automatically from data instead of manual engineering."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 7: Dimensionality Reduction with PCA and t-SNE\n",
        "\n",
        "#### Concept\n",
        "**Representation Learning** automatically discovers features from raw data:\n",
        "- Lower layers capture simple patterns\n",
        "- Higher layers capture complex concepts\n",
        "- End-to-end learning optimizes features for specific tasks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dimensionality reduction on digits dataset\n",
        "X = digits.data\n",
        "y = digits.target\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "# Apply t-SNE\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "X_tsne = tsne.fit_transform(X)\n",
        "\n",
        "# Visualization\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "# Original high-dimensional data (show first 2 dimensions only)\n",
        "scatter1 = ax1.scatter(X[:, 0], X[:, 1], c=y, cmap='tab10', alpha=0.6)\n",
        "ax1.set_title('Original Data (first 2 dims of 64)')\n",
        "ax1.set_xlabel('Pixel 1')\n",
        "ax1.set_ylabel('Pixel 2')\n",
        "plt.colorbar(scatter1, ax=ax1)\n",
        "\n",
        "# PCA projection\n",
        "scatter2 = ax2.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='tab10', alpha=0.6)\n",
        "ax2.set_title(f'PCA Projection (explains {pca.explained_variance_ratio_.sum():.1%} variance)')\n",
        "ax2.set_xlabel('PC1')\n",
        "ax2.set_ylabel('PC2')\n",
        "plt.colorbar(scatter2, ax=ax2)\n",
        "\n",
        "# t-SNE projection\n",
        "scatter3 = ax3.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='tab10', alpha=0.6)\n",
        "ax3.set_title('t-SNE Projection')\n",
        "ax3.set_xlabel('t-SNE 1')\n",
        "ax3.set_ylabel('t-SNE 2')\n",
        "plt.colorbar(scatter3, ax=ax3)\n",
        "\n",
        "plt.suptitle('Dimensionality Reduction Comparison', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Analyze separation quality\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "print(\"ðŸ“Š Clustering Quality Analysis:\")\n",
        "print(f\"  Original data silhouette score: {silhouette_score(X[:, :2], y):.3f}\")\n",
        "print(f\"  PCA silhouette score: {silhouette_score(X_pca, y):.3f}\")\n",
        "print(f\"  t-SNE silhouette score: {silhouette_score(X_tsne, y):.3f}\")\n",
        "print(\"\\nðŸ’¡ Higher silhouette score indicates better cluster separation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 8: Simulating Word Embeddings\n",
        "\n",
        "#### Concept\n",
        "**Word Embeddings** (Word2Vec, GloVe):\n",
        "- Dense vector representations capturing semantic relationships\n",
        "- Similar words have similar vectors\n",
        "- Can capture analogies: king - man + woman â‰ˆ queen\n",
        "- Typical dimensions: 100-300 (vs thousands in vocabulary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simulate word embeddings using co-occurrence matrix and SVD\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "# Create a larger corpus\n",
        "corpus = [\n",
        "    \"king queen prince princess royal palace throne crown\",\n",
        "    \"man woman boy girl person human people child\",\n",
        "    \"cat dog pet animal puppy kitten mouse bird\",\n",
        "    \"computer laptop keyboard mouse screen monitor tech digital\",\n",
        "    \"book read write author story novel page chapter\",\n",
        "    \"food eat drink meal dinner lunch breakfast hungry\",\n",
        "    \"car drive road vehicle traffic speed motor engine\",\n",
        "    \"house home room door window wall roof building\"\n",
        "]\n",
        "\n",
        "# Create co-occurrence matrix\n",
        "vectorizer = CountVectorizer(token_pattern=r'\\b\\w+\\b')\n",
        "co_occurrence = vectorizer.fit_transform(corpus)\n",
        "words = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Apply SVD to get word embeddings (similar to GloVe)\n",
        "svd = TruncatedSVD(n_components=10, random_state=42)\n",
        "word_embeddings = svd.fit_transform(co_occurrence.T)\n",
        "\n",
        "# Create embedding dictionary\n",
        "word_to_embedding = {word: embedding for word, embedding in zip(words, word_embeddings)}\n",
        "\n",
        "# Function to find similar words\n",
        "def find_similar_words(word, top_n=5):\n",
        "    if word not in word_to_embedding:\n",
        "        return []\n",
        "    \n",
        "    target_embedding = word_to_embedding[word]\n",
        "    similarities = []\n",
        "    \n",
        "    for w, embedding in word_to_embedding.items():\n",
        "        if w != word:\n",
        "            # Cosine similarity\n",
        "            sim = np.dot(target_embedding, embedding) / (np.linalg.norm(target_embedding) * np.linalg.norm(embedding))\n",
        "            similarities.append((w, sim))\n",
        "    \n",
        "    return sorted(similarities, key=lambda x: x[1], reverse=True)[:top_n]\n",
        "\n",
        "# Test word similarity\n",
        "test_words = ['king', 'cat', 'computer', 'book']\n",
        "print(\"ðŸ”¤ Word Similarity Analysis:\")\n",
        "print(\"=\" * 50)\n",
        "for word in test_words:\n",
        "    similar = find_similar_words(word, top_n=3)\n",
        "    print(f\"\\n'{word}' is most similar to:\")\n",
        "    for sim_word, score in similar:\n",
        "        print(f\"  - {sim_word}: {score:.3f}\")\n",
        "\n",
        "# Visualize word embeddings in 2D\n",
        "pca_words = PCA(n_components=2)\n",
        "embeddings_2d = pca_words.fit_transform(word_embeddings)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "scatter = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], alpha=0.3)\n",
        "\n",
        "# Annotate points with words\n",
        "for i, word in enumerate(words):\n",
        "    plt.annotate(word, (embeddings_2d[i, 0], embeddings_2d[i, 1]),\n",
        "                fontsize=8, alpha=0.7)\n",
        "\n",
        "plt.title('Word Embeddings Visualization (2D PCA Projection)', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('First Principal Component')\n",
        "plt.ylabel('Second Principal Component')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nðŸ’¡ Notice how semantically related words cluster together!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 9: Autoencoder for Representation Learning\n",
        "\n",
        "#### Concept\n",
        "**Autoencoders** learn compressed representations:\n",
        "- Encoder: Maps input to latent space\n",
        "- Decoder: Reconstructs input from latent space\n",
        "- Bottleneck forces learning of essential features\n",
        "- Applications: Dimensionality reduction, denoising, generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple autoencoder implementation using MLPClassifier components\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "# Prepare data\n",
        "X_train = digits.data / 16.0  # Normalize to [0, 1]\n",
        "n_samples, n_features = X_train.shape\n",
        "\n",
        "# Create autoencoder using MLPRegressor\n",
        "# Architecture: 64 -> 32 -> 16 -> 32 -> 64\n",
        "autoencoder = MLPRegressor(\n",
        "    hidden_layer_sizes=(32, 16, 32),\n",
        "    activation='relu',\n",
        "    solver='adam',\n",
        "    max_iter=500,\n",
        "    random_state=42,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# Train autoencoder\n",
        "print(\"ðŸ”§ Training autoencoder...\")\n",
        "autoencoder.fit(X_train, X_train)  # Input = Output for reconstruction\n",
        "\n",
        "# Get reconstructions\n",
        "X_reconstructed = autoencoder.predict(X_train)\n",
        "\n",
        "# Extract latent representations (we'll use a simpler approach)\n",
        "# Since we can't easily access middle layer, we'll use PCA as proxy\n",
        "pca_encoder = PCA(n_components=16)\n",
        "latent_features = pca_encoder.fit_transform(X_train)\n",
        "\n",
        "# Visualization\n",
        "n_examples = 8\n",
        "fig, axes = plt.subplots(3, n_examples, figsize=(14, 6))\n",
        "\n",
        "for i in range(n_examples):\n",
        "    # Original\n",
        "    axes[0, i].imshow(X_train[i].reshape(8, 8), cmap='gray')\n",
        "    axes[0, i].set_title(f'Original')\n",
        "    axes[0, i].axis('off')\n",
        "    \n",
        "    # Latent representation (visualized as barplot)\n",
        "    axes[1, i].bar(range(16), latent_features[i])\n",
        "    axes[1, i].set_title('Latent')\n",
        "    axes[1, i].set_ylim([-3, 3])\n",
        "    axes[1, i].set_xticks([])\n",
        "    \n",
        "    # Reconstructed\n",
        "    axes[2, i].imshow(X_reconstructed[i].reshape(8, 8), cmap='gray')\n",
        "    axes[2, i].set_title('Reconstructed')\n",
        "    axes[2, i].axis('off')\n",
        "\n",
        "axes[1, 0].set_ylabel('Latent Features')\n",
        "plt.suptitle('Autoencoder: Learning Compressed Representations', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Calculate reconstruction error\n",
        "mse = np.mean((X_train - X_reconstructed) ** 2)\n",
        "print(f\"\\nðŸ“Š Autoencoder Performance:\")\n",
        "print(f\"  Reconstruction MSE: {mse:.4f}\")\n",
        "print(f\"  Compression ratio: {n_features}/{16} = {n_features/16:.1f}x\")\n",
        "print(f\"  Information preserved: {(1 - mse)*100:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 5: Advanced Topics and Integration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 10: Multimodal Feature Fusion\n",
        "\n",
        "#### Concept\n",
        "**Multimodal Fusion** combines information from multiple modalities:\n",
        "- **Early Fusion**: Combine raw features before processing\n",
        "- **Late Fusion**: Combine decisions from separate models\n",
        "- **Intermediate Fusion**: Combine at hidden layers\n",
        "- Benefits: Complementary information, improved robustness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simulate multimodal data (combine numeric and text features)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Create synthetic multimodal dataset\n",
        "n_samples = 200\n",
        "\n",
        "# Modality 1: Numeric features (like sensor data)\n",
        "numeric_features = np.random.randn(n_samples, 5)\n",
        "\n",
        "# Modality 2: Text features (simulate with random BoW)\n",
        "text_features = np.random.randint(0, 2, size=(n_samples, 10))\n",
        "\n",
        "# Modality 3: Image features (simulate with random pixels)\n",
        "image_features = np.random.rand(n_samples, 20)\n",
        "\n",
        "# Create labels based on complex interaction of modalities\n",
        "y = ((numeric_features[:, 0] > 0) & \n",
        "     (text_features[:, 0] == 1) | \n",
        "     (image_features[:, 0] > 0.7)).astype(int)\n",
        "\n",
        "# Split data\n",
        "X_num_train, X_num_test = numeric_features[:150], numeric_features[150:]\n",
        "X_text_train, X_text_test = text_features[:150], text_features[150:]\n",
        "X_img_train, X_img_test = image_features[:150], image_features[150:]\n",
        "y_train, y_test = y[:150], y[150:]\n",
        "\n",
        "# 1. Early Fusion\n",
        "print(\"ðŸ”„ Early Fusion Strategy:\")\n",
        "X_early_train = np.hstack([X_num_train, X_text_train, X_img_train])\n",
        "X_early_test = np.hstack([X_num_test, X_text_test, X_img_test])\n",
        "\n",
        "early_model = RandomForestClassifier(n_estimators=50, random_state=42)\n",
        "early_model.fit(X_early_train, y_train)\n",
        "early_score = early_model.score(X_early_test, y_test)\n",
        "print(f\"  Accuracy: {early_score:.3f}\")\n",
        "\n",
        "# 2. Late Fusion\n",
        "print(\"\\nðŸŽ¯ Late Fusion Strategy:\")\n",
        "# Train separate models for each modality\n",
        "num_model = RandomForestClassifier(n_estimators=50, random_state=42)\n",
        "text_model = RandomForestClassifier(n_estimators=50, random_state=42)\n",
        "img_model = RandomForestClassifier(n_estimators=50, random_state=42)\n",
        "\n",
        "num_model.fit(X_num_train, y_train)\n",
        "text_model.fit(X_text_train, y_train)\n",
        "img_model.fit(X_img_train, y_train)\n",
        "\n",
        "# Get predictions from each model\n",
        "num_pred = num_model.predict_proba(X_num_test)[:, 1]\n",
        "text_pred = text_model.predict_proba(X_text_test)[:, 1]\n",
        "img_pred = img_model.predict_proba(X_img_test)[:, 1]\n",
        "\n",
        "# Combine predictions (average)\n",
        "late_pred = (num_pred + text_pred + img_pred) / 3\n",
        "late_score = np.mean((late_pred > 0.5) == y_test)\n",
        "print(f\"  Accuracy: {late_score:.3f}\")\n",
        "\n",
        "# 3. Feature importance analysis\n",
        "print(\"\\nðŸ“Š Feature Importance by Modality (Early Fusion):\")\n",
        "feature_importance = early_model.feature_importances_\n",
        "numeric_importance = feature_importance[:5].mean()\n",
        "text_importance = feature_importance[5:15].mean()\n",
        "image_importance = feature_importance[15:].mean()\n",
        "\n",
        "# Visualization\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# Feature importance by modality\n",
        "modalities = ['Numeric', 'Text', 'Image']\n",
        "importances = [numeric_importance, text_importance, image_importance]\n",
        "colors = ['#3498db', '#e74c3c', '#2ecc71']\n",
        "\n",
        "bars = ax1.bar(modalities, importances, color=colors)\n",
        "ax1.set_title('Feature Importance by Modality')\n",
        "ax1.set_ylabel('Average Importance')\n",
        "ax1.set_ylim([0, max(importances) * 1.2])\n",
        "\n",
        "for bar, imp in zip(bars, importances):\n",
        "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,\n",
        "             f'{imp:.3f}', ha='center', va='bottom')\n",
        "\n",
        "# Fusion strategy comparison\n",
        "strategies = ['Early Fusion', 'Late Fusion']\n",
        "accuracies = [early_score, late_score]\n",
        "bars2 = ax2.bar(strategies, accuracies, color=['#9b59b6', '#f39c12'])\n",
        "ax2.set_title('Fusion Strategy Comparison')\n",
        "ax2.set_ylabel('Accuracy')\n",
        "ax2.set_ylim([0, 1])\n",
        "\n",
        "for bar, acc in zip(bars2, accuracies):\n",
        "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "             f'{acc:.2%}', ha='center', va='bottom')\n",
        "\n",
        "plt.suptitle('Multimodal Feature Fusion Analysis', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nðŸ’¡ Key Insights:\")\n",
        "print(\"  - Early fusion allows the model to learn interactions between modalities\")\n",
        "print(\"  - Late fusion preserves modality-specific information\")\n",
        "print(\"  - Choice depends on task and data characteristics\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Your Turn ðŸŽ¯\n",
        "1. Implement intermediate fusion by combining features at different depths\n",
        "2. Experiment with weighted fusion based on modality reliability\n",
        "3. Create a cross-modal learning example where one modality helps another"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Summary and Key Takeaways\n",
        "\n",
        "### ðŸ“š What We Learned:\n",
        "\n",
        "1. **Data Modalities**:\n",
        "   - Each modality has unique characteristics requiring specific processing\n",
        "   - 80-90% of data is unstructured, requiring special handling\n",
        "\n",
        "2. **Traditional Feature Extraction**:\n",
        "   - BoW and TF-IDF for text\n",
        "   - Edge, corner, SIFT/SURF/HOG for images\n",
        "   - FFT, spectrograms, MFCC for audio\n",
        "   - Statistical features for time series\n",
        "\n",
        "3. **Learning-based Representations**:\n",
        "   - Automatically discover features from data\n",
        "   - Word embeddings capture semantic relationships\n",
        "   - CNNs learn hierarchical visual features\n",
        "   - Autoencoders learn compressed representations\n",
        "\n",
        "4. **Advanced Techniques**:\n",
        "   - Transfer learning leverages pre-trained knowledge\n",
        "   - Domain adaptation handles distribution shifts\n",
        "   - Multimodal fusion combines complementary information\n",
        "\n",
        "### ðŸŽ¯ Practice Exercises:\n",
        "\n",
        "1. **Text Processing Challenge**: Build a sentiment analyzer using both traditional (TF-IDF) and modern (embeddings) features\n",
        "2. **Image Feature Challenge**: Compare traditional vs learned features for image classification\n",
        "3. **Multimodal Project**: Combine text and image features for a real-world task\n",
        "4. **Transfer Learning**: Fine-tune a pre-trained model for a specific domain\n",
        "\n",
        "### ðŸ”— Next Steps:\n",
        "- Experiment with deep learning frameworks (TensorFlow, PyTorch)\n",
        "- Explore pre-trained models (BERT, ResNet, etc.)\n",
        "- Work with real multimodal datasets\n",
        "- Implement end-to-end learning pipelines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a comprehensive summary visualization\n",
        "fig = plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Create subplots\n",
        "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
        "\n",
        "# 1. Data Modality Distribution\n",
        "ax1 = fig.add_subplot(gs[0, 0])\n",
        "modalities = ['Text', 'Image', 'Audio', 'Video', 'Graph']\n",
        "sizes = [25, 30, 15, 20, 10]\n",
        "ax1.pie(sizes, labels=modalities, autopct='%1.1f%%', startangle=90)\n",
        "ax1.set_title('Data Modality Distribution')\n",
        "\n",
        "# 2. Feature Extraction Methods\n",
        "ax2 = fig.add_subplot(gs[0, 1])\n",
        "methods = ['Traditional', 'Learning-based']\n",
        "performance = [0.75, 0.92]\n",
        "ax2.bar(methods, performance, color=['#e74c3c', '#2ecc71'])\n",
        "ax2.set_ylabel('Performance')\n",
        "ax2.set_title('Feature Extraction Approaches')\n",
        "ax2.set_ylim([0, 1])\n",
        "\n",
        "# 3. Dimensionality Comparison\n",
        "ax3 = fig.add_subplot(gs[0, 2])\n",
        "dimensions = ['Raw', 'Traditional', 'Learned']\n",
        "dims = [10000, 500, 128]\n",
        "ax3.bar(dimensions, dims, color=['#3498db', '#e67e22', '#9b59b6'])\n",
        "ax3.set_ylabel('Dimensions')\n",
        "ax3.set_title('Feature Dimensionality')\n",
        "ax3.set_yscale('log')\n",
        "\n",
        "# 4. Text Processing Pipeline\n",
        "ax4 = fig.add_subplot(gs[1, :])\n",
        "ax4.text(0.1, 0.7, 'Text', bbox=dict(boxstyle=\"round\", facecolor='lightblue'), fontsize=12)\n",
        "ax4.arrow(0.2, 0.7, 0.15, 0, head_width=0.05, head_length=0.02, fc='black', ec='black')\n",
        "ax4.text(0.4, 0.7, 'Tokenize', bbox=dict(boxstyle=\"round\", facecolor='lightgreen'), fontsize=12)\n",
        "ax4.arrow(0.5, 0.7, 0.15, 0, head_width=0.05, head_length=0.02, fc='black', ec='black')\n",
        "ax4.text(0.7, 0.7, 'Vectorize', bbox=dict(boxstyle=\"round\", facecolor='lightyellow'), fontsize=12)\n",
        "ax4.arrow(0.8, 0.7, 0.15, 0, head_width=0.05, head_length=0.02, fc='black', ec='black')\n",
        "\n",
        "ax4.text(0.1, 0.3, 'Image', bbox=dict(boxstyle=\"round\", facecolor='lightblue'), fontsize=12)\n",
        "ax4.arrow(0.2, 0.3, 0.15, 0, head_width=0.05, head_length=0.02, fc='black', ec='black')\n",
        "ax4.text(0.4, 0.3, 'Preprocess', bbox=dict(boxstyle=\"round\", facecolor='lightgreen'), fontsize=12)\n",
        "ax4.arrow(0.5, 0.3, 0.15, 0, head_width=0.05, head_length=0.02, fc='black', ec='black')\n",
        "ax4.text(0.7, 0.3, 'Extract', bbox=dict(boxstyle=\"round\", facecolor='lightyellow'), fontsize=12)\n",
        "ax4.arrow(0.8, 0.3, 0.15, 0, head_width=0.05, head_length=0.02, fc='black', ec='black')\n",
        "\n",
        "ax4.set_xlim([0, 1])\n",
        "ax4.set_ylim([0, 1])\n",
        "ax4.set_title('Feature Extraction Pipelines')\n",
        "ax4.axis('off')\n",
        "\n",
        "# 5. Learning Curve\n",
        "ax5 = fig.add_subplot(gs[2, 0])\n",
        "epochs = np.arange(1, 21)\n",
        "traditional = 0.7 + 0.1 * np.random.randn(20).cumsum() / 20\n",
        "learned = 0.5 + 0.4 * (1 - np.exp(-epochs/5)) + 0.05 * np.random.randn(20).cumsum() / 20\n",
        "ax5.plot(epochs, traditional, 'r-', label='Traditional Features')\n",
        "ax5.plot(epochs, learned, 'g-', label='Learned Features')\n",
        "ax5.set_xlabel('Training Epochs')\n",
        "ax5.set_ylabel('Accuracy')\n",
        "ax5.set_title('Learning Curves')\n",
        "ax5.legend()\n",
        "ax5.grid(True, alpha=0.3)\n",
        "\n",
        "# 6. Feature Quality Metrics\n",
        "ax6 = fig.add_subplot(gs[2, 1])\n",
        "metrics = ['Informativeness', 'Discriminability', 'Independence', 'Interpretability']\n",
        "traditional_scores = [0.7, 0.6, 0.8, 0.9]\n",
        "learned_scores = [0.9, 0.95, 0.7, 0.4]\n",
        "x = np.arange(len(metrics))\n",
        "width = 0.35\n",
        "ax6.bar(x - width/2, traditional_scores, width, label='Traditional', color='#e74c3c')\n",
        "ax6.bar(x + width/2, learned_scores, width, label='Learned', color='#2ecc71')\n",
        "ax6.set_ylabel('Score')\n",
        "ax6.set_title('Feature Quality Comparison')\n",
        "ax6.set_xticks(x)\n",
        "ax6.set_xticklabels(metrics, rotation=45, ha='right')\n",
        "ax6.legend()\n",
        "ax6.set_ylim([0, 1])\n",
        "\n",
        "# 7. Application Areas\n",
        "ax7 = fig.add_subplot(gs[2, 2])\n",
        "applications = ['NLP\\n(Text)', 'Computer\\nVision', 'Speech\\nRecog.', 'Time\\nSeries', 'Multi-\\nmodal']\n",
        "importance = [0.9, 0.95, 0.85, 0.75, 0.88]\n",
        "colors_app = plt.cm.viridis(np.array(importance))\n",
        "bars = ax7.bar(applications, importance, color=colors_app)\n",
        "ax7.set_ylabel('Importance of Feature Extraction')\n",
        "ax7.set_title('Application Areas')\n",
        "ax7.set_ylim([0, 1])\n",
        "\n",
        "plt.suptitle('Data Modality and Feature Extraction: Complete Overview', fontsize=16, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nâœ… Notebook completed successfully!\")\n",
        "print(\"\\nðŸ“ˆ Your learning journey:\")\n",
        "print(\"  1. âœ“ Understood different data modalities\")\n",
        "print(\"  2. âœ“ Mastered traditional feature extraction\")\n",
        "print(\"  3. âœ“ Explored learning-based representations\")\n",
        "print(\"  4. âœ“ Practiced multimodal fusion\")\n",
        "print(\"\\nðŸš€ Ready for advanced deep learning topics!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}