{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b507d29b",
   "metadata": {},
   "source": [
    "# ðŸ¤– Transformer Architecture: From Theory to Practice\n",
    "## Interactive Learning Notebook Based on Lecture 13\n",
    "\n",
    "This notebook provides hands-on implementation of Transformer architecture concepts, including:\n",
    "- Self-Attention Mechanism\n",
    "- Multi-Head Attention\n",
    "- Positional Encoding\n",
    "- Complete Transformer Implementation\n",
    "- Practical Applications\n",
    "\n",
    "**Author**: Ho-min Park  \n",
    "**Contact**: homin.park@ghent.ac.kr\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74cee94",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Environment Configuration\n",
    "### 1.1 Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc7f754",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# For text processing\n",
    "from collections import Counter\n",
    "import re\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Setup complete! âœ…\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5540d690",
   "metadata": {},
   "source": [
    "## Part 2: Core Concepts - Understanding Attention Mechanism\n",
    "\n",
    "### Exercise 1: Implementing Scaled Dot-Product Attention\n",
    "#### ðŸ“š Concept\n",
    "Self-attention allows the model to look at other positions in the input sequence when encoding a current position. The key formula is:\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "Where:\n",
    "- **Q (Query)**: What information am I looking for?\n",
    "- **K (Key)**: What information do I contain?\n",
    "- **V (Value)**: The actual information I store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16c4359",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Scaled Dot-Product Attention implementation\n",
    "    As described in 'Attention Is All You Need' (Vaswani et al., 2017)\n",
    "    \"\"\"\n",
    "    def __init__(self, temperature=1.0, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        # q, k, v: [batch_size, n_heads, seq_len, d_k]\n",
    "        batch_size, n_heads, len_q, d_k = q.size()\n",
    "        \n",
    "        # Calculate attention scores\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / (d_k ** 0.5)\n",
    "        \n",
    "        # Apply mask if provided (for padding or causal masking)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # Apply softmax to get attention weights\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        # Apply attention weights to values\n",
    "        output = torch.matmul(attention_weights, v)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# Test the implementation\n",
    "def test_scaled_attention():\n",
    "    batch_size, n_heads, seq_len, d_k = 2, 8, 10, 64\n",
    "    \n",
    "    # Create random Q, K, V matrices\n",
    "    q = torch.randn(batch_size, n_heads, seq_len, d_k)\n",
    "    k = torch.randn(batch_size, n_heads, seq_len, d_k)\n",
    "    v = torch.randn(batch_size, n_heads, seq_len, d_k)\n",
    "    \n",
    "    # Create attention module\n",
    "    attention = ScaledDotProductAttention()\n",
    "    \n",
    "    # Forward pass\n",
    "    output, weights = attention(q, k, v)\n",
    "    \n",
    "    print(f\"Input shapes: Q={q.shape}, K={k.shape}, V={v.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(f\"Attention weights shape: {weights.shape}\")\n",
    "    print(f\"Attention weights sum per position: {weights[0, 0, 0].sum().item():.4f}\")\n",
    "    \n",
    "    return output, weights\n",
    "\n",
    "output, attention_weights = test_scaled_attention()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d3ed3d",
   "metadata": {},
   "source": [
    "### Visualization: Attention Weight Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d275e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention_weights(attention_weights, title=\"Attention Weight Patterns\"):\n",
    "    \"\"\"\n",
    "    Visualize attention weight patterns using both matplotlib and plotly\n",
    "    \"\"\"\n",
    "    # Take first batch, first head for visualization\n",
    "    weights = attention_weights[0, 0].detach().numpy()\n",
    "    \n",
    "    # Create subplots\n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=2,\n",
    "        subplot_titles=(\"Heatmap View\", \"3D Surface View\"),\n",
    "        specs=[[{\"type\": \"heatmap\"}, {\"type\": \"surface\"}]]\n",
    "    )\n",
    "    \n",
    "    # Heatmap\n",
    "    fig.add_trace(\n",
    "        go.Heatmap(\n",
    "            z=weights,\n",
    "            colorscale='Viridis',\n",
    "            showscale=True,\n",
    "            colorbar=dict(x=0.45)\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # 3D Surface\n",
    "    fig.add_trace(\n",
    "        go.Surface(\n",
    "            z=weights,\n",
    "            colorscale='Viridis',\n",
    "            showscale=False\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title_text=title,\n",
    "        height=400,\n",
    "        showlegend=False\n",
    "    )\n",
    "    \n",
    "    # Update axes labels\n",
    "    fig.update_xaxes(title_text=\"Key Position\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Query Position\", row=1, col=1)\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    # Also create matplotlib version for static view\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.heatmap(weights, cmap='YlOrRd', cbar_kws={'label': 'Attention Weight'})\n",
    "    plt.title('Attention Pattern Heatmap')\n",
    "    plt.xlabel('Key Position')\n",
    "    plt.ylabel('Query Position')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    # Show attention distribution for selected positions\n",
    "    positions_to_show = [0, len(weights)//2, len(weights)-1]\n",
    "    for pos in positions_to_show:\n",
    "        plt.plot(weights[pos], label=f'Query pos {pos}', marker='o', markersize=4)\n",
    "    plt.xlabel('Key Position')\n",
    "    plt.ylabel('Attention Weight')\n",
    "    plt.title('Attention Distribution for Selected Positions')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_attention_weights(attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadaf28f",
   "metadata": {},
   "source": [
    "### ðŸŽ¯ Your Turn: Exercise 1\n",
    "Modify the attention mechanism to implement **masked self-attention** for causal language modeling. \n",
    "The mask should prevent positions from attending to subsequent positions (look-ahead mask)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc72a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement causal masking\n",
    "def create_causal_mask(seq_len):\n",
    "    \"\"\"\n",
    "    Create a causal mask to prevent attending to future positions\n",
    "    Hint: Use torch.triu to create upper triangular matrix\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    mask = None  # Replace with your implementation\n",
    "    \n",
    "    # Solution (uncomment to see):\n",
    "    # mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
    "    # mask = ~mask  # Invert: True where attention is allowed\n",
    "    \n",
    "    return mask\n",
    "\n",
    "# Test your implementation\n",
    "# seq_len = 5\n",
    "# mask = create_causal_mask(seq_len)\n",
    "# print(\"Causal mask (True = can attend):\")\n",
    "# print(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea546d6",
   "metadata": {},
   "source": [
    "---\n",
    "### Exercise 2: Multi-Head Attention Implementation\n",
    "#### ðŸ“š Concept\n",
    "Multi-head attention allows the model to jointly attend to information from different representation subspaces. Instead of performing a single attention function, we linearly project the queries, keys and values h times with different projections.\n",
    "\n",
    "**Key insight**: Different heads learn different types of relationships (syntax, semantics, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6892b644",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Attention module\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model=512, n_heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        \n",
    "        # Linear projections for Q, K, V\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.attention = ScaledDotProductAttention(dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        batch_size, seq_len, _ = q.size()\n",
    "        \n",
    "        # Store residual for skip connection\n",
    "        residual = q\n",
    "        \n",
    "        # Linear projections and reshape for multi-head\n",
    "        Q = self.W_q(q).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(k).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(v).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Apply attention\n",
    "        attn_output, attn_weights = self.attention(Q, K, V, mask)\n",
    "        \n",
    "        # Concatenate heads\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(\n",
    "            batch_size, seq_len, self.d_model\n",
    "        )\n",
    "        \n",
    "        # Final linear projection\n",
    "        output = self.W_o(attn_output)\n",
    "        output = self.dropout(output)\n",
    "        \n",
    "        # Add residual and normalize\n",
    "        output = self.layer_norm(output + residual)\n",
    "        \n",
    "        return output, attn_weights\n",
    "\n",
    "# Test Multi-Head Attention\n",
    "def test_multihead_attention():\n",
    "    batch_size, seq_len, d_model = 2, 10, 512\n",
    "    n_heads = 8\n",
    "    \n",
    "    # Create random input\n",
    "    x = torch.randn(batch_size, seq_len, d_model)\n",
    "    \n",
    "    # Create MHA module\n",
    "    mha = MultiHeadAttention(d_model, n_heads)\n",
    "    \n",
    "    # Forward pass (self-attention: q=k=v=x)\n",
    "    output, attn_weights = mha(x, x, x)\n",
    "    \n",
    "    print(f\"Input shape: {x.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(f\"Attention weights shape: {attn_weights.shape}\")\n",
    "    print(f\"Number of parameters: {sum(p.numel() for p in mha.parameters()):,}\")\n",
    "    \n",
    "    return output, attn_weights\n",
    "\n",
    "mha_output, mha_weights = test_multihead_attention()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7faffffa",
   "metadata": {},
   "source": [
    "### Analyzing Multi-Head Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32299119",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_head_specialization(mha_weights):\n",
    "    \"\"\"\n",
    "    Analyze what different attention heads focus on\n",
    "    \"\"\"\n",
    "    # Take first batch for analysis\n",
    "    weights = mha_weights[0].detach().numpy()  # Shape: [n_heads, seq_len, seq_len]\n",
    "    n_heads = weights.shape[0]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for head_idx in range(min(n_heads, 8)):\n",
    "        ax = axes[head_idx]\n",
    "        im = ax.imshow(weights[head_idx], cmap='Blues', aspect='auto')\n",
    "        ax.set_title(f'Head {head_idx + 1}')\n",
    "        ax.set_xlabel('Key Position')\n",
    "        ax.set_ylabel('Query Position')\n",
    "        plt.colorbar(im, ax=ax, fraction=0.046)\n",
    "    \n",
    "    plt.suptitle('Attention Patterns Across Different Heads', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Analyze attention statistics\n",
    "    print(\"\\nðŸ“Š Attention Statistics per Head:\")\n",
    "    print(\"-\" * 50)\n",
    "    for head_idx in range(n_heads):\n",
    "        head_weights = weights[head_idx]\n",
    "        # Calculate entropy (how distributed the attention is)\n",
    "        entropy = -np.sum(head_weights * np.log(head_weights + 1e-10), axis=-1).mean()\n",
    "        # Calculate max attention (how focused the attention is)\n",
    "        max_attn = head_weights.max(axis=-1).mean()\n",
    "        print(f\"Head {head_idx + 1}: Entropy={entropy:.3f}, Max Attention={max_attn:.3f}\")\n",
    "\n",
    "analyze_head_specialization(mha_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428e2c41",
   "metadata": {},
   "source": [
    "---\n",
    "### Exercise 3: Positional Encoding\n",
    "#### ðŸ“š Concept\n",
    "Since self-attention is permutation invariant, we need to inject positional information. The original Transformer uses sinusoidal positional encoding:\n",
    "\n",
    "$$PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
    "$$PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416a4616",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Sinusoidal Positional Encoding\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, max_seq_len=5000):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Create positional encoding matrix\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        position = torch.arange(0, max_seq_len).unsqueeze(1).float()\n",
    "        \n",
    "        # Create div_term for the sinusoidal pattern\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "                           -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        # Apply sin to even indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        # Apply cos to odd indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # Add batch dimension and register as buffer\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Add positional encoding to input embeddings\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len, :]\n",
    "\n",
    "def visualize_positional_encoding():\n",
    "    \"\"\"\n",
    "    Visualize the sinusoidal positional encoding patterns\n",
    "    \"\"\"\n",
    "    d_model = 512\n",
    "    max_len = 100\n",
    "    \n",
    "    # Create positional encoding\n",
    "    pe_module = PositionalEncoding(d_model, max_len)\n",
    "    pe = pe_module.pe[0, :max_len, :].numpy()\n",
    "    \n",
    "    # Create interactive visualization\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=(\n",
    "            \"Full Positional Encoding Matrix\",\n",
    "            \"Encoding for Different Positions\",\n",
    "            \"Sinusoidal Patterns (First 8 dimensions)\",\n",
    "            \"Frequency Analysis\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # 1. Full matrix heatmap\n",
    "    fig.add_trace(\n",
    "        go.Heatmap(z=pe.T, colorscale='RdBu', showscale=True),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # 2. Encoding for specific positions\n",
    "    positions_to_plot = [0, 10, 25, 50, 99]\n",
    "    for pos in positions_to_plot:\n",
    "        fig.add_trace(\n",
    "            go.Scatter(y=pe[pos, :128], name=f'Pos {pos}', mode='lines'),\n",
    "            row=1, col=2\n",
    "        )\n",
    "    \n",
    "    # 3. Sinusoidal patterns\n",
    "    for dim in range(0, 8, 2):\n",
    "        fig.add_trace(\n",
    "            go.Scatter(y=pe[:, dim], name=f'Dim {dim} (sin)', mode='lines'),\n",
    "            row=2, col=1\n",
    "        )\n",
    "        fig.add_trace(\n",
    "            go.Scatter(y=pe[:, dim+1], name=f'Dim {dim+1} (cos)', \n",
    "                      mode='lines', line=dict(dash='dash')),\n",
    "            row=2, col=1\n",
    "        )\n",
    "    \n",
    "    # 4. Frequency spectrum\n",
    "    # Calculate frequency content\n",
    "    for dim_idx in [0, 64, 128, 256]:\n",
    "        freq_content = np.abs(np.fft.fft(pe[:, dim_idx]))[:50]\n",
    "        fig.add_trace(\n",
    "            go.Scatter(y=freq_content, name=f'Dim {dim_idx}', mode='lines'),\n",
    "            row=2, col=2\n",
    "        )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(height=800, showlegend=True, \n",
    "                     title_text=\"Positional Encoding Analysis\")\n",
    "    fig.update_xaxes(title_text=\"Dimension\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Position\", row=1, col=1)\n",
    "    fig.update_xaxes(title_text=\"Dimension Index\", row=1, col=2)\n",
    "    fig.update_xaxes(title_text=\"Position\", row=2, col=1)\n",
    "    fig.update_xaxes(title_text=\"Frequency\", row=2, col=2)\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    # Also create matplotlib visualization\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    # Heatmap\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.imshow(pe.T[:64, :], aspect='auto', cmap='coolwarm')\n",
    "    plt.colorbar(label='Encoding Value')\n",
    "    plt.xlabel('Position')\n",
    "    plt.ylabel('Dimension')\n",
    "    plt.title('Positional Encoding (First 64 dims)')\n",
    "    \n",
    "    # Line plots for different positions\n",
    "    plt.subplot(2, 3, 2)\n",
    "    for pos in [0, 10, 50, 99]:\n",
    "        plt.plot(pe[pos, :128], label=f'Position {pos}', alpha=0.7)\n",
    "    plt.xlabel('Dimension')\n",
    "    plt.ylabel('Encoding Value')\n",
    "    plt.title('Encoding Values at Different Positions')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Sinusoidal patterns\n",
    "    plt.subplot(2, 3, 3)\n",
    "    plt.plot(pe[:, 0], label='Dim 0 (sin)', color='blue')\n",
    "    plt.plot(pe[:, 1], label='Dim 1 (cos)', color='red')\n",
    "    plt.plot(pe[:, 8], label='Dim 8 (sin)', color='green')\n",
    "    plt.plot(pe[:, 9], label='Dim 9 (cos)', color='orange')\n",
    "    plt.xlabel('Position')\n",
    "    plt.ylabel('Encoding Value')\n",
    "    plt.title('Sinusoidal Patterns')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Distance matrix\n",
    "    plt.subplot(2, 3, 4)\n",
    "    # Calculate euclidean distances between positions\n",
    "    distances = np.zeros((50, 50))\n",
    "    for i in range(50):\n",
    "        for j in range(50):\n",
    "            distances[i, j] = np.linalg.norm(pe[i] - pe[j])\n",
    "    plt.imshow(distances, cmap='viridis', aspect='auto')\n",
    "    plt.colorbar(label='Euclidean Distance')\n",
    "    plt.xlabel('Position')\n",
    "    plt.ylabel('Position')\n",
    "    plt.title('Distance Matrix Between Positions')\n",
    "    \n",
    "    # Relative position encoding\n",
    "    plt.subplot(2, 3, 5)\n",
    "    base_pos = 25\n",
    "    relative_distances = [np.linalg.norm(pe[base_pos] - pe[base_pos + offset]) \n",
    "                         for offset in range(-25, 25)]\n",
    "    plt.plot(range(-25, 25), relative_distances, 'o-')\n",
    "    plt.xlabel('Relative Position')\n",
    "    plt.ylabel('Euclidean Distance')\n",
    "    plt.title(f'Distance from Position {base_pos}')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_positional_encoding()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0c2481",
   "metadata": {},
   "source": [
    "### ðŸŽ¯ Your Turn: Exercise 3\n",
    "Implement **learnable positional embeddings** as an alternative to sinusoidal encoding. \n",
    "Compare with the sinusoidal approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653f3393",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearnablePositionalEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Learnable Positional Embeddings (like BERT)\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, max_seq_len=5000):\n",
    "        super().__init__()\n",
    "        # TODO: Create learnable embedding matrix\n",
    "        # Hint: Use nn.Embedding\n",
    "        self.pos_embedding = None  # Replace with your implementation\n",
    "        \n",
    "        # Solution (uncomment to see):\n",
    "        # self.pos_embedding = nn.Embedding(max_seq_len, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: [batch_size, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        \n",
    "        # TODO: Create position indices and add embeddings\n",
    "        # positions = None  # Your code here\n",
    "        # return None  # Your code here\n",
    "        \n",
    "        # Solution (uncomment to see):\n",
    "        # positions = torch.arange(seq_len, device=x.device).unsqueeze(0)\n",
    "        # return x + self.pos_embedding(positions)\n",
    "        \n",
    "        pass\n",
    "\n",
    "# Test your implementation\n",
    "# lpe = LearnablePositionalEmbedding(512, 1000)\n",
    "# test_input = torch.randn(2, 50, 512)\n",
    "# output = lpe(test_input)\n",
    "# print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c03abbc",
   "metadata": {},
   "source": [
    "## Part 3: Building a Complete Transformer\n",
    "\n",
    "### Exercise 4: Transformer Encoder Block\n",
    "#### ðŸ“š Concept\n",
    "A Transformer encoder block consists of:\n",
    "1. Multi-Head Self-Attention\n",
    "2. Add & Norm\n",
    "3. Feed-Forward Network\n",
    "4. Add & Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe381f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Position-wise Feed-Forward Network\n",
    "    FFN(x) = max(0, xWâ‚ + bâ‚)Wâ‚‚ + bâ‚‚\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, d_ff=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Two linear transformations with ReLU activation\n",
    "        return self.linear2(self.dropout(self.activation(self.linear1(x))))\n",
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Single Transformer Encoder Block\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model=512, n_heads=8, d_ff=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.ffn = FeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        # Multi-Head Attention with residual connection\n",
    "        attn_output, attn_weights = self.mha(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        \n",
    "        # Feed-Forward with residual connection\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm2(x + self.dropout(ffn_output))\n",
    "        \n",
    "        return x, attn_weights\n",
    "\n",
    "# Test encoder block\n",
    "def test_encoder_block():\n",
    "    batch_size, seq_len, d_model = 2, 20, 512\n",
    "    \n",
    "    # Create input\n",
    "    x = torch.randn(batch_size, seq_len, d_model)\n",
    "    \n",
    "    # Create encoder block\n",
    "    encoder = TransformerEncoderBlock(d_model)\n",
    "    \n",
    "    # Forward pass\n",
    "    output, attn_weights = encoder(x)\n",
    "    \n",
    "    print(f\"Input shape: {x.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(f\"Output statistics:\")\n",
    "    print(f\"  Mean: {output.mean().item():.4f}\")\n",
    "    print(f\"  Std: {output.std().item():.4f}\")\n",
    "    print(f\"  Min: {output.min().item():.4f}\")\n",
    "    print(f\"  Max: {output.max().item():.4f}\")\n",
    "    \n",
    "    return output, attn_weights\n",
    "\n",
    "encoder_output, encoder_weights = test_encoder_block()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6752d8d",
   "metadata": {},
   "source": [
    "### Exercise 5: Complete Transformer Encoder\n",
    "#### ðŸ“š Concept\n",
    "Stack multiple encoder blocks to create a deep Transformer encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a429df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete Transformer Encoder with N stacked layers\n",
    "    \"\"\"\n",
    "    def __init__(self, n_layers=6, d_model=512, n_heads=8, d_ff=2048, \n",
    "                 max_seq_len=5000, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_seq_len)\n",
    "        \n",
    "        # Stack of encoder blocks\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderBlock(d_model, n_heads, d_ff, dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        # Add positional encoding\n",
    "        x = self.pos_encoding(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Store attention weights from each layer\n",
    "        attention_weights = []\n",
    "        \n",
    "        # Pass through each encoder layer\n",
    "        for layer in self.layers:\n",
    "            x, attn = layer(x, mask)\n",
    "            attention_weights.append(attn)\n",
    "        \n",
    "        return x, attention_weights\n",
    "\n",
    "# Create and test complete encoder\n",
    "def test_transformer_encoder():\n",
    "    batch_size, seq_len, d_model = 2, 30, 512\n",
    "    n_layers = 6\n",
    "    \n",
    "    # Create input embeddings\n",
    "    x = torch.randn(batch_size, seq_len, d_model)\n",
    "    \n",
    "    # Create encoder\n",
    "    encoder = TransformerEncoder(n_layers=n_layers, d_model=d_model)\n",
    "    \n",
    "    # Forward pass\n",
    "    output, all_attn_weights = encoder(x)\n",
    "    \n",
    "    print(f\"Encoder with {n_layers} layers:\")\n",
    "    print(f\"Input shape: {x.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(f\"Number of attention weight matrices: {len(all_attn_weights)}\")\n",
    "    print(f\"Total parameters: {sum(p.numel() for p in encoder.parameters()):,}\")\n",
    "    \n",
    "    return output, all_attn_weights\n",
    "\n",
    "transformer_output, all_attention_weights = test_transformer_encoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389967ac",
   "metadata": {},
   "source": [
    "### Analyzing Layer-wise Attention Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cacca47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_layerwise_attention(all_attention_weights):\n",
    "    \"\"\"\n",
    "    Analyze how attention patterns change across layers\n",
    "    \"\"\"\n",
    "    n_layers = len(all_attention_weights)\n",
    "    \n",
    "    # Create subplots for first 6 layers\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for layer_idx in range(min(n_layers, 6)):\n",
    "        # Take first batch, first head for visualization\n",
    "        weights = all_attention_weights[layer_idx][0, 0].detach().numpy()\n",
    "        \n",
    "        ax = axes[layer_idx]\n",
    "        im = ax.imshow(weights, cmap='Reds', aspect='auto')\n",
    "        ax.set_title(f'Layer {layer_idx + 1}')\n",
    "        ax.set_xlabel('Key Position')\n",
    "        ax.set_ylabel('Query Position')\n",
    "        plt.colorbar(im, ax=ax, fraction=0.046)\n",
    "    \n",
    "    plt.suptitle('Attention Patterns Across Transformer Layers', \n",
    "                fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Analyze attention entropy across layers\n",
    "    print(\"\\nðŸ“Š Layer-wise Attention Analysis:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    layer_entropies = []\n",
    "    for layer_idx, attn in enumerate(all_attention_weights):\n",
    "        # Calculate average entropy across all heads and positions\n",
    "        weights = attn[0].detach().numpy()  # First batch\n",
    "        entropy = -np.sum(weights * np.log(weights + 1e-10), axis=-1).mean()\n",
    "        layer_entropies.append(entropy)\n",
    "        print(f\"Layer {layer_idx + 1}: Average Entropy = {entropy:.3f}\")\n",
    "    \n",
    "    # Plot entropy trend\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(range(1, len(layer_entropies) + 1), layer_entropies, \n",
    "            'o-', linewidth=2, markersize=8)\n",
    "    plt.xlabel('Layer')\n",
    "    plt.ylabel('Average Attention Entropy')\n",
    "    plt.title('Attention Entropy Across Layers')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "analyze_layerwise_attention(all_attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f8d054",
   "metadata": {},
   "source": [
    "## Part 4: Practical Applications\n",
    "\n",
    "### Exercise 6: Text Classification with Transformers\n",
    "#### ðŸ“š Concept\n",
    "Apply Transformer encoder for sequence classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592f007a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer-based text classifier\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, n_classes, d_model=512, n_layers=6, \n",
    "                 n_heads=8, d_ff=2048, max_seq_len=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Token embeddings\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.scale = math.sqrt(d_model)\n",
    "        \n",
    "        # Transformer encoder\n",
    "        self.encoder = TransformerEncoder(\n",
    "            n_layers, d_model, n_heads, d_ff, max_seq_len, dropout\n",
    "        )\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model // 2, n_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        # Embed tokens\n",
    "        x = self.embedding(x) * self.scale\n",
    "        \n",
    "        # Encode with transformer\n",
    "        encoded, attn_weights = self.encoder(x, mask)\n",
    "        \n",
    "        # Use [CLS] token or mean pooling for classification\n",
    "        # Here we use mean pooling\n",
    "        if mask is not None:\n",
    "            mask_expanded = mask.unsqueeze(-1).expand(encoded.size()).float()\n",
    "            sum_embeddings = torch.sum(encoded * mask_expanded, dim=1)\n",
    "            sum_mask = torch.clamp(mask_expanded.sum(dim=1), min=1e-9)\n",
    "            pooled = sum_embeddings / sum_mask\n",
    "        else:\n",
    "            pooled = encoded.mean(dim=1)\n",
    "        \n",
    "        # Classify\n",
    "        logits = self.classifier(pooled)\n",
    "        \n",
    "        return logits, attn_weights\n",
    "\n",
    "# Create synthetic classification dataset\n",
    "def create_synthetic_dataset():\n",
    "    \"\"\"\n",
    "    Create a synthetic text classification dataset\n",
    "    \"\"\"\n",
    "    vocab_size = 1000\n",
    "    seq_len = 50\n",
    "    n_samples = 100\n",
    "    n_classes = 3\n",
    "    \n",
    "    # Generate random sequences\n",
    "    X = torch.randint(0, vocab_size, (n_samples, seq_len))\n",
    "    # Generate random labels\n",
    "    y = torch.randint(0, n_classes, (n_samples,))\n",
    "    \n",
    "    # Create attention mask (simulate variable length sequences)\n",
    "    mask = torch.ones(n_samples, seq_len)\n",
    "    for i in range(n_samples):\n",
    "        actual_len = torch.randint(20, seq_len, (1,)).item()\n",
    "        mask[i, actual_len:] = 0\n",
    "    \n",
    "    return X, y, mask\n",
    "\n",
    "# Train the classifier\n",
    "def train_classifier():\n",
    "    # Create dataset\n",
    "    X, y, mask = create_synthetic_dataset()\n",
    "    \n",
    "    # Model parameters\n",
    "    vocab_size = 1000\n",
    "    n_classes = 3\n",
    "    d_model = 256  # Smaller model for demo\n",
    "    \n",
    "    # Create model\n",
    "    model = TransformerClassifier(\n",
    "        vocab_size, n_classes, d_model=d_model, \n",
    "        n_layers=3, n_heads=4\n",
    "    )\n",
    "    \n",
    "    # Training setup\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Training loop (simplified)\n",
    "    model.train()\n",
    "    n_epochs = 10\n",
    "    batch_size = 16\n",
    "    \n",
    "    print(\"Training Transformer Classifier...\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    losses = []\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss = 0\n",
    "        n_batches = len(X) // batch_size\n",
    "        \n",
    "        for i in range(n_batches):\n",
    "            # Get batch\n",
    "            batch_X = X[i*batch_size:(i+1)*batch_size]\n",
    "            batch_y = y[i*batch_size:(i+1)*batch_size]\n",
    "            batch_mask = mask[i*batch_size:(i+1)*batch_size]\n",
    "            \n",
    "            # Forward pass\n",
    "            logits, _ = model(batch_X, batch_mask)\n",
    "            loss = criterion(logits, batch_y)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_loss = epoch_loss / n_batches\n",
    "        losses.append(avg_loss)\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs}, Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Plot training curve\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(losses, 'o-', linewidth=2, markersize=6)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss Curve')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    return model, losses\n",
    "\n",
    "trained_model, training_losses = train_classifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa393b6",
   "metadata": {},
   "source": [
    "### Exercise 7: Sequence-to-Sequence with Transformers\n",
    "#### ðŸ“š Concept\n",
    "Implement a simple sequence-to-sequence model using Transformer architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0740ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerSeq2Seq(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple Transformer for Sequence-to-Sequence tasks\n",
    "    (Encoder-only for simplicity)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_vocab_size, output_vocab_size, \n",
    "                 d_model=512, n_layers=6, n_heads=8, d_ff=2048, \n",
    "                 max_seq_len=100, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embeddings\n",
    "        self.input_embedding = nn.Embedding(input_vocab_size, d_model)\n",
    "        self.output_embedding = nn.Embedding(output_vocab_size, d_model)\n",
    "        self.scale = math.sqrt(d_model)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_seq_len)\n",
    "        \n",
    "        # Transformer encoder\n",
    "        self.encoder = TransformerEncoder(\n",
    "            n_layers, d_model, n_heads, d_ff, max_seq_len, dropout\n",
    "        )\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_projection = nn.Linear(d_model, output_vocab_size)\n",
    "        \n",
    "    def forward(self, src, tgt=None):\n",
    "        # Encode source\n",
    "        src_emb = self.input_embedding(src) * self.scale\n",
    "        src_emb = self.pos_encoding(src_emb)\n",
    "        \n",
    "        # Get encoder output\n",
    "        encoder_output, _ = self.encoder(src_emb)\n",
    "        \n",
    "        # Project to output vocabulary\n",
    "        output = self.output_projection(encoder_output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Demonstrate sequence generation\n",
    "def demonstrate_seq2seq():\n",
    "    \"\"\"\n",
    "    Simple demonstration of sequence-to-sequence translation\n",
    "    \"\"\"\n",
    "    # Model parameters\n",
    "    input_vocab_size = 100\n",
    "    output_vocab_size = 100\n",
    "    seq_len = 20\n",
    "    d_model = 256\n",
    "    \n",
    "    # Create model\n",
    "    model = TransformerSeq2Seq(\n",
    "        input_vocab_size, output_vocab_size,\n",
    "        d_model=d_model, n_layers=2, n_heads=4\n",
    "    )\n",
    "    \n",
    "    # Create sample input\n",
    "    batch_size = 2\n",
    "    src = torch.randint(0, input_vocab_size, (batch_size, seq_len))\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(src)\n",
    "    \n",
    "    print(f\"Sequence-to-Sequence Model:\")\n",
    "    print(f\"Input shape: {src.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(f\"Output vocab size: {output_vocab_size}\")\n",
    "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    # Get predictions\n",
    "    predictions = output.argmax(dim=-1)\n",
    "    print(f\"\\nSample predictions shape: {predictions.shape}\")\n",
    "    \n",
    "    return model, output\n",
    "\n",
    "seq2seq_model, seq2seq_output = demonstrate_seq2seq()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18e9c42",
   "metadata": {},
   "source": [
    "### ðŸŽ¯ Your Turn: Exercise 7\n",
    "Implement beam search for better sequence generation. Currently, we use greedy decoding (argmax).\n",
    "Implement beam search with beam_size=3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5659a498",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search(model, src, beam_size=3, max_length=50):\n",
    "    \"\"\"\n",
    "    Implement beam search for sequence generation\n",
    "    TODO: Complete this implementation\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    # Hints:\n",
    "    # 1. Maintain beam_size hypotheses\n",
    "    # 2. At each step, expand each hypothesis\n",
    "    # 3. Keep top beam_size candidates based on cumulative probability\n",
    "    # 4. Stop when EOS token or max_length reached\n",
    "    \n",
    "    # Placeholder return\n",
    "    return None\n",
    "\n",
    "# Test your implementation\n",
    "# src_sequence = torch.randint(0, 100, (1, 20))\n",
    "# best_sequence = beam_search(seq2seq_model, src_sequence, beam_size=3)\n",
    "# print(f\"Best sequence: {best_sequence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9c9724",
   "metadata": {},
   "source": [
    "## Part 5: Advanced Topics\n",
    "\n",
    "### Exercise 8: Attention Visualization and Interpretability\n",
    "#### ðŸ“š Concept\n",
    "Visualize what the model is \"looking at\" when making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468bcf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_attention_visualization(text_tokens, attention_weights):\n",
    "    \"\"\"\n",
    "    Create an interactive attention visualization\n",
    "    \"\"\"\n",
    "    # Convert attention weights to numpy if needed\n",
    "    if torch.is_tensor(attention_weights):\n",
    "        attention_weights = attention_weights.detach().cpu().numpy()\n",
    "    \n",
    "    # Take first head for visualization\n",
    "    if len(attention_weights.shape) > 2:\n",
    "        attention_weights = attention_weights[0]  # First head\n",
    "    \n",
    "    # Create interactive heatmap with Plotly\n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=attention_weights,\n",
    "        x=text_tokens,\n",
    "        y=text_tokens,\n",
    "        colorscale='Reds',\n",
    "        text=np.round(attention_weights, 3),\n",
    "        texttemplate=\"%{text}\",\n",
    "        textfont={\"size\": 8},\n",
    "        showscale=True,\n",
    "        hoverongaps=False,\n",
    "        hovertemplate=\"From: %{y}<br>To: %{x}<br>Weight: %{z:.3f}<extra></extra>\"\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=\"Attention Weight Visualization\",\n",
    "        xaxis_title=\"Keys (To)\",\n",
    "        yaxis_title=\"Queries (From)\",\n",
    "        height=600,\n",
    "        width=700,\n",
    "        xaxis={'side': 'bottom'},\n",
    "        yaxis={'autorange': 'reversed'}\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    # Also create a flow diagram showing top attention connections\n",
    "    threshold = 0.1  # Only show connections above this threshold\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Create node positions in a circle\n",
    "    n_tokens = len(text_tokens)\n",
    "    angles = np.linspace(0, 2*np.pi, n_tokens, endpoint=False)\n",
    "    x_pos = np.cos(angles)\n",
    "    y_pos = np.sin(angles)\n",
    "    \n",
    "    # Draw tokens as nodes\n",
    "    for i, (x, y, token) in enumerate(zip(x_pos, y_pos, text_tokens)):\n",
    "        plt.scatter(x, y, s=1000, c='lightblue', edgecolors='black', linewidth=2, zorder=3)\n",
    "        plt.text(x, y, token, ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # Draw attention connections as edges\n",
    "    for i in range(n_tokens):\n",
    "        for j in range(n_tokens):\n",
    "            if attention_weights[i, j] > threshold:\n",
    "                # Draw arrow from i to j with weight as thickness\n",
    "                plt.arrow(x_pos[i], y_pos[i], \n",
    "                         0.8*(x_pos[j] - x_pos[i]), 0.8*(y_pos[j] - y_pos[i]),\n",
    "                         head_width=0.05, head_length=0.05,\n",
    "                         alpha=attention_weights[i, j],\n",
    "                         color='red', zorder=1,\n",
    "                         length_includes_head=True,\n",
    "                         width=attention_weights[i, j] * 0.02)\n",
    "    \n",
    "    plt.title(\"Attention Flow Diagram (Threshold > 0.1)\", fontsize=14, fontweight='bold')\n",
    "    plt.axis('equal')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage with sample tokens\n",
    "sample_tokens = [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\", \".\", \"<PAD>\"]\n",
    "sample_attention = torch.softmax(torch.randn(8, 8) * 2, dim=-1)\n",
    "\n",
    "create_attention_visualization(sample_tokens, sample_attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b2c04c",
   "metadata": {},
   "source": [
    "### Exercise 9: Efficient Attention Mechanisms\n",
    "#### ðŸ“š Concept\n",
    "Explore efficient attention variants to handle longer sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11e5098",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Linear Attention - O(n) complexity instead of O(nÂ²)\n",
    "    Using kernel trick to approximate attention\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        batch_size, seq_len, _ = q.size()\n",
    "        \n",
    "        # Linear projections\n",
    "        Q = self.W_q(q).view(batch_size, seq_len, self.n_heads, self.d_k)\n",
    "        K = self.W_k(k).view(batch_size, seq_len, self.n_heads, self.d_k)\n",
    "        V = self.W_v(v).view(batch_size, seq_len, self.n_heads, self.d_k)\n",
    "        \n",
    "        # Apply kernel feature map (simplified - using ELU + 1)\n",
    "        Q = F.elu(Q) + 1\n",
    "        K = F.elu(K) + 1\n",
    "        \n",
    "        # Compute attention in linear time\n",
    "        # Instead of (QK^T)V, compute Q(K^TV)\n",
    "        K_T_V = torch.einsum('bshd,bshf->bhdf', K, V)\n",
    "        QK_T_V = torch.einsum('bshd,bhdf->bshf', Q, K_T_V)\n",
    "        \n",
    "        # Normalization\n",
    "        K_sum = K.sum(dim=1, keepdim=True)\n",
    "        Q_K_sum = torch.einsum('bshd,bhd->bsh', Q, K_sum.squeeze(1))\n",
    "        output = QK_T_V / (Q_K_sum.unsqueeze(-1) + 1e-6)\n",
    "        \n",
    "        # Reshape and project\n",
    "        output = output.reshape(batch_size, seq_len, self.d_model)\n",
    "        output = self.W_o(output)\n",
    "        \n",
    "        return output, None  # No attention weights in linear attention\n",
    "\n",
    "def compare_attention_mechanisms():\n",
    "    \"\"\"\n",
    "    Compare standard attention vs linear attention\n",
    "    \"\"\"\n",
    "    import time\n",
    "    \n",
    "    d_model = 256\n",
    "    n_heads = 4\n",
    "    \n",
    "    # Test different sequence lengths\n",
    "    seq_lengths = [50, 100, 200, 500, 1000]\n",
    "    \n",
    "    standard_times = []\n",
    "    linear_times = []\n",
    "    \n",
    "    print(\"Comparing Attention Mechanisms:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for seq_len in seq_lengths:\n",
    "        batch_size = 4\n",
    "        x = torch.randn(batch_size, seq_len, d_model)\n",
    "        \n",
    "        # Standard attention\n",
    "        standard_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        start = time.time()\n",
    "        with torch.no_grad():\n",
    "            _, _ = standard_attn(x, x, x)\n",
    "        standard_time = time.time() - start\n",
    "        standard_times.append(standard_time)\n",
    "        \n",
    "        # Linear attention\n",
    "        linear_attn = LinearAttention(d_model, n_heads)\n",
    "        start = time.time()\n",
    "        with torch.no_grad():\n",
    "            _, _ = linear_attn(x, x, x)\n",
    "        linear_time = time.time() - start\n",
    "        linear_times.append(linear_time)\n",
    "        \n",
    "        print(f\"Seq Length {seq_len:4d}: Standard={standard_time:.4f}s, \"\n",
    "              f\"Linear={linear_time:.4f}s, Speedup={standard_time/linear_time:.2f}x\")\n",
    "    \n",
    "    # Plot comparison\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(seq_lengths, standard_times, 'o-', label='Standard Attention', linewidth=2)\n",
    "    plt.plot(seq_lengths, linear_times, 's-', label='Linear Attention', linewidth=2)\n",
    "    plt.xlabel('Sequence Length')\n",
    "    plt.ylabel('Time (seconds)')\n",
    "    plt.title('Computation Time Comparison')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    speedup = [s/l for s, l in zip(standard_times, linear_times)]\n",
    "    plt.bar(range(len(seq_lengths)), speedup, color='green', alpha=0.7)\n",
    "    plt.xticks(range(len(seq_lengths)), seq_lengths)\n",
    "    plt.xlabel('Sequence Length')\n",
    "    plt.ylabel('Speedup Factor')\n",
    "    plt.title('Linear Attention Speedup')\n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "compare_attention_mechanisms()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e794a91",
   "metadata": {},
   "source": [
    "### Exercise 10: Training Dynamics Analysis\n",
    "#### ðŸ“š Concept\n",
    "Analyze and visualize the training dynamics of Transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98b7e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingMonitor:\n",
    "    \"\"\"\n",
    "    Monitor and visualize transformer training dynamics\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.metrics = {\n",
    "            'loss': [],\n",
    "            'gradient_norm': [],\n",
    "            'learning_rate': [],\n",
    "            'attention_entropy': []\n",
    "        }\n",
    "        \n",
    "    def update(self, loss, model, lr, attention_weights=None):\n",
    "        # Record loss\n",
    "        self.metrics['loss'].append(loss)\n",
    "        \n",
    "        # Calculate gradient norm\n",
    "        total_norm = 0\n",
    "        for p in model.parameters():\n",
    "            if p.grad is not None:\n",
    "                param_norm = p.grad.data.norm(2)\n",
    "                total_norm += param_norm.item() ** 2\n",
    "        total_norm = total_norm ** 0.5\n",
    "        self.metrics['gradient_norm'].append(total_norm)\n",
    "        \n",
    "        # Record learning rate\n",
    "        self.metrics['learning_rate'].append(lr)\n",
    "        \n",
    "        # Calculate attention entropy if provided\n",
    "        if attention_weights is not None:\n",
    "            # Calculate entropy of attention distribution\n",
    "            weights = attention_weights.detach().cpu().numpy()\n",
    "            entropy = -np.sum(weights * np.log(weights + 1e-10), axis=-1).mean()\n",
    "            self.metrics['attention_entropy'].append(entropy)\n",
    "        \n",
    "    def plot_metrics(self):\n",
    "        \"\"\"\n",
    "        Create comprehensive training visualization\n",
    "        \"\"\"\n",
    "        fig = make_subplots(\n",
    "            rows=2, cols=2,\n",
    "            subplot_titles=('Training Loss', 'Gradient Norm', \n",
    "                          'Learning Rate Schedule', 'Attention Entropy')\n",
    "        )\n",
    "        \n",
    "        # Training loss\n",
    "        fig.add_trace(\n",
    "            go.Scatter(y=self.metrics['loss'], mode='lines', name='Loss'),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # Gradient norm\n",
    "        fig.add_trace(\n",
    "            go.Scatter(y=self.metrics['gradient_norm'], mode='lines', \n",
    "                      name='Grad Norm', line=dict(color='red')),\n",
    "            row=1, col=2\n",
    "        )\n",
    "        \n",
    "        # Learning rate\n",
    "        fig.add_trace(\n",
    "            go.Scatter(y=self.metrics['learning_rate'], mode='lines', \n",
    "                      name='LR', line=dict(color='green')),\n",
    "            row=2, col=1\n",
    "        )\n",
    "        \n",
    "        # Attention entropy\n",
    "        if self.metrics['attention_entropy']:\n",
    "            fig.add_trace(\n",
    "                go.Scatter(y=self.metrics['attention_entropy'], mode='lines', \n",
    "                          name='Entropy', line=dict(color='purple')),\n",
    "                row=2, col=2\n",
    "            )\n",
    "        \n",
    "        fig.update_layout(height=600, showlegend=False, \n",
    "                         title_text=\"Training Dynamics Analysis\")\n",
    "        fig.update_xaxes(title_text=\"Step\")\n",
    "        fig.update_yaxes(title_text=\"Value\")\n",
    "        \n",
    "        fig.show()\n",
    "\n",
    "# Simulate training with monitoring\n",
    "def simulate_training_with_monitoring():\n",
    "    \"\"\"\n",
    "    Demonstrate training monitoring\n",
    "    \"\"\"\n",
    "    # Create simple model\n",
    "    model = TransformerEncoder(n_layers=2, d_model=128, n_heads=4)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    monitor = TrainingMonitor()\n",
    "    \n",
    "    # Simulate training steps\n",
    "    print(\"Simulating training with monitoring...\")\n",
    "    for step in range(50):\n",
    "        # Fake data\n",
    "        x = torch.randn(4, 20, 128)\n",
    "        \n",
    "        # Forward pass\n",
    "        output, attn_weights = model(x)\n",
    "        loss = output.mean()  # Dummy loss\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update monitor\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        monitor.update(\n",
    "            loss.item(), model, current_lr, \n",
    "            attn_weights[0] if attn_weights else None\n",
    "        )\n",
    "        \n",
    "        if (step + 1) % 10 == 0:\n",
    "            print(f\"Step {step+1}: Loss={loss.item():.4f}\")\n",
    "    \n",
    "    # Plot training dynamics\n",
    "    monitor.plot_metrics()\n",
    "    \n",
    "    return monitor\n",
    "\n",
    "training_monitor = simulate_training_with_monitoring()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd346fa",
   "metadata": {},
   "source": [
    "### ðŸŽ¯ Your Turn: Exercise 10\n",
    "Implement learning rate warmup schedule as used in the original Transformer paper.\n",
    "The formula is: `lr = d_model^(-0.5) * min(step^(-0.5), step * warmup_steps^(-1.5))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9902abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WarmupScheduler:\n",
    "    \"\"\"\n",
    "    Learning rate scheduler with warmup\n",
    "    TODO: Implement the warmup schedule\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "        \n",
    "    def get_lr(self, step):\n",
    "        \"\"\"\n",
    "        Calculate learning rate for given step\n",
    "        \"\"\"\n",
    "        # TODO: Implement the formula\n",
    "        # lr = d_model^(-0.5) * min(step^(-0.5), step * warmup_steps^(-1.5))\n",
    "        \n",
    "        # Your code here\n",
    "        lr = 0.001  # Replace with proper implementation\n",
    "        \n",
    "        # Solution (uncomment to see):\n",
    "        # arg1 = step ** (-0.5)\n",
    "        # arg2 = step * (self.warmup_steps ** (-1.5))\n",
    "        # lr = (self.d_model ** (-0.5)) * min(arg1, arg2)\n",
    "        \n",
    "        return lr\n",
    "\n",
    "# Test and visualize the schedule\n",
    "def visualize_lr_schedule():\n",
    "    scheduler = WarmupScheduler(d_model=512, warmup_steps=4000)\n",
    "    \n",
    "    steps = np.arange(1, 10000)\n",
    "    lrs = [scheduler.get_lr(step) for step in steps]\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(steps, lrs, linewidth=2)\n",
    "    plt.axvline(x=4000, color='red', linestyle='--', label='Warmup ends')\n",
    "    plt.xlabel('Training Step')\n",
    "    plt.ylabel('Learning Rate')\n",
    "    plt.title('Learning Rate Schedule with Warmup')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# visualize_lr_schedule()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6749cdd",
   "metadata": {},
   "source": [
    "## Part 6: Summary and Final Exercises\n",
    "\n",
    "### ðŸŽ“ Key Takeaways\n",
    "\n",
    "1. **Self-Attention**: Enables parallel processing and captures global dependencies\n",
    "2. **Multi-Head Attention**: Learns different types of relationships simultaneously\n",
    "3. **Positional Encoding**: Injects position information into the model\n",
    "4. **Architecture**: Encoder-decoder structure with residual connections and normalization\n",
    "5. **Applications**: Versatile architecture for NLP, vision, and multimodal tasks\n",
    "\n",
    "### ðŸ“Š Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d96297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive comparison chart\n",
    "def create_architecture_comparison():\n",
    "    \"\"\"\n",
    "    Compare Transformer with RNN/LSTM/GRU\n",
    "    \"\"\"\n",
    "    architectures = ['RNN', 'LSTM', 'GRU', 'Transformer']\n",
    "    \n",
    "    # Metrics (relative scores)\n",
    "    metrics = {\n",
    "        'Parallelization': [1, 1, 1, 10],\n",
    "        'Long-range Dependencies': [2, 5, 5, 9],\n",
    "        'Training Speed': [3, 2, 3, 8],\n",
    "        'Memory Efficiency': [8, 6, 7, 4],\n",
    "        'Parameter Efficiency': [9, 5, 6, 7]\n",
    "    }\n",
    "    \n",
    "    # Create radar chart\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    for arch in architectures:\n",
    "        values = [metrics[metric][architectures.index(arch)] \n",
    "                 for metric in metrics.keys()]\n",
    "        values.append(values[0])  # Close the polygon\n",
    "        \n",
    "        fig.add_trace(go.Scatterpolar(\n",
    "            r=values,\n",
    "            theta=list(metrics.keys()) + [list(metrics.keys())[0]],\n",
    "            fill='toself',\n",
    "            name=arch\n",
    "        ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        polar=dict(\n",
    "            radialaxis=dict(\n",
    "                visible=True,\n",
    "                range=[0, 10]\n",
    "            )),\n",
    "        showlegend=True,\n",
    "        title=\"Architecture Comparison: RNN vs Transformer\",\n",
    "        height=500\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    # Also create bar chart comparison\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.2\n",
    "    \n",
    "    for i, arch in enumerate(architectures):\n",
    "        values = [metrics[metric][i] for metric in metrics.keys()]\n",
    "        plt.bar(x + i*width, values, width, label=arch)\n",
    "    \n",
    "    plt.xlabel('Metrics')\n",
    "    plt.ylabel('Score (1-10)')\n",
    "    plt.title('Architecture Performance Comparison')\n",
    "    plt.xticks(x + width*1.5, list(metrics.keys()), rotation=45)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "create_architecture_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360adcac",
   "metadata": {},
   "source": [
    "### ðŸš€ Next Steps and Resources\n",
    "\n",
    "#### Recommended Implementations to Try:\n",
    "1. **BERT-style Pretraining**: Implement masked language modeling\n",
    "2. **GPT-style Generation**: Build an autoregressive language model\n",
    "3. **Vision Transformer**: Apply Transformers to image patches\n",
    "4. **Cross-modal Attention**: Combine text and image understanding\n",
    "\n",
    "#### Key Papers to Read:\n",
    "- **\"Attention Is All You Need\"** (Vaswani et al., 2017) - Original Transformer\n",
    "- **\"BERT: Pre-training of Deep Bidirectional Transformers\"** (Devlin et al., 2018)\n",
    "- **\"Language Models are Few-Shot Learners\"** (Brown et al., 2020) - GPT-3\n",
    "- **\"An Image is Worth 16x16 Words\"** (Dosovitskiy et al., 2020) - Vision Transformer\n",
    "\n",
    "#### Useful Libraries:\n",
    "- **Hugging Face Transformers**: Production-ready implementations\n",
    "- **PyTorch**: Flexible deep learning framework\n",
    "- **Einops**: Elegant tensor operations\n",
    "- **Weights & Biases**: Experiment tracking\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“ Final Challenge Exercise\n",
    "\n",
    "Implement a mini-BERT model for masked language modeling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d271f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Challenge: Mini-BERT Implementation\n",
    "class MiniBERT(nn.Module):\n",
    "    \"\"\"\n",
    "    Simplified BERT for Masked Language Modeling\n",
    "    Your task: Complete the implementation\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d_model=256, n_layers=4, n_heads=4, \n",
    "                 max_seq_len=512, mask_token_id=103):\n",
    "        super().__init__()\n",
    "        self.mask_token_id = mask_token_id\n",
    "        \n",
    "        # TODO: Add components\n",
    "        # 1. Token embeddings\n",
    "        # 2. Segment embeddings (for sentence A/B)\n",
    "        # 3. Positional encoding\n",
    "        # 4. Transformer encoder\n",
    "        # 5. MLM head (prediction layer)\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def forward(self, input_ids, mask_positions):\n",
    "        \"\"\"\n",
    "        Forward pass for masked language modeling\n",
    "        \n",
    "        Args:\n",
    "            input_ids: Token IDs with [MASK] tokens\n",
    "            mask_positions: Positions of masked tokens\n",
    "        \n",
    "        Returns:\n",
    "            predictions: Vocabulary predictions for masked positions\n",
    "        \"\"\"\n",
    "        # TODO: Implement forward pass\n",
    "        pass\n",
    "    \n",
    "    def create_mlm_data(self, text_ids, mask_prob=0.15):\n",
    "        \"\"\"\n",
    "        Create masked language modeling training data\n",
    "        \"\"\"\n",
    "        # TODO: Randomly mask tokens\n",
    "        pass\n",
    "\n",
    "# Placeholder for testing\n",
    "print(\"Challenge: Implement MiniBERT for masked language modeling!\")\n",
    "print(\"This combines all concepts learned in this notebook.\")\n",
    "print(\"Good luck! ðŸš€\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3923206",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ‰ Congratulations!\n",
    "\n",
    "You've completed the comprehensive Transformer Architecture notebook! You've learned:\n",
    "\n",
    "âœ… Self-attention mechanism and its implementation  \n",
    "âœ… Multi-head attention for learning diverse patterns  \n",
    "âœ… Positional encoding techniques  \n",
    "âœ… Complete Transformer architecture  \n",
    "âœ… Practical applications and optimizations  \n",
    "âœ… Advanced topics like efficient attention and training dynamics  \n",
    "\n",
    "### ðŸ“š Additional Exercises:\n",
    "1. Experiment with different positional encoding methods\n",
    "2. Implement cross-attention for encoder-decoder models\n",
    "3. Try different attention patterns (local, sparse, etc.)\n",
    "4. Build a small-scale language model\n",
    "5. Apply Transformers to your own dataset\n",
    "\n",
    "### ðŸ¤ Connect and Share:\n",
    "Share your implementations and insights with the community!\n",
    "\n",
    "---\n",
    "**Remember**: The best way to understand Transformers is to implement them from scratch. Keep experimenting and building! ðŸ’ª"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
