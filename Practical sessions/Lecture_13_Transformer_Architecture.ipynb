{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b507d29b",
   "metadata": {},
   "source": [
    "# 🤖 Transformer Architecture: From Theory to Practice\n",
    "## Interactive Learning Notebook Based on Lecture 13\n",
    "\n",
    "This notebook provides hands-on implementation of Transformer architecture concepts, including:\n",
    "- Self-Attention Mechanism\n",
    "- Multi-Head Attention\n",
    "- Positional Encoding\n",
    "- Complete Transformer Implementation\n",
    "- Practical Applications\n",
    "\n",
    "**Author**: Ho-min Park  \n",
    "**Contact**: homin.park@ghent.ac.kr\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74cee94",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Environment Configuration\n",
    "### 1.1 Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc7f754",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# For text processing\n",
    "from collections import Counter\n",
    "import re\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Setup complete! ✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5540d690",
   "metadata": {},
   "source": [
    "## Part 2: Core Concepts - Understanding Attention Mechanism\n",
    "\n",
    "### Exercise 1: Implementing Scaled Dot-Product Attention\n",
    "#### 📚 Concept\n",
    "Self-attention allows the model to look at other positions in the input sequence when encoding a current position. The key formula is:\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "Where:\n",
    "- **Q (Query)**: What information am I looking for?\n",
    "- **K (Key)**: What information do I contain?\n",
    "- **V (Value)**: The actual information I store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16c4359",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Scaled Dot-Product Attention implementation\n",
    "    As described in 'Attention Is All You Need' (Vaswani et al., 2017)\n",
    "    \"\"\"\n",
    "    def __init__(self, temperature=1.0, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        # q, k, v: [batch_size, n_heads, seq_len, d_k]\n",
    "        batch_size, n_heads, len_q, d_k = q.size()\n",
    "        \n",
    "        # Calculate attention scores\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / (d_k ** 0.5)\n",
    "        \n",
    "        # Apply mask if provided (for padding or causal masking)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # Apply softmax to get attention weights\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        # Apply attention weights to values\n",
    "        output = torch.matmul(attention_weights, v)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# Test the implementation\n",
    "def test_scaled_attention():\n",
    "    batch_size, n_heads, seq_len, d_k = 2, 8, 10, 64\n",
    "    \n",
    "    # Create random Q, K, V matrices\n",
    "    q = torch.randn(batch_size, n_heads, seq_len, d_k)\n",
    "    k = torch.randn(batch_size, n_heads, seq_len, d_k)\n",
    "    v = torch.randn(batch_size, n_heads, seq_len, d_k)\n",
    "    \n",
    "    # Create attention module\n",
    "    attention = ScaledDotProductAttention()\n",
    "    \n",
    "    # Forward pass\n",
    "    output, weights = attention(q, k, v)\n",
    "    \n",
    "    print(f\"Input shapes: Q={q.shape}, K={k.shape}, V={v.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(f\"Attention weights shape: {weights.shape}\")\n",
    "    print(f\"Attention weights sum per position: {weights[0, 0, 0].sum().item():.4f}\")\n",
    "    \n",
    "    return output, weights\n",
    "\n",
    "output, attention_weights = test_scaled_attention()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d3ed3d",
   "metadata": {},
   "source": [
    "### Visualization: Attention Weight Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d275e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention_weights(attention_weights, title=\"Attention Weight Patterns\"):\n",
    "    \"\"\"\n",
    "    Visualize attention weight patterns using both matplotlib and plotly\n",
    "    \"\"\"\n",
    "    # Take first batch, first head for visualization\n",
    "    weights = attention_weights[0, 0].detach().numpy()\n",
    "    \n",
    "    # Create subplots\n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=2,\n",
    "        subplot_titles=(\"Heatmap View\", \"3D Surface View\"),\n",
    "        specs=[[{\"type\": \"heatmap\"}, {\"type\": \"surface\"}]]\n",
    "    )\n",
    "    \n",
    "    # Heatmap\n",
    "    fig.add_trace(\n",
    "        go.Heatmap(\n",
    "            z=weights,\n",
    "            colorscale='Viridis',\n",
    "            showscale=True,\n",
    "            colorbar=dict(x=0.45)\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # 3D Surface\n",
    "    fig.add_trace(\n",
    "        go.Surface(\n",
    "            z=weights,\n",
    "            colorscale='Viridis',\n",
    "            showscale=False\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title_text=title,\n",
    "        height=400,\n",
    "        showlegend=False\n",
    "    )\n",
    "    \n",
    "    # Update axes labels\n",
    "    fig.update_xaxes(title_text=\"Key Position\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Query Position\", row=1, col=1)\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    # Also create matplotlib version for static view\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.heatmap(weights, cmap='YlOrRd', cbar_kws={'label': 'Attention Weight'})\n",
    "    plt.title('Attention Pattern Heatmap')\n",
    "    plt.xlabel('Key Position')\n",
    "    plt.ylabel('Query Position')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    # Show attention distribution for selected positions\n",
    "    positions_to_show = [0, len(weights)//2, len(weights)-1]\n",
    "    for pos in positions_to_show:\n",
    "        plt.plot(weights[pos], label=f'Query pos {pos}', marker='o', markersize=4)\n",
    "    plt.xlabel('Key Position')\n",
    "    plt.ylabel('Attention Weight')\n",
    "    plt.title('Attention Distribution for Selected Positions')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_attention_weights(attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadaf28f",
   "metadata": {},
   "source": [
    "### 🎯 Your Turn: Exercise 1\n",
    "Modify the attention mechanism to implement **masked self-attention** for causal language modeling. \n",
    "The mask should prevent positions from attending to subsequent positions (look-ahead mask)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc72a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement causal masking\n",
    "def create_causal_mask(seq_len):\n",
    "    \"\"\"\n",
    "    Create a causal mask to prevent attending to future positions\n",
    "    Hint: Use torch.triu to create upper triangular matrix\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    mask = None  # Replace with your implementation\n",
    "    \n",
    "    # Solution (uncomment to see):\n",
    "    # mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
    "    # mask = ~mask  # Invert: True where attention is allowed\n",
    "    \n",
    "    return mask\n",
    "\n",
    "# Test your implementation\n",
    "# seq_len = 5\n",
    "# mask = create_causal_mask(seq_len)\n",
    "# print(\"Causal mask (True = can attend):\")\n",
    "# print(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea546d6",
   "metadata": {},
   "source": [
    "---\n",
    "### Exercise 2: Multi-Head Attention Implementation\n",
    "#### 📚 Concept\n",
    "Multi-head attention allows the model to jointly attend to information from different representation subspaces. Instead of performing a single attention function, we linearly project the queries, keys and values h times with different projections.\n",
    "\n",
    "**Key insight**: Different heads learn different types of relationships (syntax, semantics, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6892b644",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Attention module\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model=512, n_heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        \n",
    "        # Linear projections for Q, K, V\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.attention = ScaledDotProductAttention(dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        batch_size, seq_len, _ = q.size()\n",
    "        \n",
    "        # Store residual for skip connection\n",
    "        residual = q\n",
    "        \n",
    "        # Linear projections and reshape for multi-head\n",
    "        Q = self.W_q(q).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(k).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(v).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Apply attention\n",
    "        attn_output, attn_weights = self.attention(Q, K, V, mask)\n",
    "        \n",
    "        # Concatenate heads\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(\n",
    "            batch_size, seq_len, self.d_model\n",
    "        )\n",
    "        \n",
    "        # Final linear projection\n",
    "        output = self.W_o(attn_output)\n",
    "        output = self.dropout(output)\n",
    "        \n",
    "        # Add residual and normalize\n",
    "        output = self.layer_norm(output + residual)\n",
    "        \n",
    "        return output, attn_weights\n",
    "\n",
    "# Test Multi-Head Attention\n",
    "def test_multihead_attention():\n",
    "    batch_size, seq_len, d_model = 2, 10, 512\n",
    "    n_heads = 8\n",
    "    \n",
    "    # Create random input\n",
    "    x = torch.randn(batch_size, seq_len, d_model)\n",
    "    \n",
    "    # Create MHA module\n",
    "    mha = MultiHeadAttention(d_model, n_heads)\n",
    "    \n",
    "    # Forward pass (self-attention: q=k=v=x)\n",
    "    output, attn_weights = mha(x, x, x)\n",
    "    \n",
    "    print(f\"Input shape: {x.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(f\"Attention weights shape: {attn_weights.shape}\")\n",
    "    print(f\"Number of parameters: {sum(p.numel() for p in mha.parameters()):,}\")\n",
    "    \n",
    "    return output, attn_weights\n",
    "\n",
    "mha_output, mha_weights = test_multihead_attention()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7faffffa",
   "metadata": {},
   "source": [
    "### Analyzing Multi-Head Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32299119",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_head_specialization(mha_weights):\n",
    "    \"\"\"\n",
    "    Analyze what different attention heads focus on\n",
    "    \"\"\"\n",
    "    # Take first batch for analysis\n",
    "    weights = mha_weights[0].detach().numpy()  # Shape: [n_heads, seq_len, seq_len]\n",
    "    n_heads = weights.shape[0]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for head_idx in range(min(n_heads, 8)):\n",
    "        ax = axes[head_idx]\n",
    "        im = ax.imshow(weights[head_idx], cmap='Blues', aspect='auto')\n",
    "        ax.set_title(f'Head {head_idx + 1}')\n",
    "        ax.set_xlabel('Key Position')\n",
    "        ax.set_ylabel('Query Position')\n",
    "        plt.colorbar(im, ax=ax, fraction=0.046)\n",
    "    \n",
    "    plt.suptitle('Attention Patterns Across Different Heads', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Analyze attention statistics\n",
    "    print(\"\\n📊 Attention Statistics per Head:\")\n",
    "    print(\"-\" * 50)\n",
    "    for head_idx in range(n_heads):\n",
    "        head_weights = weights[head_idx]\n",
    "        # Calculate entropy (how distributed the attention is)\n",
    "        entropy = -np.sum(head_weights * np.log(head_weights + 1e-10), axis=-1).mean()\n",
    "        # Calculate max attention (how focused the attention is)\n",
    "        max_attn = head_weights.max(axis=-1).mean()\n",
    "        print(f\"Head {head_idx + 1}: Entropy={entropy:.3f}, Max Attention={max_attn:.3f}\")\n",
    "\n",
    "analyze_head_specialization(mha_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428e2c41",
   "metadata": {},
   "source": [
    "---\n",
    "### Exercise 3: Positional Encoding\n",
    "#### 📚 Concept\n",
    "Since self-attention is permutation invariant, we need to inject positional information. The original Transformer uses sinusoidal positional encoding:\n",
    "\n",
    "$$PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
    "$$PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416a4616",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Sinusoidal Positional Encoding\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, max_seq_len=5000):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Create positional encoding matrix\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        position = torch.arange(0, max_seq_len).unsqueeze(1).float()\n",
    "        \n",
    "        # Create div_term for the sinusoidal pattern\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "                           -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        # Apply sin to even indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        # Apply cos to odd indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # Add batch dimension and register as buffer\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Add positional encoding to input embeddings\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len, :]\n",
    "\n",
    "def visualize_positional_encoding():\n",
    "    \"\"\"\n",
    "    Visualize the sinusoidal positional encoding patterns\n",
    "    \"\"\"\n",
    "    d_model = 512\n",
    "    max_len = 100\n",
    "    \n",
    "    # Create positional encoding\n",
    "    pe_module = PositionalEncoding(d_model, max_len)\n",
    "    pe = pe_module.pe[0, :max_len, :].numpy()\n",
    "    \n",
    "    # Create interactive visualization\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=(\n",
    "            \"Full Positional Encoding Matrix\",\n",
    "            \"Encoding for Different Positions\",\n",
    "            \"Sinusoidal Patterns (First 8 dimensions)\",\n",
    "            \"Frequency Analysis\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # 1. Full matrix heatmap\n",
    "    fig.add_trace(\n",
    "        go.Heatmap(z=pe.T, colorscale='RdBu', showscale=True),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # 2. Encoding for specific positions\n",
    "    positions_to_plot = [0, 10, 25, 50, 99]\n",
    "    for pos in positions_to_plot:\n",
    "        fig.add_trace(\n",
    "            go.Scatter(y=pe[pos, :128], name=f'Pos {pos}', mode='lines'),\n",
    "            row=1, col=2\n",
    "        )\n",
    "    \n",
    "    # 3. Sinusoidal patterns\n",
    "    for dim in range(0, 8, 2):\n",
    "        fig.add_trace(\n",
    "            go.Scatter(y=pe[:, dim], name=f'Dim {dim} (sin)', mode='lines'),\n",
    "            row=2, col=1\n",
    "        )\n",
    "        fig.add_trace(\n",
    "            go.Scatter(y=pe[:, dim+1], name=f'Dim {dim+1} (cos)', \n",
    "                      mode='lines', line=dict(dash='dash')),\n",
    "            row=2, col=1\n",
    "        )\n",
    "    \n",
    "    # 4. Frequency spectrum\n",
    "    # Calculate frequency content\n",
    "    for dim_idx in [0, 64, 128, 256]:\n",
    "        freq_content = np.abs(np.fft.fft(pe[:, dim_idx]))[:50]\n",
    "        fig.add_trace(\n",
    "            go.Scatter(y=freq_content, name=f'Dim {dim_idx}', mode='lines'),\n",
    "            row=2, col=2\n",
    "        )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(height=800, showlegend=True, \n",
    "                     title_text=\"Positional Encoding Analysis\")\n",
    "    fig.update_xaxes(title_text=\"Dimension\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Position\", row=1, col=1)\n",
    "    fig.update_xaxes(title_text=\"Dimension Index\", row=1, col=2)\n",
    "    fig.update_xaxes(title_text=\"Position\", row=2, col=1)\n",
    "    fig.update_xaxes(title_text=\"Frequency\", row=2, col=2)\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    # Also create matplotlib visualization\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    # Heatmap\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.imshow(pe.T[:64, :], aspect='auto', cmap='coolwarm')\n",
    "    plt.colorbar(label='Encoding Value')\n",
    "    plt.xlabel('Position')\n",
    "    plt.ylabel('Dimension')\n",
    "    plt.title('Positional Encoding (First 64 dims)')\n",
    "    \n",
    "    # Line plots for different positions\n",
    "    plt.subplot(2, 3, 2)\n",
    "    for pos in [0, 10, 50, 99]:\n",
    "        plt.plot(pe[pos, :128], label=f'Position {pos}', alpha=0.7)\n",
    "    plt.xlabel('Dimension')\n",
    "    plt.ylabel('Encoding Value')\n",
    "    plt.title('Encoding Values at Different Positions')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Sinusoidal patterns\n",
    "    plt.subplot(2, 3, 3)\n",
    "    plt.plot(pe[:, 0], label='Dim 0 (sin)', color='blue')\n",
    "    plt.plot(pe[:, 1], label='Dim 1 (cos)', color='red')\n",
    "    plt.plot(pe[:, 8], label='Dim 8 (sin)', color='green')\n",
    "    plt.plot(pe[:, 9], label='Dim 9 (cos)', color='orange')\n",
    "    plt.xlabel('Position')\n",
    "    plt.ylabel('Encoding Value')\n",
    "    plt.title('Sinusoidal Patterns')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Distance matrix\n",
    "    plt.subplot(2, 3, 4)\n",
    "    # Calculate euclidean distances between positions\n",
    "    distances = np.zeros((50, 50))\n",
    "    for i in range(50):\n",
    "        for j in range(50):\n",
    "            distances[i, j] = np.linalg.norm(pe[i] - pe[j])\n",
    "    plt.imshow(distances, cmap='viridis', aspect='auto')\n",
    "    plt.colorbar(label='Euclidean Distance')\n",
    "    plt.xlabel('Position')\n",
    "    plt.ylabel('Position')\n",
    "    plt.title('Distance Matrix Between Positions')\n",
    "    \n",
    "    # Relative position encoding\n",
    "    plt.subplot(2, 3, 5)\n",
    "    base_pos = 25\n",
    "    relative_distances = [np.linalg.norm(pe[base_pos] - pe[base_pos + offset]) \n",
    "                         for offset in range(-25, 25)]\n",
    "    plt.plot(range(-25, 25), relative_distances, 'o-')\n",
    "    plt.xlabel('Relative Position')\n",
    "    plt.ylabel('Euclidean Distance')\n",
    "    plt.title(f'Distance from Position {base_pos}')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_positional_encoding()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0c2481",
   "metadata": {},
   "source": [
    "### 🎯 Your Turn: Exercise 3\n",
    "Implement **learnable positional embeddings** as an alternative to sinusoidal encoding. \n",
    "Compare with the sinusoidal approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653f3393",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearnablePositionalEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Learnable Positional Embeddings (like BERT)\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, max_seq_len=5000):\n",
    "        super().__init__()\n",
    "        # TODO: Create learnable embedding matrix\n",
    "        # Hint: Use nn.Embedding\n",
    "        self.pos_embedding = None  # Replace with your implementation\n",
    "        \n",
    "        # Solution (uncomment to see):\n",
    "        # self.pos_embedding = nn.Embedding(max_seq_len, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: [batch_size, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        \n",
    "        # TODO: Create position indices and add embeddings\n",
    "        # positions = None  # Your code here\n",
    "        # return None  # Your code here\n",
    "        \n",
    "        # Solution (uncomment to see):\n",
    "        # positions = torch.arange(seq_len, device=x.device).unsqueeze(0)\n",
    "        # return x + self.pos_embedding(positions)\n",
    "        \n",
    "        pass\n",
    "\n",
    "# Test your implementation\n",
    "# lpe = LearnablePositionalEmbedding(512, 1000)\n",
    "# test_input = torch.randn(2, 50, 512)\n",
    "# output = lpe(test_input)\n",
    "# print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c03abbc",
   "metadata": {},
   "source": [
    "## Part 3: Building a Complete Transformer\n",
    "\n",
    "### Exercise 4: Transformer Encoder Block\n",
    "#### 📚 Concept\n",
    "A Transformer encoder block consists of:\n",
    "1. Multi-Head Self-Attention\n",
    "2. Add & Norm\n",
    "3. Feed-Forward Network\n",
    "4. Add & Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe381f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Position-wise Feed-Forward Network\n",
    "    FFN(x) = max(0, xW₁ + b₁)W₂ + b₂\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, d_ff=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Two linear transformations with ReLU activation\n",
    "        return self.linear2(self.dropout(self.activation(self.linear1(x))))\n",
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Single Transformer Encoder Block\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model=512, n_heads=8, d_ff=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.ffn = FeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        # Multi-Head Attention with residual connection\n",
    "        attn_output, attn_weights = self.mha(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        \n",
    "        # Feed-Forward with residual connection\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm2(x + self.dropout(ffn_output))\n",
    "        \n",
    "        return x, attn_weights\n",
    "\n",
    "# Test encoder block\n",
    "def test_encoder_block():\n",
    "    batch_size, seq_len, d_model = 2, 20, 512\n",
    "    \n",
    "    # Create input\n",
    "    x = torch.randn(batch_size, seq_len, d_model)\n",
    "    \n",
    "    # Create encoder block\n",
    "    encoder = TransformerEncoderBlock(d_model)\n",
    "    \n",
    "    # Forward pass\n",
    "    output, attn_weights = encoder(x)\n",
    "    \n",
    "    print(f\"Input shape: {x.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(f\"Output statistics:\")\n",
    "    print(f\"  Mean: {output.mean().item():.4f}\")\n",
    "    print(f\"  Std: {output.std().item():.4f}\")\n",
    "    print(f\"  Min: {output.min().item():.4f}\")\n",
    "    print(f\"  Max: {output.max().item():.4f}\")\n",
    "    \n",
    "    return output, attn_weights\n",
    "\n",
    "encoder_output, encoder_weights = test_encoder_block()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6752d8d",
   "metadata": {},
   "source": [
    "### Exercise 5: Complete Transformer Encoder\n",
    "#### 📚 Concept\n",
    "Stack multiple encoder blocks to create a deep Transformer encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a429df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete Transformer Encoder with N stacked layers\n",
    "    \"\"\"\n",
    "    def __init__(self, n_layers=6, d_model=512, n_heads=8, d_ff=2048, \n",
    "                 max_seq_len=5000, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_seq_len)\n",
    "        \n",
    "        # Stack of encoder blocks\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderBlock(d_model, n_heads, d_ff, dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        # Add positional encoding\n",
    "        x = self.pos_encoding(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Store attention weights from each layer\n",
    "        attention_weights = []\n",
    "        \n",
    "        # Pass through each encoder layer\n",
    "        for layer in self.layers:\n",
    "            x, attn = layer(x, mask)\n",
    "            attention_weights.append(attn)\n",
    "        \n",
    "        return x, attention_weights\n",
    "\n",
    "# Create and test complete encoder\n",
    "def test_transformer_encoder():\n",
    "    batch_size, seq_len, d_model = 2, 30, 512\n",
    "    n_layers = 6\n",
    "    \n",
    "    # Create input embeddings\n",
    "    x = torch.randn(batch_size, seq_len, d_model)\n",
    "    \n",
    "    # Create encoder\n",
    "    encoder = TransformerEncoder(n_layers=n_layers, d_model=d_model)\n",
    "    \n",
    "    # Forward pass\n",
    "    output, all_attn_weights = encoder(x)\n",
    "    \n",
    "    print(f\"Encoder with {n_layers} layers:\")\n",
    "    print(f\"Input shape: {x.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(f\"Number of attention weight matrices: {len(all_attn_weights)}\")\n",
    "    print(f\"Total parameters: {sum(p.numel() for p in encoder.parameters()):,}\")\n",
    "    \n",
    "    return output, all_attn_weights\n",
    "\n",
    "transformer_output, all_attention_weights = test_transformer_encoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389967ac",
   "metadata": {},
   "source": [
    "### Analyzing Layer-wise Attention Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cacca47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_layerwise_attention(all_attention_weights):\n",
    "    \"\"\"\n",
    "    Analyze how attention patterns change across layers\n",
    "    \"\"\"\n",
    "    n_layers = len(all_attention_weights)\n",
    "    \n",
    "    # Create subplots for first 6 layers\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for layer_idx in range(min(n_layers, 6)):\n",
    "        # Take first batch, first head for visualization\n",
    "        weights = all_attention_weights[layer_idx][0, 0].detach().numpy()\n",
    "        \n",
    "        ax = axes[layer_idx]\n",
    "        im = ax.imshow(weights, cmap='Reds', aspect='auto')\n",
    "        ax.set_title(f'Layer {layer_idx + 1}')\n",
    "        ax.set_xlabel('Key Position')\n",
    "        ax.set_ylabel('Query Position')\n",
    "        plt.colorbar(im, ax=ax, fraction=0.046)\n",
    "    \n",
    "    plt.suptitle('Attention Patterns Across Transformer Layers', \n",
    "                fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Analyze attention entropy across layers\n",
    "    print(\"\\n📊 Layer-wise Attention Analysis:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    layer_entropies = []\n",
    "    for layer_idx, attn in enumerate(all_attention_weights):\n",
    "        # Calculate average entropy across all heads and positions\n",
    "        weights = attn[0].detach().numpy()  # First batch\n",
    "        entropy = -np.sum(weights * np.log(weights + 1e-10), axis=-1).mean()\n",
    "        layer_entropies.append(entropy)\n",
    "        print(f\"Layer {layer_idx + 1}: Average Entropy = {entropy:.3f}\")\n",
    "    \n",
    "    # Plot entropy trend\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(range(1, len(layer_entropies) + 1), layer_entropies, \n",
    "            'o-', linewidth=2, markersize=8)\n",
    "    plt.xlabel('Layer')\n",
    "    plt.ylabel('Average Attention Entropy')\n",
    "    plt.title('Attention Entropy Across Layers')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "analyze_layerwise_attention(all_attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f8d054",
   "metadata": {},
   "source": [
    "## Part 4: Practical Applications\n",
    "\n",
    "### Exercise 6: Text Classification with Transformers\n",
    "#### 📚 Concept\n",
    "Apply Transformer encoder for sequence classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592f007a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer-based text classifier\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, n_classes, d_model=512, n_layers=6, \n",
    "                 n_heads=8, d_ff=2048, max_seq_len=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Token embeddings\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.scale = math.sqrt(d_model)\n",
    "        \n",
    "        # Transformer encoder\n",
    "        self.encoder = TransformerEncoder(\n",
    "            n_layers, d_model, n_heads, d_ff, max_seq_len, dropout\n",
    "        )\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model // 2, n_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        # Embed tokens\n",
    "        x = self.embedding(x) * self.scale\n",
    "        \n",
    "        # Encode with transformer\n",
    "        encoded, attn_weights = self.encoder(x, mask)\n",
    "        \n",
    "        # Use [CLS] token or mean pooling for classification\n",
    "        # Here we use mean pooling\n",
    "        if mask is not None:\n",
    "            mask_expanded = mask.unsqueeze(-1).expand(encoded.size()).float()\n",
    "            sum_embeddings = torch.sum(encoded * mask_expanded, dim=1)\n",
    "            sum_mask = torch.clamp(mask_expanded.sum(dim=1), min=1e-9)\n",
    "            pooled = sum_embeddings / sum_mask\n",
    "        else:\n",
    "            pooled = encoded.mean(dim=1)\n",
    "        \n",
    "        # Classify\n",
    "        logits = self.classifier(pooled)\n",
    "        \n",
    "        return logits, attn_weights\n",
    "\n",
    "# Create synthetic classification dataset\n",
    "def create_synthetic_dataset():\n",
    "    \"\"\"\n",
    "    Create a synthetic text classification dataset\n",
    "    \"\"\"\n",
    "    vocab_size = 1000\n",
    "    seq_len = 50\n",
    "    n_samples = 100\n",
    "    n_classes = 3\n",
    "    \n",
    "    # Generate random sequences\n",
    "    X = torch.randint(0, vocab_size, (n_samples, seq_len))\n",
    "    # Generate random labels\n",
    "    y = torch.randint(0, n_classes, (n_samples,))\n",
    "    \n",
    "    # Create attention mask (simulate variable length sequences)\n",
    "    mask = torch.ones(n_samples, seq_len)\n",
    "    for i in range(n_samples):\n",
    "        actual_len = torch.randint(20, seq_len, (1,)).item()\n",
    "        mask[i, actual_len:] = 0\n",
    "    \n",
    "    return X, y, mask\n",
    "\n",
    "# Train the classifier\n",
    "def train_classifier():\n",
    "    # Create dataset\n",
    "    X, y, mask = create_synthetic_dataset()\n",
    "    \n",
    "    # Model parameters\n",
    "    vocab_size = 1000\n",
    "    n_classes = 3\n",
    "    d_model = 256  # Smaller model for demo\n",
    "    \n",
    "    # Create model\n",
    "    model = TransformerClassifier(\n",
    "        vocab_size, n_classes, d_model=d_model, \n",
    "        n_layers=3, n_heads=4\n",
    "    )\n",
    "    \n",
    "    # Training setup\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Training loop (simplified)\n",
    "    model.train()\n",
    "    n_epochs = 10\n",
    "    batch_size = 16\n",
    "    \n",
    "    print(\"Training Transformer Classifier...\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    losses = []\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss = 0\n",
    "        n_batches = len(X) // batch_size\n",
    "        \n",
    "        for i in range(n_batches):\n",
    "            # Get batch\n",
    "            batch_X = X[i*batch_size:(i+1)*batch_size]\n",
    "            batch_y = y[i*batch_size:(i+1)*batch_size]\n",
    "            batch_mask = mask[i*batch_size:(i+1)*batch_size]\n",
    "            \n",
    "            # Forward pass\n",
    "            logits, _ = model(batch_X, batch_mask)\n",
    "            loss = criterion(logits, batch_y)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_loss = epoch_loss / n_batches\n",
    "        losses.append(avg_loss)\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs}, Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Plot training curve\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(losses, 'o-', linewidth=2, markersize=6)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss Curve')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    return model, losses\n",
    "\n",
    "trained_model, training_losses = train_classifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa393b6",
   "metadata": {},
   "source": [
    "### Exercise 7: Sequence-to-Sequence with Transformers\n",
    "#### 📚 Concept\n",
    "Implement a simple sequence-to-sequence model using Transformer architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0740ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerSeq2Seq(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple Transformer for Sequence-to-Sequence tasks\n",
    "    (Encoder-only for simplicity)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_vocab_size, output_vocab_size, \n",
    "                 d_model=512, n_layers=6, n_heads=8, d_ff=2048, \n",
    "                 max_seq_len=100, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embeddings\n",
    "        self.input_embedding = nn.Embedding(input_vocab_size, d_model)\n",
    "        self.output_embedding = nn.Embedding(output_vocab_size, d_model)\n",
    "        self.scale = math.sqrt(d_model)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_seq_len)\n",
    "        \n",
    "        # Transformer encoder\n",
    "        self.encoder = TransformerEncoder(\n",
    "            n_layers, d_model, n_heads, d_ff, max_seq_len, dropout\n",
    "        )\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_projection = nn.Linear(d_model, output_vocab_size)\n",
    "        \n",
    "    def forward(self, src, tgt=None):\n",
    "        # Encode source\n",
    "        src_emb = self.input_embedding(src) * self.scale\n",
    "        src_emb = self.pos_encoding(src_emb)\n",
    "        \n",
    "        # Get encoder output\n",
    "        encoder_output, _ = self.encoder(src_emb)\n",
    "        \n",
    "        # Project to output vocabulary\n",
    "        output = self.output_projection(encoder_output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Demonstrate sequence generation\n",
    "def demonstrate_seq2seq():\n",
    "    \"\"\"\n",
    "    Simple demonstration of sequence-to-sequence translation\n",
    "    \"\"\"\n",
    "    # Model parameters\n",
    "    input_vocab_size = 100\n",
    "    output_vocab_size = 100\n",
    "    seq_len = 20\n",
    "    d_model = 256\n",
    "    \n",
    "    # Create model\n",
    "    model = TransformerSeq2Seq(\n",
    "        input_vocab_size, output_vocab_size,\n",
    "        d_model=d_model, n_layers=2, n_heads=4\n",
    "    )\n",
    "    \n",
    "    # Create sample input\n",
    "    batch_size = 2\n",
    "    src = torch.randint(0, input_vocab_size, (batch_size, seq_len))\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(src)\n",
    "    \n",
    "    print(f\"Sequence-to-Sequence Model:\")\n",
    "    print(f\"Input shape: {src.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(f\"Output vocab size: {output_vocab_size}\")\n",
    "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    # Get predictions\n",
    "    predictions = output.argmax(dim=-1)\n",
    "    print(f\"\\nSample predictions shape: {predictions.shape}\")\n",
    "    \n",
    "    return model, output\n",
    "\n",
    "seq2seq_model, seq2seq_output = demonstrate_seq2seq()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18e9c42",
   "metadata": {},
   "source": [
    "### 🎯 Your Turn: Exercise 7\n",
    "Implement beam search for better sequence generation. Currently, we use greedy decoding (argmax).\n",
    "Implement beam search with beam_size=3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5659a498",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search(model, src, beam_size=3, max_length=50):\n",
    "    \"\"\"\n",
    "    Implement beam search for sequence generation\n",
    "    TODO: Complete this implementation\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    # Hints:\n",
    "    # 1. Maintain beam_size hypotheses\n",
    "    # 2. At each step, expand each hypothesis\n",
    "    # 3. Keep top beam_size candidates based on cumulative probability\n",
    "    # 4. Stop when EOS token or max_length reached\n",
    "    \n",
    "    # Placeholder return\n",
    "    return None\n",
    "\n",
    "# Test your implementation\n",
    "# src_sequence = torch.randint(0, 100, (1, 20))\n",
    "# best_sequence = beam_search(seq2seq_model, src_sequence, beam_size=3)\n",
    "# print(f\"Best sequence: {best_sequence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9c9724",
   "metadata": {},
   "source": [
    "## Part 5: Advanced Topics\n",
    "\n",
    "### Exercise 8: Attention Visualization and Interpretability\n",
    "#### 📚 Concept\n",
    "Visualize what the model is \"looking at\" when making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468bcf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_attention_visualization(text_tokens, attention_weights):\n",
    "    \"\"\"\n",
    "    Create an interactive attention visualization\n",
    "    \"\"\"\n",
    "    # Convert attention weights to numpy if needed\n",
    "    if torch.is_tensor(attention_weights):\n",
    "        attention_weights = attention_weights.detach().cpu().numpy()\n",
    "    \n",
    "    # Take first head for visualization\n",
    "    if len(attention_weights.shape) > 2:\n",
    "        attention_weights = attention_weights[0]  # First head\n",
    "    \n",
    "    # Create interactive heatmap with Plotly\n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=attention_weights,\n",
    "        x=text_tokens,\n",
    "        y=text_tokens,\n",
    "        colorscale='Reds',\n",
    "        text=np.round(attention_weights, 3),\n",
    "        texttemplate=\"%{text}\",\n",
    "        textfont={\"size\": 8},\n",
    "        showscale=True,\n",
    "        hoverongaps=False,\n",
    "        hovertemplate=\"From: %{y}<br>To: %{x}<br>Weight: %{z:.3f}<extra></extra>\"\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=\"Attention Weight Visualization\",\n",
    "        xaxis_title=\"Keys (To)\",\n",
    "        yaxis_title=\"Queries (From)\",\n",
    "        height=600,\n",
    "        width=700,\n",
    "        xaxis={'side': 'bottom'},\n",
    "        yaxis={'autorange': 'reversed'}\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    # Also create a flow diagram showing top attention connections\n",
    "    threshold = 0.1  # Only show connections above this threshold\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Create node positions in a circle\n",
    "    n_tokens = len(text_tokens)\n",
    "    angles = np.linspace(0, 2*np.pi, n_tokens, endpoint=False)\n",
    "    x_pos = np.cos(angles)\n",
    "    y_pos = np.sin(angles)\n",
    "    \n",
    "    # Draw tokens as nodes\n",
    "    for i, (x, y, token) in enumerate(zip(x_pos, y_pos, text_tokens)):\n",
    "        plt.scatter(x, y, s=1000, c='lightblue', edgecolors='black', linewidth=2, zorder=3)\n",
    "        plt.text(x, y, token, ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # Draw attention connections as edges\n",
    "    for i in range(n_tokens):\n",
    "        for j in range(n_tokens):\n",
    "            if attention_weights[i, j] > threshold:\n",
    "                # Draw arrow from i to j with weight as thickness\n",
    "                plt.arrow(x_pos[i], y_pos[i], \n",
    "                         0.8*(x_pos[j] - x_pos[i]), 0.8*(y_pos[j] - y_pos[i]),\n",
    "                         head_width=0.05, head_length=0.05,\n",
    "                         alpha=attention_weights[i, j],\n",
    "                         color='red', zorder=1,\n",
    "                         length_includes_head=True,\n",
    "                         width=attention_weights[i, j] * 0.02)\n",
    "    \n",
    "    plt.title(\"Attention Flow Diagram (Threshold > 0.1)\", fontsize=14, fontweight='bold')\n",
    "    plt.axis('equal')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage with sample tokens\n",
    "sample_tokens = [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\", \".\", \"<PAD>\"]\n",
    "sample_attention = torch.softmax(torch.randn(8, 8) * 2, dim=-1)\n",
    "\n",
    "create_attention_visualization(sample_tokens, sample_attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b2c04c",
   "metadata": {},
   "source": [
    "### Exercise 9: Efficient Attention Mechanisms\n",
    "#### 📚 Concept\n",
    "Explore efficient attention variants to handle longer sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11e5098",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Linear Attention - O(n) complexity instead of O(n²)\n",
    "    Using kernel trick to approximate attention\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        batch_size, seq_len, _ = q.size()\n",
    "        \n",
    "        # Linear projections\n",
    "        Q = self.W_q(q).view(batch_size, seq_len, self.n_heads, self.d_k)\n",
    "        K = self.W_k(k).view(batch_size, seq_len, self.n_heads, self.d_k)\n",
    "        V = self.W_v(v).view(batch_size, seq_len, self.n_heads, self.d_k)\n",
    "        \n",
    "        # Apply kernel feature map (simplified - using ELU + 1)\n",
    "        Q = F.elu(Q) + 1\n",
    "        K = F.elu(K) + 1\n",
    "        \n",
    "        # Compute attention in linear time\n",
    "        # Instead of (QK^T)V, compute Q(K^TV)\n",
    "        K_T_V = torch.einsum('bshd,bshf->bhdf', K, V)\n",
    "        QK_T_V = torch.einsum('bshd,bhdf->bshf', Q, K_T_V)\n",
    "        \n",
    "        # Normalization\n",
    "        K_sum = K.sum(dim=1, keepdim=True)\n",
    "        Q_K_sum = torch.einsum('bshd,bhd->bsh', Q, K_sum.squeeze(1))\n",
    "        output = QK_T_V / (Q_K_sum.unsqueeze(-1) + 1e-6)\n",
    "        \n",
    "        # Reshape and project\n",
    "        output = output.reshape(batch_size, seq_len, self.d_model)\n",
    "        output = self.W_o(output)\n",
    "        \n",
    "        return output, None  # No attention weights in linear attention\n",
    "\n",
    "def compare_attention_mechanisms():\n",
    "    \"\"\"\n",
    "    Compare standard attention vs linear attention\n",
    "    \"\"\"\n",
    "    import time\n",
    "    \n",
    "    d_model = 256\n",
    "    n_heads = 4\n",
    "    \n",
    "    # Test different sequence lengths\n",
    "    seq_lengths = [50, 100, 200, 500, 1000]\n",
    "    \n",
    "    standard_times = []\n",
    "    linear_times = []\n",
    "    \n",
    "    print(\"Comparing Attention Mechanisms:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for seq_len in seq_lengths:\n",
    "        batch_size = 4\n",
    "        x = torch.randn(batch_size, seq_len, d_model)\n",
    "        \n",
    "        # Standard attention\n",
    "        standard_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        start = time.time()\n",
    "        with torch.no_grad():\n",
    "            _, _ = standard_attn(x, x, x)\n",
    "        standard_time = time.time() - start\n",
    "        standard_times.append(standard_time)\n",
    "        \n",
    "        # Linear attention\n",
    "        linear_attn = LinearAttention(d_model, n_heads)\n",
    "        start = time.time()\n",
    "        with torch.no_grad():\n",
    "            _, _ = linear_attn(x, x, x)\n",
    "        linear_time = time.time() - start\n",
    "        linear_times.append(linear_time)\n",
    "        \n",
    "        print(f\"Seq Length {seq_len:4d}: Standard={standard_time:.4f}s, \"\n",
    "              f\"Linear={linear_time:.4f}s, Speedup={standard_time/linear_time:.2f}x\")\n",
    "    \n",
    "    # Plot comparison\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(seq_lengths, standard_times, 'o-', label='Standard Attention', linewidth=2)\n",
    "    plt.plot(seq_lengths, linear_times, 's-', label='Linear Attention', linewidth=2)\n",
    "    plt.xlabel('Sequence Length')\n",
    "    plt.ylabel('Time (seconds)')\n",
    "    plt.title('Computation Time Comparison')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    speedup = [s/l for s, l in zip(standard_times, linear_times)]\n",
    "    plt.bar(range(len(seq_lengths)), speedup, color='green', alpha=0.7)\n",
    "    plt.xticks(range(len(seq_lengths)), seq_lengths)\n",
    "    plt.xlabel('Sequence Length')\n",
    "    plt.ylabel('Speedup Factor')\n",
    "    plt.title('Linear Attention Speedup')\n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "compare_attention_mechanisms()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e794a91",
   "metadata": {},
   "source": [
    "### Exercise 10: Training Dynamics Analysis\n",
    "#### 📚 Concept\n",
    "Analyze and visualize the training dynamics of Transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98b7e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingMonitor:\n",
    "    \"\"\"\n",
    "    Monitor and visualize transformer training dynamics\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.metrics = {\n",
    "            'loss': [],\n",
    "            'gradient_norm': [],\n",
    "            'learning_rate': [],\n",
    "            'attention_entropy': []\n",
    "        }\n",
    "        \n",
    "    def update(self, loss, model, lr, attention_weights=None):\n",
    "        # Record loss\n",
    "        self.metrics['loss'].append(loss)\n",
    "        \n",
    "        # Calculate gradient norm\n",
    "        total_norm = 0\n",
    "        for p in model.parameters():\n",
    "            if p.grad is not None:\n",
    "                param_norm = p.grad.data.norm(2)\n",
    "                total_norm += param_norm.item() ** 2\n",
    "        total_norm = total_norm ** 0.5\n",
    "        self.metrics['gradient_norm'].append(total_norm)\n",
    "        \n",
    "        # Record learning rate\n",
    "        self.metrics['learning_rate'].append(lr)\n",
    "        \n",
    "        # Calculate attention entropy if provided\n",
    "        if attention_weights is not None:\n",
    "            # Calculate entropy of attention distribution\n",
    "            weights = attention_weights.detach().cpu().numpy()\n",
    "            entropy = -np.sum(weights * np.log(weights + 1e-10), axis=-1).mean()\n",
    "            self.metrics['attention_entropy'].append(entropy)\n",
    "        \n",
    "    def plot_metrics(self):\n",
    "        \"\"\"\n",
    "        Create comprehensive training visualization\n",
    "        \"\"\"\n",
    "        fig = make_subplots(\n",
    "            rows=2, cols=2,\n",
    "            subplot_titles=('Training Loss', 'Gradient Norm', \n",
    "                          'Learning Rate Schedule', 'Attention Entropy')\n",
    "        )\n",
    "        \n",
    "        # Training loss\n",
    "        fig.add_trace(\n",
    "            go.Scatter(y=self.metrics['loss'], mode='lines', name='Loss'),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # Gradient norm\n",
    "        fig.add_trace(\n",
    "            go.Scatter(y=self.metrics['gradient_norm'], mode='lines', \n",
    "                      name='Grad Norm', line=dict(color='red')),\n",
    "            row=1, col=2\n",
    "        )\n",
    "        \n",
    "        # Learning rate\n",
    "        fig.add_trace(\n",
    "            go.Scatter(y=self.metrics['learning_rate'], mode='lines', \n",
    "                      name='LR', line=dict(color='green')),\n",
    "            row=2, col=1\n",
    "        )\n",
    "        \n",
    "        # Attention entropy\n",
    "        if self.metrics['attention_entropy']:\n",
    "            fig.add_trace(\n",
    "                go.Scatter(y=self.metrics['attention_entropy'], mode='lines', \n",
    "                          name='Entropy', line=dict(color='purple')),\n",
    "                row=2, col=2\n",
    "            )\n",
    "        \n",
    "        fig.update_layout(height=600, showlegend=False, \n",
    "                         title_text=\"Training Dynamics Analysis\")\n",
    "        fig.update_xaxes(title_text=\"Step\")\n",
    "        fig.update_yaxes(title_text=\"Value\")\n",
    "        \n",
    "        fig.show()\n",
    "\n",
    "# Simulate training with monitoring\n",
    "def simulate_training_with_monitoring():\n",
    "    \"\"\"\n",
    "    Demonstrate training monitoring\n",
    "    \"\"\"\n",
    "    # Create simple model\n",
    "    model = TransformerEncoder(n_layers=2, d_model=128, n_heads=4)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    monitor = TrainingMonitor()\n",
    "    \n",
    "    # Simulate training steps\n",
    "    print(\"Simulating training with monitoring...\")\n",
    "    for step in range(50):\n",
    "        # Fake data\n",
    "        x = torch.randn(4, 20, 128)\n",
    "        \n",
    "        # Forward pass\n",
    "        output, attn_weights = model(x)\n",
    "        loss = output.mean()  # Dummy loss\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update monitor\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        monitor.update(\n",
    "            loss.item(), model, current_lr, \n",
    "            attn_weights[0] if attn_weights else None\n",
    "        )\n",
    "        \n",
    "        if (step + 1) % 10 == 0:\n",
    "            print(f\"Step {step+1}: Loss={loss.item():.4f}\")\n",
    "    \n",
    "    # Plot training dynamics\n",
    "    monitor.plot_metrics()\n",
    "    \n",
    "    return monitor\n",
    "\n",
    "training_monitor = simulate_training_with_monitoring()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd346fa",
   "metadata": {},
   "source": [
    "### 🎯 Your Turn: Exercise 10\n",
    "Implement learning rate warmup schedule as used in the original Transformer paper.\n",
    "The formula is: `lr = d_model^(-0.5) * min(step^(-0.5), step * warmup_steps^(-1.5))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9902abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WarmupScheduler:\n",
    "    \"\"\"\n",
    "    Learning rate scheduler with warmup\n",
    "    TODO: Implement the warmup schedule\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "        \n",
    "    def get_lr(self, step):\n",
    "        \"\"\"\n",
    "        Calculate learning rate for given step\n",
    "        \"\"\"\n",
    "        # TODO: Implement the formula\n",
    "        # lr = d_model^(-0.5) * min(step^(-0.5), step * warmup_steps^(-1.5))\n",
    "        \n",
    "        # Your code here\n",
    "        lr = 0.001  # Replace with proper implementation\n",
    "        \n",
    "        # Solution (uncomment to see):\n",
    "        # arg1 = step ** (-0.5)\n",
    "        # arg2 = step * (self.warmup_steps ** (-1.5))\n",
    "        # lr = (self.d_model ** (-0.5)) * min(arg1, arg2)\n",
    "        \n",
    "        return lr\n",
    "\n",
    "# Test and visualize the schedule\n",
    "def visualize_lr_schedule():\n",
    "    scheduler = WarmupScheduler(d_model=512, warmup_steps=4000)\n",
    "    \n",
    "    steps = np.arange(1, 10000)\n",
    "    lrs = [scheduler.get_lr(step) for step in steps]\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(steps, lrs, linewidth=2)\n",
    "    plt.axvline(x=4000, color='red', linestyle='--', label='Warmup ends')\n",
    "    plt.xlabel('Training Step')\n",
    "    plt.ylabel('Learning Rate')\n",
    "    plt.title('Learning Rate Schedule with Warmup')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# visualize_lr_schedule()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6749cdd",
   "metadata": {},
   "source": [
    "## Part 6: Summary and Final Exercises\n",
    "\n",
    "### 🎓 Key Takeaways\n",
    "\n",
    "1. **Self-Attention**: Enables parallel processing and captures global dependencies\n",
    "2. **Multi-Head Attention**: Learns different types of relationships simultaneously\n",
    "3. **Positional Encoding**: Injects position information into the model\n",
    "4. **Architecture**: Encoder-decoder structure with residual connections and normalization\n",
    "5. **Applications**: Versatile architecture for NLP, vision, and multimodal tasks\n",
    "\n",
    "### 📊 Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d96297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive comparison chart\n",
    "def create_architecture_comparison():\n",
    "    \"\"\"\n",
    "    Compare Transformer with RNN/LSTM/GRU\n",
    "    \"\"\"\n",
    "    architectures = ['RNN', 'LSTM', 'GRU', 'Transformer']\n",
    "    \n",
    "    # Metrics (relative scores)\n",
    "    metrics = {\n",
    "        'Parallelization': [1, 1, 1, 10],\n",
    "        'Long-range Dependencies': [2, 5, 5, 9],\n",
    "        'Training Speed': [3, 2, 3, 8],\n",
    "        'Memory Efficiency': [8, 6, 7, 4],\n",
    "        'Parameter Efficiency': [9, 5, 6, 7]\n",
    "    }\n",
    "    \n",
    "    # Create radar chart\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    for arch in architectures:\n",
    "        values = [metrics[metric][architectures.index(arch)] \n",
    "                 for metric in metrics.keys()]\n",
    "        values.append(values[0])  # Close the polygon\n",
    "        \n",
    "        fig.add_trace(go.Scatterpolar(\n",
    "            r=values,\n",
    "            theta=list(metrics.keys()) + [list(metrics.keys())[0]],\n",
    "            fill='toself',\n",
    "            name=arch\n",
    "        ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        polar=dict(\n",
    "            radialaxis=dict(\n",
    "                visible=True,\n",
    "                range=[0, 10]\n",
    "            )),\n",
    "        showlegend=True,\n",
    "        title=\"Architecture Comparison: RNN vs Transformer\",\n",
    "        height=500\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    # Also create bar chart comparison\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.2\n",
    "    \n",
    "    for i, arch in enumerate(architectures):\n",
    "        values = [metrics[metric][i] for metric in metrics.keys()]\n",
    "        plt.bar(x + i*width, values, width, label=arch)\n",
    "    \n",
    "    plt.xlabel('Metrics')\n",
    "    plt.ylabel('Score (1-10)')\n",
    "    plt.title('Architecture Performance Comparison')\n",
    "    plt.xticks(x + width*1.5, list(metrics.keys()), rotation=45)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "create_architecture_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360adcac",
   "metadata": {},
   "source": [
    "### 🚀 Next Steps and Resources\n",
    "\n",
    "#### Recommended Implementations to Try:\n",
    "1. **BERT-style Pretraining**: Implement masked language modeling\n",
    "2. **GPT-style Generation**: Build an autoregressive language model\n",
    "3. **Vision Transformer**: Apply Transformers to image patches\n",
    "4. **Cross-modal Attention**: Combine text and image understanding\n",
    "\n",
    "#### Key Papers to Read:\n",
    "- **\"Attention Is All You Need\"** (Vaswani et al., 2017) - Original Transformer\n",
    "- **\"BERT: Pre-training of Deep Bidirectional Transformers\"** (Devlin et al., 2018)\n",
    "- **\"Language Models are Few-Shot Learners\"** (Brown et al., 2020) - GPT-3\n",
    "- **\"An Image is Worth 16x16 Words\"** (Dosovitskiy et al., 2020) - Vision Transformer\n",
    "\n",
    "#### Useful Libraries:\n",
    "- **Hugging Face Transformers**: Production-ready implementations\n",
    "- **PyTorch**: Flexible deep learning framework\n",
    "- **Einops**: Elegant tensor operations\n",
    "- **Weights & Biases**: Experiment tracking\n",
    "\n",
    "---\n",
    "\n",
    "### 📝 Final Challenge Exercise\n",
    "\n",
    "Implement a mini-BERT model for masked language modeling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d271f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Challenge: Mini-BERT Implementation\n",
    "class MiniBERT(nn.Module):\n",
    "    \"\"\"\n",
    "    Simplified BERT for Masked Language Modeling\n",
    "    Your task: Complete the implementation\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d_model=256, n_layers=4, n_heads=4, \n",
    "                 max_seq_len=512, mask_token_id=103):\n",
    "        super().__init__()\n",
    "        self.mask_token_id = mask_token_id\n",
    "        \n",
    "        # TODO: Add components\n",
    "        # 1. Token embeddings\n",
    "        # 2. Segment embeddings (for sentence A/B)\n",
    "        # 3. Positional encoding\n",
    "        # 4. Transformer encoder\n",
    "        # 5. MLM head (prediction layer)\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def forward(self, input_ids, mask_positions):\n",
    "        \"\"\"\n",
    "        Forward pass for masked language modeling\n",
    "        \n",
    "        Args:\n",
    "            input_ids: Token IDs with [MASK] tokens\n",
    "            mask_positions: Positions of masked tokens\n",
    "        \n",
    "        Returns:\n",
    "            predictions: Vocabulary predictions for masked positions\n",
    "        \"\"\"\n",
    "        # TODO: Implement forward pass\n",
    "        pass\n",
    "    \n",
    "    def create_mlm_data(self, text_ids, mask_prob=0.15):\n",
    "        \"\"\"\n",
    "        Create masked language modeling training data\n",
    "        \"\"\"\n",
    "        # TODO: Randomly mask tokens\n",
    "        pass\n",
    "\n",
    "# Placeholder for testing\n",
    "print(\"Challenge: Implement MiniBERT for masked language modeling!\")\n",
    "print(\"This combines all concepts learned in this notebook.\")\n",
    "print(\"Good luck! 🚀\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3923206",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🎉 Congratulations!\n",
    "\n",
    "You've completed the comprehensive Transformer Architecture notebook! You've learned:\n",
    "\n",
    "✅ Self-attention mechanism and its implementation  \n",
    "✅ Multi-head attention for learning diverse patterns  \n",
    "✅ Positional encoding techniques  \n",
    "✅ Complete Transformer architecture  \n",
    "✅ Practical applications and optimizations  \n",
    "✅ Advanced topics like efficient attention and training dynamics  \n",
    "\n",
    "### 📚 Additional Exercises:\n",
    "1. Experiment with different positional encoding methods\n",
    "2. Implement cross-attention for encoder-decoder models\n",
    "3. Try different attention patterns (local, sparse, etc.)\n",
    "4. Build a small-scale language model\n",
    "5. Apply Transformers to your own dataset\n",
    "\n",
    "### 🤝 Connect and Share:\n",
    "Share your implementations and insights with the community!\n",
    "\n",
    "---\n",
    "**Remember**: The best way to understand Transformers is to implement them from scratch. Keep experimenting and building! 💪"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
