{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multi-layer Perceptrons (MLPs): From Theory to Practice\n",
        "## Based on Lecture 5: From Logistic Regression to Multi-layer Perceptrons\n",
        "\n",
        "**Author:** Ho-min Park  \n",
        "**Interactive Notebook Version**\n",
        "\n",
        "---\n",
        "\n",
        "## 🎯 Learning Objectives\n",
        "\n",
        "By the end of this notebook, you will:\n",
        "1. Understand why we need neural networks (XOR problem)\n",
        "2. Implement neurons and activation functions from scratch\n",
        "3. Build a complete MLP architecture\n",
        "4. Master forward propagation\n",
        "5. Implement backpropagation algorithm\n",
        "6. Train neural networks on real datasets\n",
        "7. Visualize decision boundaries and training dynamics\n",
        "8. Compare different activation functions and architectures\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 0: Setup and Imports\n",
        "\n",
        "Let's start by importing all necessary libraries and setting up our environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Essential imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import make_classification, make_moons, make_circles\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Configure plotting\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette('husl')\n",
        "%matplotlib inline\n",
        "\n",
        "print('Setup complete! ✅')\n",
        "print(f'NumPy version: {np.__version__}')\n",
        "print(f'Pandas version: {pd.__version__}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 1: Neural Network Motivation\n",
        "\n",
        "### Why do we need neural networks?\n",
        "\n",
        "Linear models like logistic regression have fundamental limitations. Let's explore the famous XOR problem that demonstrates why we need non-linear models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 1: The XOR Problem - Linear Inseparability\n",
        "\n",
        "#### 📚 Concept\n",
        "The XOR (exclusive OR) problem is a classic example that shows the limitations of linear classifiers. The XOR function outputs 1 when inputs are different, and 0 when they're the same. This creates a pattern that cannot be separated by a single straight line.\n",
        "\n",
        "#### 💻 Code Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create XOR dataset\n",
        "def create_xor_data():\n",
        "    \"\"\"Generate XOR problem dataset\"\"\"\n",
        "    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "    y = np.array([0, 1, 1, 0])  # XOR logic\n",
        "    return X, y\n",
        "\n",
        "# Generate data\n",
        "X_xor, y_xor = create_xor_data()\n",
        "\n",
        "# Display truth table\n",
        "xor_df = pd.DataFrame({\n",
        "    'x₁': X_xor[:, 0],\n",
        "    'x₂': X_xor[:, 1],\n",
        "    'XOR Output': y_xor\n",
        "})\n",
        "\n",
        "print(\"XOR Truth Table:\")\n",
        "print(xor_df.to_string(index=False))\n",
        "print(\"\\n🔍 Notice: Outputs with same inputs → 0, different inputs → 1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize XOR problem\n",
        "plt.figure(figsize=(10, 4))\n",
        "\n",
        "# Subplot 1: Data points\n",
        "plt.subplot(1, 2, 1)\n",
        "colors = ['red' if y == 0 else 'blue' for y in y_xor]\n",
        "plt.scatter(X_xor[:, 0], X_xor[:, 1], c=colors, s=200, edgecolors='black', linewidth=2)\n",
        "\n",
        "# Add labels\n",
        "for i, (x1, x2, y) in enumerate(zip(X_xor[:, 0], X_xor[:, 1], y_xor)):\n",
        "    plt.annotate(f'({int(x1)},{int(x2)})\\ny={y}', \n",
        "                xy=(x1, x2), xytext=(x1-0.15, x2+0.1), fontsize=10)\n",
        "\n",
        "plt.xlim(-0.5, 1.5)\n",
        "plt.ylim(-0.5, 1.5)\n",
        "plt.xlabel('x₁', fontsize=12)\n",
        "plt.ylabel('x₂', fontsize=12)\n",
        "plt.title('XOR Problem: Data Points', fontsize=14, fontweight='bold')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Subplot 2: Failed linear separation attempt\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(X_xor[:, 0], X_xor[:, 1], c=colors, s=200, edgecolors='black', linewidth=2)\n",
        "\n",
        "# Try to draw a separating line (will fail)\n",
        "x_line = np.linspace(-0.5, 1.5, 100)\n",
        "y_line = 1 - x_line  # Example line\n",
        "plt.plot(x_line, y_line, 'g--', linewidth=2, label='Attempted separator')\n",
        "\n",
        "plt.xlim(-0.5, 1.5)\n",
        "plt.ylim(-0.5, 1.5)\n",
        "plt.xlabel('x₁', fontsize=12)\n",
        "plt.ylabel('x₂', fontsize=12)\n",
        "plt.title('XOR: No Linear Separation Possible!', fontsize=14, fontweight='bold')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"❌ No single line can separate red from blue points!\")\n",
        "print(\"✅ This is why we need neural networks with hidden layers.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 🎯 Your Turn\n",
        "\n",
        "**Task:** Implement a simple logistic regression to verify it cannot solve XOR. Then, think about what transformation might help.\n",
        "\n",
        "**Hint:** What if we could transform the input space? For example, what if we added a feature like x₁ × x₂?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Your code here\n",
        "# 1. Try logistic regression on XOR (it will fail)\n",
        "# 2. Add a new feature: x1 * x2\n",
        "# 3. Try logistic regression again with the new feature\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Your implementation:\n",
        "# lr = LogisticRegression()\n",
        "# lr.fit(X_xor, y_xor)\n",
        "# predictions = lr.predict(X_xor)\n",
        "# print(f\"Accuracy with linear features: {accuracy_score(y_xor, predictions):.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "### Exercise 2: Implementing Activation Functions\n",
        "\n",
        "#### 📚 Concept\n",
        "Activation functions introduce non-linearity into neural networks, allowing them to learn complex patterns. Without activation functions, even deep networks would collapse to a single linear transformation.\n",
        "\n",
        "#### 💻 Code Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ActivationFunctions:\n",
        "    \"\"\"Collection of activation functions and their derivatives\"\"\"\n",
        "    \n",
        "    @staticmethod\n",
        "    def sigmoid(z):\n",
        "        \"\"\"Sigmoid activation: σ(z) = 1/(1 + e^(-z))\"\"\"\n",
        "        return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
        "    \n",
        "    @staticmethod\n",
        "    def sigmoid_derivative(z):\n",
        "        \"\"\"Derivative of sigmoid: σ'(z) = σ(z)(1 - σ(z))\"\"\"\n",
        "        s = ActivationFunctions.sigmoid(z)\n",
        "        return s * (1 - s)\n",
        "    \n",
        "    @staticmethod\n",
        "    def tanh(z):\n",
        "        \"\"\"Hyperbolic tangent activation\"\"\"\n",
        "        return np.tanh(z)\n",
        "    \n",
        "    @staticmethod\n",
        "    def tanh_derivative(z):\n",
        "        \"\"\"Derivative of tanh: (1 - tanh²(z))\"\"\"\n",
        "        return 1 - np.tanh(z) ** 2\n",
        "    \n",
        "    @staticmethod\n",
        "    def relu(z):\n",
        "        \"\"\"ReLU activation: max(0, z)\"\"\"\n",
        "        return np.maximum(0, z)\n",
        "    \n",
        "    @staticmethod\n",
        "    def relu_derivative(z):\n",
        "        \"\"\"Derivative of ReLU: 1 if z > 0, else 0\"\"\"\n",
        "        return (z > 0).astype(float)\n",
        "    \n",
        "    @staticmethod\n",
        "    def leaky_relu(z, alpha=0.01):\n",
        "        \"\"\"Leaky ReLU: max(αz, z)\"\"\"\n",
        "        return np.where(z > 0, z, alpha * z)\n",
        "    \n",
        "    @staticmethod\n",
        "    def leaky_relu_derivative(z, alpha=0.01):\n",
        "        \"\"\"Derivative of Leaky ReLU\"\"\"\n",
        "        return np.where(z > 0, 1, alpha)\n",
        "\n",
        "# Test activation functions\n",
        "af = ActivationFunctions()\n",
        "z = np.linspace(-5, 5, 100)\n",
        "\n",
        "print(\"Activation Functions Implementation Complete! ✅\")\n",
        "print(f\"Sigmoid at z=0: {af.sigmoid(0):.4f}\")\n",
        "print(f\"ReLU at z=-1: {af.relu(-1):.4f}\")\n",
        "print(f\"ReLU at z=1: {af.relu(1):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize activation functions\n",
        "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
        "z = np.linspace(-5, 5, 100)\n",
        "\n",
        "# Define functions and derivatives\n",
        "functions = [\n",
        "    ('Sigmoid', af.sigmoid, af.sigmoid_derivative),\n",
        "    ('Tanh', af.tanh, af.tanh_derivative),\n",
        "    ('ReLU', af.relu, af.relu_derivative),\n",
        "    ('Leaky ReLU', af.leaky_relu, af.leaky_relu_derivative)\n",
        "]\n",
        "\n",
        "for idx, (name, func, deriv) in enumerate(functions):\n",
        "    # Plot activation function\n",
        "    ax = axes[0, idx]\n",
        "    ax.plot(z, func(z), linewidth=2.5, color=f'C{idx}')\n",
        "    ax.set_title(f'{name}', fontsize=12, fontweight='bold')\n",
        "    ax.set_xlabel('z')\n",
        "    ax.set_ylabel('f(z)')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.axhline(y=0, color='k', linestyle='-', alpha=0.2)\n",
        "    ax.axvline(x=0, color='k', linestyle='-', alpha=0.2)\n",
        "    \n",
        "    # Plot derivative\n",
        "    ax = axes[1, idx]\n",
        "    ax.plot(z, deriv(z), linewidth=2.5, color=f'C{idx}', linestyle='--')\n",
        "    ax.set_title(f'{name} Derivative', fontsize=12)\n",
        "    ax.set_xlabel('z')\n",
        "    ax.set_ylabel(\"f'(z)\")\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.axhline(y=0, color='k', linestyle='-', alpha=0.2)\n",
        "    ax.axvline(x=0, color='k', linestyle='-', alpha=0.2)\n",
        "\n",
        "plt.suptitle('Activation Functions and Their Derivatives', fontsize=16, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"📊 Key Observations:\")\n",
        "print(\"• Sigmoid: Bounded [0,1], suffers from vanishing gradients\")\n",
        "print(\"• Tanh: Zero-centered, bounded [-1,1], also vanishing gradients\")\n",
        "print(\"• ReLU: Unbounded, fast, but has dead neurons problem\")\n",
        "print(\"• Leaky ReLU: Solves dead neurons by allowing small negative gradients\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "### Exercise 3: Building a Single Neuron\n",
        "\n",
        "#### 📚 Concept\n",
        "A neuron (perceptron) is the basic building block of neural networks. It computes a weighted sum of inputs, adds a bias, and applies an activation function.\n",
        "\n",
        "**Formula:** `output = activation(w₁x₁ + w₂x₂ + ... + wₙxₙ + b)`\n",
        "\n",
        "#### 💻 Code Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Neuron:\n",
        "    \"\"\"A single neuron with configurable activation\"\"\"\n",
        "    \n",
        "    def __init__(self, n_inputs, activation='sigmoid'):\n",
        "        \"\"\"Initialize neuron with random weights and bias\"\"\"\n",
        "        self.weights = np.random.randn(n_inputs) * 0.1\n",
        "        self.bias = np.random.randn() * 0.1\n",
        "        self.activation = activation\n",
        "        self.af = ActivationFunctions()\n",
        "        \n",
        "        # Store for backpropagation\n",
        "        self.last_input = None\n",
        "        self.last_z = None\n",
        "        self.last_output = None\n",
        "    \n",
        "    def forward(self, inputs):\n",
        "        \"\"\"Compute neuron output\"\"\"\n",
        "        self.last_input = inputs\n",
        "        self.last_z = np.dot(inputs, self.weights) + self.bias\n",
        "        \n",
        "        # Apply activation\n",
        "        if self.activation == 'sigmoid':\n",
        "            self.last_output = self.af.sigmoid(self.last_z)\n",
        "        elif self.activation == 'tanh':\n",
        "            self.last_output = self.af.tanh(self.last_z)\n",
        "        elif self.activation == 'relu':\n",
        "            self.last_output = self.af.relu(self.last_z)\n",
        "        else:\n",
        "            self.last_output = self.last_z  # Linear\n",
        "        \n",
        "        return self.last_output\n",
        "    \n",
        "    def backward(self, error, learning_rate=0.01):\n",
        "        \"\"\"Update weights using gradient descent\"\"\"\n",
        "        # Compute gradient based on activation\n",
        "        if self.activation == 'sigmoid':\n",
        "            grad = error * self.af.sigmoid_derivative(self.last_z)\n",
        "        elif self.activation == 'tanh':\n",
        "            grad = error * self.af.tanh_derivative(self.last_z)\n",
        "        elif self.activation == 'relu':\n",
        "            grad = error * self.af.relu_derivative(self.last_z)\n",
        "        else:\n",
        "            grad = error\n",
        "        \n",
        "        # Update weights and bias\n",
        "        self.weights -= learning_rate * grad * self.last_input\n",
        "        self.bias -= learning_rate * grad\n",
        "        \n",
        "        return grad\n",
        "\n",
        "# Test single neuron\n",
        "neuron = Neuron(n_inputs=2, activation='sigmoid')\n",
        "test_input = np.array([1.0, 0.5])\n",
        "output = neuron.forward(test_input)\n",
        "\n",
        "print(f\"Neuron created with {len(neuron.weights)} weights\")\n",
        "print(f\"Weights: {neuron.weights}\")\n",
        "print(f\"Bias: {neuron.bias:.4f}\")\n",
        "print(f\"Input: {test_input}\")\n",
        "print(f\"Output: {output:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 🎯 Your Turn\n",
        "\n",
        "**Task:** Train a single neuron to learn the AND gate logic.\n",
        "\n",
        "AND gate truth table:\n",
        "- (0,0) → 0\n",
        "- (0,1) → 0  \n",
        "- (1,0) → 0\n",
        "- (1,1) → 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Train a neuron to learn AND gate\n",
        "# Hint: Use the Neuron class above with a training loop\n",
        "\n",
        "# AND gate data\n",
        "X_and = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y_and = np.array([0, 0, 0, 1])\n",
        "\n",
        "# Your code here:\n",
        "# and_neuron = Neuron(n_inputs=2, activation='sigmoid')\n",
        "# for epoch in range(1000):\n",
        "#     for x, target in zip(X_and, y_and):\n",
        "#         output = and_neuron.forward(x)\n",
        "#         error = target - output\n",
        "#         and_neuron.backward(error, learning_rate=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 2: Multi-Layer Perceptron Architecture\n",
        "\n",
        "Now let's build a complete MLP from scratch!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 4: Complete MLP Implementation\n",
        "\n",
        "#### 📚 Concept\n",
        "An MLP consists of:\n",
        "- **Input layer**: Receives raw features\n",
        "- **Hidden layers**: Learn representations through non-linear transformations\n",
        "- **Output layer**: Produces final predictions\n",
        "\n",
        "Information flows forward during prediction and gradients flow backward during training.\n",
        "\n",
        "#### 💻 Code Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MLP:\n",
        "    \"\"\"Multi-Layer Perceptron implementation from scratch\"\"\"\n",
        "    \n",
        "    def __init__(self, layer_sizes, activation='relu', output_activation='sigmoid'):\n",
        "        \"\"\"\n",
        "        Initialize MLP\n",
        "        layer_sizes: list of layer dimensions [input_size, hidden1, hidden2, ..., output_size]\n",
        "        \"\"\"\n",
        "        self.layer_sizes = layer_sizes\n",
        "        self.n_layers = len(layer_sizes)\n",
        "        self.activation = activation\n",
        "        self.output_activation = output_activation\n",
        "        self.af = ActivationFunctions()\n",
        "        \n",
        "        # Initialize weights and biases\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "        \n",
        "        for i in range(self.n_layers - 1):\n",
        "            # He initialization for ReLU, Xavier for others\n",
        "            if activation == 'relu':\n",
        "                w = np.random.randn(layer_sizes[i], layer_sizes[i+1]) * np.sqrt(2.0 / layer_sizes[i])\n",
        "            else:\n",
        "                w = np.random.randn(layer_sizes[i], layer_sizes[i+1]) * np.sqrt(1.0 / layer_sizes[i])\n",
        "            b = np.zeros((1, layer_sizes[i+1]))\n",
        "            \n",
        "            self.weights.append(w)\n",
        "            self.biases.append(b)\n",
        "        \n",
        "        # Storage for forward pass (needed for backprop)\n",
        "        self.activations = []\n",
        "        self.z_values = []\n",
        "    \n",
        "    def forward(self, X):\n",
        "        \"\"\"Forward propagation\"\"\"\n",
        "        self.activations = [X]\n",
        "        self.z_values = []\n",
        "        \n",
        "        current_input = X\n",
        "        \n",
        "        for i in range(self.n_layers - 1):\n",
        "            z = np.dot(current_input, self.weights[i]) + self.biases[i]\n",
        "            self.z_values.append(z)\n",
        "            \n",
        "            # Apply activation function\n",
        "            if i == self.n_layers - 2:  # Output layer\n",
        "                if self.output_activation == 'sigmoid':\n",
        "                    a = self.af.sigmoid(z)\n",
        "                elif self.output_activation == 'softmax':\n",
        "                    a = self.softmax(z)\n",
        "                else:\n",
        "                    a = z  # Linear\n",
        "            else:  # Hidden layers\n",
        "                if self.activation == 'relu':\n",
        "                    a = self.af.relu(z)\n",
        "                elif self.activation == 'tanh':\n",
        "                    a = self.af.tanh(z)\n",
        "                elif self.activation == 'sigmoid':\n",
        "                    a = self.af.sigmoid(z)\n",
        "                else:\n",
        "                    a = z\n",
        "            \n",
        "            self.activations.append(a)\n",
        "            current_input = a\n",
        "        \n",
        "        return self.activations[-1]\n",
        "    \n",
        "    def backward(self, X, y, learning_rate=0.01):\n",
        "        \"\"\"Backpropagation algorithm\"\"\"\n",
        "        m = X.shape[0]\n",
        "        \n",
        "        # Compute output layer gradient\n",
        "        delta = self.activations[-1] - y\n",
        "        \n",
        "        # Backpropagate through layers\n",
        "        for i in range(self.n_layers - 2, -1, -1):\n",
        "            # Compute gradients\n",
        "            dW = (1/m) * np.dot(self.activations[i].T, delta)\n",
        "            db = (1/m) * np.sum(delta, axis=0, keepdims=True)\n",
        "            \n",
        "            # Update weights and biases\n",
        "            self.weights[i] -= learning_rate * dW\n",
        "            self.biases[i] -= learning_rate * db\n",
        "            \n",
        "            # Compute delta for next layer\n",
        "            if i > 0:\n",
        "                delta = np.dot(delta, self.weights[i].T)\n",
        "                # Apply activation derivative\n",
        "                if self.activation == 'relu':\n",
        "                    delta *= self.af.relu_derivative(self.z_values[i-1])\n",
        "                elif self.activation == 'tanh':\n",
        "                    delta *= self.af.tanh_derivative(self.z_values[i-1])\n",
        "                elif self.activation == 'sigmoid':\n",
        "                    delta *= self.af.sigmoid_derivative(self.z_values[i-1])\n",
        "    \n",
        "    def train(self, X, y, epochs=100, learning_rate=0.01, verbose=True):\n",
        "        \"\"\"Train the network\"\"\"\n",
        "        losses = []\n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            # Forward pass\n",
        "            output = self.forward(X)\n",
        "            \n",
        "            # Compute loss (binary cross-entropy for binary classification)\n",
        "            loss = -np.mean(y * np.log(output + 1e-8) + (1 - y) * np.log(1 - output + 1e-8))\n",
        "            losses.append(loss)\n",
        "            \n",
        "            # Backward pass\n",
        "            self.backward(X, y, learning_rate)\n",
        "            \n",
        "            if verbose and epoch % 10 == 0:\n",
        "                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
        "        \n",
        "        return losses\n",
        "    \n",
        "    def predict(self, X):\n",
        "        \"\"\"Make predictions\"\"\"\n",
        "        output = self.forward(X)\n",
        "        return (output > 0.5).astype(int)\n",
        "    \n",
        "    def softmax(self, z):\n",
        "        \"\"\"Softmax activation for multi-class\"\"\"\n",
        "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
        "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
        "\n",
        "# Test MLP on XOR\n",
        "mlp_xor = MLP(layer_sizes=[2, 4, 1], activation='tanh', output_activation='sigmoid')\n",
        "print(f\"Created MLP with architecture: {mlp_xor.layer_sizes}\")\n",
        "print(f\"Number of parameters: {sum([w.size for w in mlp_xor.weights]) + sum([b.size for b in mlp_xor.biases])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 5: Solving XOR with MLP\n",
        "\n",
        "#### 📚 Concept\n",
        "Now we'll demonstrate that an MLP with hidden layers CAN solve the XOR problem that stumped our linear models.\n",
        "\n",
        "#### 💻 Code Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train MLP on XOR\n",
        "X_xor, y_xor = create_xor_data()\n",
        "y_xor = y_xor.reshape(-1, 1)\n",
        "\n",
        "# Create and train network\n",
        "mlp_xor = MLP(layer_sizes=[2, 4, 1], activation='tanh', output_activation='sigmoid')\n",
        "losses = mlp_xor.train(X_xor, y_xor, epochs=1000, learning_rate=0.5, verbose=False)\n",
        "\n",
        "# Make predictions\n",
        "predictions = mlp_xor.predict(X_xor)\n",
        "accuracy = np.mean(predictions == y_xor)\n",
        "\n",
        "print(f\"✅ MLP Accuracy on XOR: {accuracy:.2%}\")\n",
        "print(\"\\nPredictions:\")\n",
        "for i, (x, y_true, y_pred) in enumerate(zip(X_xor, y_xor.flatten(), predictions.flatten())):\n",
        "    print(f\"  Input: {x}, True: {y_true}, Predicted: {y_pred}\")\n",
        "\n",
        "# Plot training loss\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(losses, linewidth=2)\n",
        "plt.title('Training Loss', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Binary Cross-Entropy Loss')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Visualize decision boundary\n",
        "plt.subplot(1, 2, 2)\n",
        "x_min, x_max = -0.5, 1.5\n",
        "y_min, y_max = -0.5, 1.5\n",
        "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
        "                     np.linspace(y_min, y_max, 100))\n",
        "Z = mlp_xor.forward(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "plt.contourf(xx, yy, Z, levels=20, cmap='RdBu', alpha=0.6)\n",
        "plt.colorbar(label='Output')\n",
        "colors = ['red' if y == 0 else 'blue' for y in y_xor.flatten()]\n",
        "plt.scatter(X_xor[:, 0], X_xor[:, 1], c=colors, s=200, edgecolors='black', linewidth=2)\n",
        "plt.title('MLP Decision Boundary', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('x₁')\n",
        "plt.ylabel('x₂')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n🎉 SUCCESS! The MLP learned a non-linear decision boundary to solve XOR!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "### Exercise 6: Visualizing Hidden Layer Representations\n",
        "\n",
        "#### 📚 Concept\n",
        "Hidden layers learn to transform the input space into representations where the data becomes linearly separable. Let's visualize what the hidden layer learns.\n",
        "\n",
        "#### 💻 Code Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get hidden layer activations\n",
        "_ = mlp_xor.forward(X_xor)\n",
        "hidden_activations = mlp_xor.activations[1]  # First hidden layer\n",
        "\n",
        "print(\"Hidden Layer Activations (4 neurons):\")\n",
        "print(hidden_activations)\n",
        "print(f\"\\nShape: {hidden_activations.shape}\")\n",
        "\n",
        "# Create interactive 3D visualization with Plotly\n",
        "fig = make_subplots(\n",
        "    rows=1, cols=2,\n",
        "    subplot_titles=('Original XOR Space', 'Hidden Layer Space'),\n",
        "    specs=[[{'type': 'scatter'}, {'type': 'scatter3d'}]]\n",
        ")\n",
        "\n",
        "# Original 2D space\n",
        "colors_plotly = ['red' if y == 0 else 'blue' for y in y_xor.flatten()]\n",
        "fig.add_trace(\n",
        "    go.Scatter(x=X_xor[:, 0], y=X_xor[:, 1],\n",
        "               mode='markers+text',\n",
        "               marker=dict(size=15, color=colors_plotly, line=dict(width=2, color='black')),\n",
        "               text=[f'({int(x)},{int(y)})' for x, y in X_xor],\n",
        "               textposition='top center',\n",
        "               name='Original'),\n",
        "    row=1, col=1\n",
        ")\n",
        "\n",
        "# Hidden layer 3D space (using first 3 hidden neurons)\n",
        "if hidden_activations.shape[1] >= 3:\n",
        "    fig.add_trace(\n",
        "        go.Scatter3d(x=hidden_activations[:, 0],\n",
        "                     y=hidden_activations[:, 1],\n",
        "                     z=hidden_activations[:, 2],\n",
        "                     mode='markers+text',\n",
        "                     marker=dict(size=10, color=colors_plotly, line=dict(width=2, color='black')),\n",
        "                     text=[f'Point {i}' for i in range(4)],\n",
        "                     name='Hidden'),\n",
        "        row=1, col=2\n",
        "    )\n",
        "\n",
        "fig.update_layout(height=500, title_text=\"Feature Space Transformation\", showlegend=False)\n",
        "fig.update_xaxes(title_text=\"x₁\", row=1, col=1)\n",
        "fig.update_yaxes(title_text=\"x₂\", row=1, col=1)\n",
        "fig.show()\n",
        "\n",
        "print(\"\\n🔍 Notice how the hidden layer transforms the space!\")\n",
        "print(\"The points that were not linearly separable in 2D might become separable in the hidden layer space.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 3: Practical Applications\n",
        "\n",
        "Let's apply our MLP to more complex datasets!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 7: Non-linear Classification - Moons Dataset\n",
        "\n",
        "#### 📚 Concept\n",
        "Real-world data often has complex, non-linear patterns. The moons dataset is a classic example of interleaving crescent shapes that require non-linear decision boundaries.\n",
        "\n",
        "#### 💻 Code Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate moons dataset\n",
        "from sklearn.datasets import make_moons\n",
        "X_moons, y_moons = make_moons(n_samples=200, noise=0.2, random_state=42)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_moons, y_moons, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Visualize dataset\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='viridis', alpha=0.7, edgecolors='black')\n",
        "plt.title('Training Data', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.colorbar(label='Class')\n",
        "\n",
        "# Train MLP\n",
        "mlp_moons = MLP(layer_sizes=[2, 8, 8, 1], activation='relu', output_activation='sigmoid')\n",
        "y_train_reshaped = y_train.reshape(-1, 1)\n",
        "\n",
        "print(\"Training MLP on Moons dataset...\")\n",
        "losses = mlp_moons.train(X_train_scaled, y_train_reshaped, epochs=200, learning_rate=0.1, verbose=False)\n",
        "\n",
        "# Evaluate\n",
        "train_pred = mlp_moons.predict(X_train_scaled)\n",
        "test_pred = mlp_moons.predict(X_test_scaled)\n",
        "\n",
        "train_acc = np.mean(train_pred.flatten() == y_train)\n",
        "test_acc = np.mean(test_pred.flatten() == y_test)\n",
        "\n",
        "print(f\"\\nResults:\")\n",
        "print(f\"Training Accuracy: {train_acc:.2%}\")\n",
        "print(f\"Testing Accuracy: {test_acc:.2%}\")\n",
        "\n",
        "# Plot loss curve\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(losses, linewidth=2, color='orange')\n",
        "plt.title('Training Loss', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot decision boundary\n",
        "plt.subplot(1, 3, 3)\n",
        "x_min, x_max = X_train_scaled[:, 0].min() - 0.5, X_train_scaled[:, 0].max() + 0.5\n",
        "y_min, y_max = X_train_scaled[:, 1].min() - 0.5, X_train_scaled[:, 1].max() + 0.5\n",
        "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
        "                     np.linspace(y_min, y_max, 100))\n",
        "Z = mlp_moons.forward(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "plt.contourf(xx, yy, Z, levels=20, cmap='viridis', alpha=0.4)\n",
        "plt.scatter(X_train_scaled[:, 0], X_train_scaled[:, 1], c=y_train, \n",
        "           cmap='viridis', edgecolors='black', linewidth=1)\n",
        "plt.title('Decision Boundary', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Feature 1 (scaled)')\n",
        "plt.ylabel('Feature 2 (scaled)')\n",
        "plt.colorbar(label='Prediction')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 🎯 Your Turn\n",
        "\n",
        "**Task:** Experiment with different architectures and activation functions. Try:\n",
        "1. Changing the number of hidden layers (try [2, 16, 1] vs [2, 8, 8, 8, 1])\n",
        "2. Using different activation functions (relu vs tanh)\n",
        "3. Adjusting the learning rate\n",
        "\n",
        "What combination gives the best test accuracy?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Your experiments here\n",
        "# Try different architectures:\n",
        "# architectures = [\n",
        "#     [2, 16, 1],\n",
        "#     [2, 8, 8, 1],\n",
        "#     [2, 8, 8, 8, 1],\n",
        "#     [2, 32, 16, 8, 1]\n",
        "# ]\n",
        "\n",
        "# for arch in architectures:\n",
        "#     mlp = MLP(layer_sizes=arch, activation='relu')\n",
        "#     # Train and evaluate..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "### Exercise 8: Comparing Activation Functions\n",
        "\n",
        "#### 📚 Concept\n",
        "Different activation functions have different properties:\n",
        "- **Sigmoid**: Smooth, bounded [0,1], but suffers from vanishing gradients\n",
        "- **Tanh**: Zero-centered, bounded [-1,1], also vanishing gradients\n",
        "- **ReLU**: Fast, no vanishing gradients, but can have dead neurons\n",
        "\n",
        "#### 💻 Code Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare activation functions on circles dataset\n",
        "X_circles, y_circles = make_circles(n_samples=300, noise=0.1, factor=0.5, random_state=42)\n",
        "\n",
        "# Prepare data\n",
        "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(\n",
        "    X_circles, y_circles, test_size=0.3, random_state=42\n",
        ")\n",
        "scaler_c = StandardScaler()\n",
        "X_train_c_scaled = scaler_c.fit_transform(X_train_c)\n",
        "X_test_c_scaled = scaler_c.transform(X_test_c)\n",
        "y_train_c = y_train_c.reshape(-1, 1)\n",
        "y_test_c = y_test_c.reshape(-1, 1)\n",
        "\n",
        "# Test different activations\n",
        "activations = ['sigmoid', 'tanh', 'relu']\n",
        "results = {}\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "fig.suptitle('Activation Function Comparison on Circles Dataset', fontsize=16, fontweight='bold')\n",
        "\n",
        "for idx, activation in enumerate(activations):\n",
        "    print(f\"\\nTraining with {activation} activation...\")\n",
        "    \n",
        "    # Train model\n",
        "    mlp = MLP(layer_sizes=[2, 16, 16, 1], activation=activation, output_activation='sigmoid')\n",
        "    losses = mlp.train(X_train_c_scaled, y_train_c, epochs=200, learning_rate=0.1, verbose=False)\n",
        "    \n",
        "    # Evaluate\n",
        "    train_pred = mlp.predict(X_train_c_scaled)\n",
        "    test_pred = mlp.predict(X_test_c_scaled)\n",
        "    train_acc = np.mean(train_pred == y_train_c)\n",
        "    test_acc = np.mean(test_pred == y_test_c)\n",
        "    \n",
        "    results[activation] = {\n",
        "        'train_acc': train_acc,\n",
        "        'test_acc': test_acc,\n",
        "        'final_loss': losses[-1]\n",
        "    }\n",
        "    \n",
        "    # Plot loss curve\n",
        "    ax = axes[0, idx]\n",
        "    ax.plot(losses, linewidth=2, color=f'C{idx}')\n",
        "    ax.set_title(f'{activation.upper()} - Loss Curve', fontsize=12, fontweight='bold')\n",
        "    ax.set_xlabel('Epoch')\n",
        "    ax.set_ylabel('Loss')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.text(0.5, 0.95, f'Train Acc: {train_acc:.2%}\\nTest Acc: {test_acc:.2%}',\n",
        "            transform=ax.transAxes, verticalalignment='top',\n",
        "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "    \n",
        "    # Plot decision boundary\n",
        "    ax = axes[1, idx]\n",
        "    x_min, x_max = X_train_c_scaled[:, 0].min() - 0.5, X_train_c_scaled[:, 0].max() + 0.5\n",
        "    y_min, y_max = X_train_c_scaled[:, 1].min() - 0.5, X_train_c_scaled[:, 1].max() + 0.5\n",
        "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
        "                         np.linspace(y_min, y_max, 100))\n",
        "    Z = mlp.forward(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    \n",
        "    ax.contourf(xx, yy, Z, levels=20, cmap='coolwarm', alpha=0.4)\n",
        "    ax.scatter(X_train_c_scaled[:, 0], X_train_c_scaled[:, 1], \n",
        "              c=y_train_c.flatten(), cmap='coolwarm', edgecolors='black', linewidth=1)\n",
        "    ax.set_title(f'{activation.upper()} - Decision Boundary', fontsize=12, fontweight='bold')\n",
        "    ax.set_xlabel('Feature 1')\n",
        "    ax.set_ylabel('Feature 2')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Summary table\n",
        "results_df = pd.DataFrame(results).T\n",
        "results_df.columns = ['Train Accuracy', 'Test Accuracy', 'Final Loss']\n",
        "print(\"\\n📊 Results Summary:\")\n",
        "print(results_df.round(4))\n",
        "print(f\"\\n🏆 Best activation: {results_df['Test Accuracy'].idxmax()} with {results_df['Test Accuracy'].max():.2%} test accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 4: Advanced Topics\n",
        "\n",
        "Let's explore gradient flow and backpropagation visualization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 9: Understanding Gradient Flow\n",
        "\n",
        "#### 📚 Concept\n",
        "Backpropagation uses the chain rule to compute gradients layer by layer. Understanding gradient flow helps diagnose training problems like vanishing or exploding gradients.\n",
        "\n",
        "#### 💻 Code Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MLPWithGradientTracking(MLP):\n",
        "    \"\"\"Extended MLP that tracks gradients for visualization\"\"\"\n",
        "    \n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.gradient_history = []\n",
        "    \n",
        "    def backward(self, X, y, learning_rate=0.01):\n",
        "        \"\"\"Backpropagation with gradient tracking\"\"\"\n",
        "        m = X.shape[0]\n",
        "        gradients = []\n",
        "        \n",
        "        # Compute output layer gradient\n",
        "        delta = self.activations[-1] - y\n",
        "        \n",
        "        # Backpropagate through layers\n",
        "        for i in range(self.n_layers - 2, -1, -1):\n",
        "            # Compute gradients\n",
        "            dW = (1/m) * np.dot(self.activations[i].T, delta)\n",
        "            db = (1/m) * np.sum(delta, axis=0, keepdims=True)\n",
        "            \n",
        "            # Store gradient magnitudes\n",
        "            grad_norm = np.linalg.norm(dW)\n",
        "            gradients.append(grad_norm)\n",
        "            \n",
        "            # Update weights and biases\n",
        "            self.weights[i] -= learning_rate * dW\n",
        "            self.biases[i] -= learning_rate * db\n",
        "            \n",
        "            # Compute delta for next layer\n",
        "            if i > 0:\n",
        "                delta = np.dot(delta, self.weights[i].T)\n",
        "                if self.activation == 'relu':\n",
        "                    delta *= self.af.relu_derivative(self.z_values[i-1])\n",
        "                elif self.activation == 'tanh':\n",
        "                    delta *= self.af.tanh_derivative(self.z_values[i-1])\n",
        "                elif self.activation == 'sigmoid':\n",
        "                    delta *= self.af.sigmoid_derivative(self.z_values[i-1])\n",
        "        \n",
        "        self.gradient_history.append(gradients[::-1])  # Reverse to match layer order\n",
        "\n",
        "# Train network with gradient tracking\n",
        "print(\"Training network with gradient tracking...\")\n",
        "mlp_grad = MLPWithGradientTracking(layer_sizes=[2, 8, 8, 1], activation='sigmoid')\n",
        "X_sample, y_sample = make_moons(n_samples=100, noise=0.1, random_state=42)\n",
        "y_sample = y_sample.reshape(-1, 1)\n",
        "\n",
        "# Manual training loop to track gradients\n",
        "for epoch in range(100):\n",
        "    output = mlp_grad.forward(X_sample)\n",
        "    mlp_grad.backward(X_sample, y_sample, learning_rate=0.5)\n",
        "\n",
        "# Visualize gradient flow\n",
        "gradient_history = np.array(mlp_grad.gradient_history)\n",
        "\n",
        "plt.figure(figsize=(14, 5))\n",
        "\n",
        "# Plot 1: Gradient magnitude over time\n",
        "plt.subplot(1, 2, 1)\n",
        "for layer in range(gradient_history.shape[1]):\n",
        "    plt.plot(gradient_history[:, layer], label=f'Layer {layer+1}', linewidth=2)\n",
        "plt.title('Gradient Magnitude During Training', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Training Step')\n",
        "plt.ylabel('Gradient Norm')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.yscale('log')\n",
        "\n",
        "# Plot 2: Gradient heatmap\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(gradient_history.T, aspect='auto', cmap='viridis', interpolation='nearest')\n",
        "plt.colorbar(label='Gradient Magnitude')\n",
        "plt.title('Gradient Flow Heatmap', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Training Step')\n",
        "plt.ylabel('Layer')\n",
        "plt.yticks(range(gradient_history.shape[1]), [f'Layer {i+1}' for i in range(gradient_history.shape[1])])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n📊 Gradient Flow Analysis:\")\n",
        "print(f\"• Average gradient (first layer): {gradient_history[:, 0].mean():.6f}\")\n",
        "print(f\"• Average gradient (last layer): {gradient_history[:, -1].mean():.6f}\")\n",
        "print(f\"• Gradient ratio (last/first): {gradient_history[:, -1].mean() / gradient_history[:, 0].mean():.2f}\")\n",
        "print(\"\\n💡 Healthy gradient flow shows relatively consistent magnitudes across layers.\")\n",
        "print(\"   Vanishing gradients: earlier layers have much smaller gradients.\")\n",
        "print(\"   Exploding gradients: gradients grow exponentially through layers.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "### Exercise 10: Mini-batch Gradient Descent\n",
        "\n",
        "#### 📚 Concept\n",
        "Mini-batch gradient descent balances between:\n",
        "- **Batch GD**: Uses all data, stable but slow\n",
        "- **SGD**: Uses one sample, fast but noisy\n",
        "- **Mini-batch**: Uses small batches, good balance\n",
        "\n",
        "#### 💻 Code Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_with_minibatch(mlp, X, y, epochs=100, batch_size=32, learning_rate=0.01):\n",
        "    \"\"\"Train MLP using mini-batch gradient descent\"\"\"\n",
        "    n_samples = X.shape[0]\n",
        "    losses = []\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        # Shuffle data\n",
        "        indices = np.random.permutation(n_samples)\n",
        "        X_shuffled = X[indices]\n",
        "        y_shuffled = y[indices]\n",
        "        \n",
        "        epoch_losses = []\n",
        "        \n",
        "        # Mini-batch training\n",
        "        for start_idx in range(0, n_samples, batch_size):\n",
        "            end_idx = min(start_idx + batch_size, n_samples)\n",
        "            X_batch = X_shuffled[start_idx:end_idx]\n",
        "            y_batch = y_shuffled[start_idx:end_idx]\n",
        "            \n",
        "            # Forward and backward pass\n",
        "            output = mlp.forward(X_batch)\n",
        "            loss = -np.mean(y_batch * np.log(output + 1e-8) + \n",
        "                           (1 - y_batch) * np.log(1 - output + 1e-8))\n",
        "            epoch_losses.append(loss)\n",
        "            mlp.backward(X_batch, y_batch, learning_rate)\n",
        "        \n",
        "        losses.append(np.mean(epoch_losses))\n",
        "        \n",
        "        if epoch % 20 == 0:\n",
        "            print(f\"Epoch {epoch}, Loss: {losses[-1]:.4f}\")\n",
        "    \n",
        "    return losses\n",
        "\n",
        "# Compare batch sizes\n",
        "X_train_mb, y_train_mb = make_classification(n_samples=500, n_features=2, n_redundant=0,\n",
        "                                            n_informative=2, n_clusters_per_class=2,\n",
        "                                            random_state=42)\n",
        "y_train_mb = y_train_mb.reshape(-1, 1)\n",
        "\n",
        "batch_sizes = [1, 16, 32, 64, 500]  # 500 = full batch\n",
        "results = {}\n",
        "\n",
        "plt.figure(figsize=(15, 4))\n",
        "\n",
        "for idx, batch_size in enumerate(batch_sizes):\n",
        "    print(f\"\\nTraining with batch size: {batch_size}\")\n",
        "    \n",
        "    # Create new network for each batch size\n",
        "    mlp_mb = MLP(layer_sizes=[2, 16, 1], activation='relu')\n",
        "    \n",
        "    # Train\n",
        "    losses = train_with_minibatch(mlp_mb, X_train_mb, y_train_mb, \n",
        "                                 epochs=100, batch_size=batch_size, \n",
        "                                 learning_rate=0.01)\n",
        "    \n",
        "    results[f'Batch {batch_size}'] = losses\n",
        "    \n",
        "    # Plot\n",
        "    plt.subplot(1, 5, idx+1)\n",
        "    plt.plot(losses, linewidth=2)\n",
        "    plt.title(f'Batch Size: {batch_size}', fontsize=12, fontweight='bold')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.ylim([0, max(losses[10:]) * 1.1])  # Zoom in after initial epochs\n",
        "\n",
        "plt.suptitle('Effect of Batch Size on Training', fontsize=16, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n📊 Batch Size Comparison:\")\n",
        "print(\"• Batch Size 1 (SGD): Very noisy, fast updates\")\n",
        "print(\"• Batch Size 16-64: Good balance of speed and stability\")\n",
        "print(\"• Batch Size 500 (Full): Smooth but fewer updates per epoch\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 🎯 Your Turn\n",
        "\n",
        "**Task:** Implement learning rate scheduling - reduce the learning rate as training progresses.\n",
        "\n",
        "Common schedules:\n",
        "1. Step decay: lr = lr * 0.9 every 10 epochs\n",
        "2. Exponential decay: lr = lr * exp(-decay * epoch)\n",
        "3. 1/t decay: lr = lr / (1 + decay * epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Implement learning rate scheduling\n",
        "def train_with_lr_schedule(mlp, X, y, epochs=100, initial_lr=0.1, schedule='step'):\n",
        "    \"\"\"Train with learning rate scheduling\"\"\"\n",
        "    losses = []\n",
        "    lrs = []\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        # Calculate current learning rate\n",
        "        if schedule == 'step':\n",
        "            lr = initial_lr * (0.9 ** (epoch // 10))\n",
        "        elif schedule == 'exponential':\n",
        "            lr = initial_lr * np.exp(-0.01 * epoch)\n",
        "        elif schedule == '1/t':\n",
        "            lr = initial_lr / (1 + 0.01 * epoch)\n",
        "        else:\n",
        "            lr = initial_lr\n",
        "        \n",
        "        lrs.append(lr)\n",
        "        \n",
        "        # Your training code here\n",
        "        # output = mlp.forward(X)\n",
        "        # loss = ...\n",
        "        # mlp.backward(X, y, lr)\n",
        "        \n",
        "    return losses, lrs\n",
        "\n",
        "# Test your implementation\n",
        "# mlp_schedule = MLP([2, 16, 1])\n",
        "# losses, lrs = train_with_lr_schedule(mlp_schedule, X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 5: Summary and Final Exercises\n",
        "\n",
        "### 🎓 Key Takeaways\n",
        "\n",
        "1. **Why Neural Networks?**\n",
        "   - Linear models cannot solve XOR and other non-linearly separable problems\n",
        "   - Hidden layers learn feature transformations\n",
        "   - Multiple layers create hierarchical representations\n",
        "\n",
        "2. **Architecture Components:**\n",
        "   - **Neurons**: Basic computational units (weighted sum + activation)\n",
        "   - **Activation Functions**: Introduce non-linearity\n",
        "   - **Layers**: Transform representations progressively\n",
        "   - **Weights & Biases**: Learnable parameters\n",
        "\n",
        "3. **Learning Process:**\n",
        "   - **Forward Propagation**: Compute predictions layer by layer\n",
        "   - **Loss Function**: Measure prediction error\n",
        "   - **Backpropagation**: Compute gradients using chain rule\n",
        "   - **Gradient Descent**: Update weights to minimize loss\n",
        "\n",
        "4. **Practical Considerations:**\n",
        "   - **Initialization**: He/Xavier initialization for stable training\n",
        "   - **Activation Choice**: ReLU for hidden layers, sigmoid/softmax for output\n",
        "   - **Architecture Design**: Depth vs width tradeoff\n",
        "   - **Training Techniques**: Mini-batch, learning rate scheduling\n",
        "\n",
        "---\n",
        "\n",
        "### 📝 Final Exercise: Build Your Own Neural Network Library\n",
        "\n",
        "Create a complete neural network library with:\n",
        "1. Multiple activation functions\n",
        "2. Different weight initialization schemes\n",
        "3. Various optimizers (SGD, Momentum, Adam)\n",
        "4. Regularization (L1, L2, Dropout)\n",
        "5. Early stopping\n",
        "6. Model saving/loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your complete neural network library\n",
        "class NeuralNetwork:\n",
        "    \"\"\"Your enhanced neural network implementation\"\"\"\n",
        "    \n",
        "    def __init__(self, architecture, activation='relu', optimizer='sgd',\n",
        "                 regularization=None, reg_lambda=0.01):\n",
        "        \"\"\"Initialize your network\"\"\"\n",
        "        # TODO: Your implementation\n",
        "        pass\n",
        "    \n",
        "    def add_layer(self, units, activation=None):\n",
        "        \"\"\"Add a layer to the network\"\"\"\n",
        "        # TODO: Your implementation\n",
        "        pass\n",
        "    \n",
        "    def compile(self, loss='binary_crossentropy', metrics=['accuracy']):\n",
        "        \"\"\"Compile the model\"\"\"\n",
        "        # TODO: Your implementation\n",
        "        pass\n",
        "    \n",
        "    def fit(self, X, y, epochs=100, batch_size=32, validation_split=0.2,\n",
        "            callbacks=None):\n",
        "        \"\"\"Train the model\"\"\"\n",
        "        # TODO: Your implementation\n",
        "        pass\n",
        "    \n",
        "    def predict(self, X):\n",
        "        \"\"\"Make predictions\"\"\"\n",
        "        # TODO: Your implementation\n",
        "        pass\n",
        "    \n",
        "    def save(self, filepath):\n",
        "        \"\"\"Save model weights\"\"\"\n",
        "        # TODO: Your implementation\n",
        "        pass\n",
        "    \n",
        "    def load(self, filepath):\n",
        "        \"\"\"Load model weights\"\"\"\n",
        "        # TODO: Your implementation\n",
        "        pass\n",
        "\n",
        "print(\"🎉 Congratulations! You've completed the MLP tutorial!\")\n",
        "print(\"\\n📚 Next Steps:\")\n",
        "print(\"1. Implement the complete neural network library above\")\n",
        "print(\"2. Try on real datasets (MNIST, Fashion-MNIST)\")\n",
        "print(\"3. Explore convolutional neural networks (CNNs)\")\n",
        "print(\"4. Learn about recurrent neural networks (RNNs)\")\n",
        "print(\"5. Dive into modern architectures (Transformers, GANs)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "### 🔗 Resources for Further Learning\n",
        "\n",
        "1. **Books:**\n",
        "   - Deep Learning by Goodfellow, Bengio, and Courville\n",
        "   - Neural Networks and Deep Learning by Michael Nielsen\n",
        "\n",
        "2. **Courses:**\n",
        "   - Andrew Ng's Deep Learning Specialization\n",
        "   - Fast.ai Practical Deep Learning\n",
        "\n",
        "3. **Frameworks to Explore:**\n",
        "   - PyTorch\n",
        "   - TensorFlow/Keras\n",
        "   - JAX\n",
        "\n",
        "4. **Papers:**\n",
        "   - Backpropagation: Rumelhart et al. (1986)\n",
        "   - Universal Approximation: Cybenko (1989)\n",
        "   - Deep Learning Review: LeCun, Bengio, Hinton (2015)\n",
        "\n",
        "---\n",
        "\n",
        "**Thank you for learning with us!** 🚀\n",
        "\n",
        "**Author:** Ho-min Park  \n",
        "**Contact:** homin.park@ghent.ac.kr"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}