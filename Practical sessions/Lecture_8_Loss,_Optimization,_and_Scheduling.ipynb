{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üìö Lecture 8: Loss Functions, Optimization, and Learning Rate Scheduling\n",
        "\n",
        "## Interactive Hands-on Practice Notebook\n",
        "\n",
        "**Author**: Ho-min Park  \n",
        "**Contact**: homin.park@ghent.ac.kr\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Learning Objectives\n",
        "\n",
        "By the end of this notebook, you will be able to:\n",
        "1. Implement and compare different loss functions for regression and classification\n",
        "2. Build and visualize optimization algorithms from scratch\n",
        "3. Design and apply learning rate scheduling strategies\n",
        "4. Analyze the impact of different choices on model training\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Setup and Imports\n",
        "\n",
        "Let's start by importing all necessary libraries and setting up our environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import make_classification, make_regression, load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Deep learning libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Visualization settings\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette('husl')\n",
        "%matplotlib inline\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "print(\"‚úÖ All libraries imported successfully!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 2: Loss Function Design and Implementation\n",
        "\n",
        "In this section, we'll explore various loss functions and understand their characteristics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 1: Comparing Regression Loss Functions (MSE vs MAE vs Huber)\n",
        "\n",
        "#### üìñ Concept\n",
        "\n",
        "Different regression loss functions have different properties:\n",
        "- **MSE (L2 Loss)**: Penalizes large errors quadratically, sensitive to outliers\n",
        "- **MAE (L1 Loss)**: Linear penalty, robust to outliers\n",
        "- **Huber Loss**: Combines MSE for small errors and MAE for large errors\n",
        "\n",
        "#### üíª Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mse_loss(y_true, y_pred):\n",
        "    \"\"\"Mean Squared Error Loss\"\"\"\n",
        "    return np.mean((y_true - y_pred) ** 2)\n",
        "\n",
        "def mae_loss(y_true, y_pred):\n",
        "    \"\"\"Mean Absolute Error Loss\"\"\"\n",
        "    return np.mean(np.abs(y_true - y_pred))\n",
        "\n",
        "def huber_loss(y_true, y_pred, delta=1.0):\n",
        "    \"\"\"Huber Loss - combination of MSE and MAE\"\"\"\n",
        "    error = y_true - y_pred\n",
        "    is_small_error = np.abs(error) <= delta\n",
        "    \n",
        "    squared_loss = 0.5 * error ** 2\n",
        "    linear_loss = delta * np.abs(error) - 0.5 * delta ** 2\n",
        "    \n",
        "    return np.mean(np.where(is_small_error, squared_loss, linear_loss))\n",
        "\n",
        "# Generate sample data with outliers\n",
        "np.random.seed(42)\n",
        "X = np.linspace(-5, 5, 100)\n",
        "y_true = 2 * X + 1 + np.random.normal(0, 0.5, 100)\n",
        "# Add some outliers\n",
        "outlier_indices = np.random.choice(100, 10, replace=False)\n",
        "y_true[outlier_indices] += np.random.normal(0, 5, 10)\n",
        "\n",
        "# Create predictions with varying errors\n",
        "errors = np.linspace(-3, 3, 50)\n",
        "losses_mse = []\n",
        "losses_mae = []\n",
        "losses_huber = []\n",
        "\n",
        "for error in errors:\n",
        "    y_pred = y_true + error\n",
        "    losses_mse.append(mse_loss(y_true, y_pred))\n",
        "    losses_mae.append(mae_loss(y_true, y_pred))\n",
        "    losses_huber.append(huber_loss(y_true, y_pred, delta=1.5))\n",
        "\n",
        "# Visualization\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Plot 1: Loss comparison\n",
        "ax1.plot(errors, losses_mse, label='MSE', linewidth=2)\n",
        "ax1.plot(errors, losses_mae, label='MAE', linewidth=2)\n",
        "ax1.plot(errors, losses_huber, label='Huber (Œ¥=1.5)', linewidth=2)\n",
        "ax1.set_xlabel('Prediction Error', fontsize=12)\n",
        "ax1.set_ylabel('Loss Value', fontsize=12)\n",
        "ax1.set_title('Comparison of Regression Loss Functions', fontsize=14, fontweight='bold')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Data with outliers\n",
        "ax2.scatter(X[~np.isin(np.arange(100), outlier_indices)], \n",
        "           y_true[~np.isin(np.arange(100), outlier_indices)],\n",
        "           alpha=0.6, label='Normal points')\n",
        "ax2.scatter(X[outlier_indices], y_true[outlier_indices], \n",
        "           color='red', s=100, label='Outliers', marker='^')\n",
        "ax2.set_xlabel('X', fontsize=12)\n",
        "ax2.set_ylabel('y', fontsize=12)\n",
        "ax2.set_title('Data with Outliers', fontsize=14, fontweight='bold')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüìä Key Insights:\")\n",
        "print(\"‚Ä¢ MSE grows quadratically with error - very sensitive to outliers\")\n",
        "print(\"‚Ä¢ MAE grows linearly - more robust to outliers\")\n",
        "print(\"‚Ä¢ Huber combines best of both - quadratic for small errors, linear for large\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### üéØ Your Turn\n",
        "\n",
        "**Task**: Implement a custom weighted Huber loss where you can adjust the weight for outliers. Test it with different delta values and visualize how it affects the loss landscape.\n",
        "\n",
        "```python\n",
        "# TODO: Implement weighted_huber_loss(y_true, y_pred, delta, outlier_weight)\n",
        "# Hint: Multiply the linear part by outlier_weight\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "### Exercise 2: Classification Losses - Cross-Entropy vs Focal Loss\n",
        "\n",
        "#### üìñ Concept\n",
        "\n",
        "**Cross-Entropy Loss**: Standard loss for classification, but can struggle with class imbalance.  \n",
        "**Focal Loss**: Designed to address class imbalance by down-weighting easy examples and focusing on hard ones.\n",
        "\n",
        "Formula: `FL = -Œ±(1-p)^Œ≥ * log(p)`\n",
        "\n",
        "#### üíª Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "        \n",
        "    def forward(self, inputs, targets):\n",
        "        ce_loss = nn.functional.cross_entropy(inputs, targets, reduction='none')\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
        "        \n",
        "        if self.reduction == 'mean':\n",
        "            return focal_loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            return focal_loss.sum()\n",
        "        else:\n",
        "            return focal_loss\n",
        "\n",
        "# Create imbalanced dataset\n",
        "n_samples = 1000\n",
        "n_features = 2\n",
        "n_classes = 2\n",
        "\n",
        "# Generate imbalanced data (90% class 0, 10% class 1)\n",
        "X, y = make_classification(n_samples=n_samples, n_features=n_features,\n",
        "                          n_informative=2, n_redundant=0,\n",
        "                          n_clusters_per_class=1, weights=[0.9, 0.1],\n",
        "                          flip_y=0.01, random_state=42)\n",
        "\n",
        "# Convert to tensors\n",
        "X_tensor = torch.FloatTensor(X)\n",
        "y_tensor = torch.LongTensor(y)\n",
        "\n",
        "# Simple neural network\n",
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(2, 16)\n",
        "        self.fc2 = nn.Linear(16, 8)\n",
        "        self.fc3 = nn.Linear(8, 2)\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Train with different losses\n",
        "def train_with_loss(loss_fn, loss_name, epochs=100):\n",
        "    model = SimpleNet()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "    \n",
        "    losses = []\n",
        "    accuracies = []\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_tensor)\n",
        "        loss = loss_fn(outputs, y_tensor)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Calculate accuracy\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        accuracy = (predicted == y_tensor).float().mean()\n",
        "        \n",
        "        losses.append(loss.item())\n",
        "        accuracies.append(accuracy.item())\n",
        "    \n",
        "    # Calculate per-class accuracy\n",
        "    outputs = model(X_tensor)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    \n",
        "    class_0_acc = ((predicted == 0) & (y_tensor == 0)).sum().float() / (y_tensor == 0).sum()\n",
        "    class_1_acc = ((predicted == 1) & (y_tensor == 1)).sum().float() / (y_tensor == 1).sum()\n",
        "    \n",
        "    return losses, accuracies, class_0_acc.item(), class_1_acc.item()\n",
        "\n",
        "# Train with both losses\n",
        "ce_loss = nn.CrossEntropyLoss()\n",
        "focal_loss = FocalLoss(alpha=1, gamma=2)\n",
        "\n",
        "ce_losses, ce_accs, ce_c0_acc, ce_c1_acc = train_with_loss(ce_loss, 'Cross-Entropy')\n",
        "fl_losses, fl_accs, fl_c0_acc, fl_c1_acc = train_with_loss(focal_loss, 'Focal Loss')\n",
        "\n",
        "# Visualization\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# Loss curves\n",
        "axes[0, 0].plot(ce_losses, label='Cross-Entropy', linewidth=2)\n",
        "axes[0, 0].plot(fl_losses, label='Focal Loss', linewidth=2)\n",
        "axes[0, 0].set_xlabel('Epoch')\n",
        "axes[0, 0].set_ylabel('Loss')\n",
        "axes[0, 0].set_title('Training Loss Comparison', fontweight='bold')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Accuracy curves\n",
        "axes[0, 1].plot(ce_accs, label='Cross-Entropy', linewidth=2)\n",
        "axes[0, 1].plot(fl_accs, label='Focal Loss', linewidth=2)\n",
        "axes[0, 1].set_xlabel('Epoch')\n",
        "axes[0, 1].set_ylabel('Overall Accuracy')\n",
        "axes[0, 1].set_title('Overall Accuracy Comparison', fontweight='bold')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Class-wise accuracy\n",
        "classes = ['Majority\\n(90%)', 'Minority\\n(10%)']\n",
        "ce_class_accs = [ce_c0_acc, ce_c1_acc]\n",
        "fl_class_accs = [fl_c0_acc, fl_c1_acc]\n",
        "\n",
        "x = np.arange(len(classes))\n",
        "width = 0.35\n",
        "\n",
        "axes[1, 0].bar(x - width/2, ce_class_accs, width, label='Cross-Entropy', color='steelblue')\n",
        "axes[1, 0].bar(x + width/2, fl_class_accs, width, label='Focal Loss', color='coral')\n",
        "axes[1, 0].set_xlabel('Class')\n",
        "axes[1, 0].set_ylabel('Accuracy')\n",
        "axes[1, 0].set_title('Per-Class Accuracy', fontweight='bold')\n",
        "axes[1, 0].set_xticks(x)\n",
        "axes[1, 0].set_xticklabels(classes)\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Data distribution\n",
        "axes[1, 1].scatter(X[y==0, 0], X[y==0, 1], alpha=0.5, label=f'Class 0 (n={sum(y==0)})')\n",
        "axes[1, 1].scatter(X[y==1, 0], X[y==1, 1], alpha=0.8, label=f'Class 1 (n={sum(y==1)})', \n",
        "                  color='red', s=50)\n",
        "axes[1, 1].set_xlabel('Feature 1')\n",
        "axes[1, 1].set_ylabel('Feature 2')\n",
        "axes[1, 1].set_title('Imbalanced Dataset', fontweight='bold')\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüìä Key Insights:\")\n",
        "print(f\"‚Ä¢ Cross-Entropy - Majority class: {ce_c0_acc:.2%}, Minority class: {ce_c1_acc:.2%}\")\n",
        "print(f\"‚Ä¢ Focal Loss - Majority class: {fl_c0_acc:.2%}, Minority class: {fl_c1_acc:.2%}\")\n",
        "print(\"‚Ä¢ Focal Loss improves minority class performance significantly!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 3: Optimization Algorithms\n",
        "\n",
        "Let's implement and visualize different optimization algorithms to understand their behavior."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 3: Optimizer Comparison - SGD vs Momentum vs Adam\n",
        "\n",
        "#### üìñ Concept\n",
        "\n",
        "Different optimizers navigate the loss landscape differently:\n",
        "- **SGD**: Simple gradient descent, can get stuck in local minima\n",
        "- **Momentum**: Accelerates SGD by accumulating velocity\n",
        "- **Adam**: Adaptive learning rates with momentum\n",
        "\n",
        "#### üíª Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a complex loss landscape\n",
        "def create_loss_landscape():\n",
        "    x = np.linspace(-2, 2, 100)\n",
        "    y = np.linspace(-2, 2, 100)\n",
        "    X, Y = np.meshgrid(x, y)\n",
        "    \n",
        "    # Rosenbrock function (challenging optimization landscape)\n",
        "    a, b = 1, 100\n",
        "    Z = (a - X)**2 + b * (Y - X**2)**2\n",
        "    \n",
        "    return X, Y, Z\n",
        "\n",
        "def rosenbrock_gradient(x, y):\n",
        "    \"\"\"Gradient of Rosenbrock function\"\"\"\n",
        "    a, b = 1, 100\n",
        "    dx = -2 * (a - x) - 4 * b * x * (y - x**2)\n",
        "    dy = 2 * b * (y - x**2)\n",
        "    return np.array([dx, dy])\n",
        "\n",
        "# Implement optimizers from scratch\n",
        "class Optimizer:\n",
        "    def __init__(self, lr=0.01):\n",
        "        self.lr = lr\n",
        "        self.path = []\n",
        "    \n",
        "    def step(self, pos, grad):\n",
        "        raise NotImplementedError\n",
        "\n",
        "class SGD(Optimizer):\n",
        "    def step(self, pos, grad):\n",
        "        new_pos = pos - self.lr * grad\n",
        "        self.path.append(new_pos.copy())\n",
        "        return new_pos\n",
        "\n",
        "class Momentum(Optimizer):\n",
        "    def __init__(self, lr=0.01, momentum=0.9):\n",
        "        super().__init__(lr)\n",
        "        self.momentum = momentum\n",
        "        self.velocity = np.array([0.0, 0.0])\n",
        "    \n",
        "    def step(self, pos, grad):\n",
        "        self.velocity = self.momentum * self.velocity + grad\n",
        "        new_pos = pos - self.lr * self.velocity\n",
        "        self.path.append(new_pos.copy())\n",
        "        return new_pos\n",
        "\n",
        "class AdamSimple(Optimizer):\n",
        "    def __init__(self, lr=0.01, beta1=0.9, beta2=0.999, eps=1e-8):\n",
        "        super().__init__(lr)\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.eps = eps\n",
        "        self.m = np.array([0.0, 0.0])\n",
        "        self.v = np.array([0.0, 0.0])\n",
        "        self.t = 0\n",
        "    \n",
        "    def step(self, pos, grad):\n",
        "        self.t += 1\n",
        "        self.m = self.beta1 * self.m + (1 - self.beta1) * grad\n",
        "        self.v = self.beta2 * self.v + (1 - self.beta2) * grad**2\n",
        "        \n",
        "        # Bias correction\n",
        "        m_hat = self.m / (1 - self.beta1**self.t)\n",
        "        v_hat = self.v / (1 - self.beta2**self.t)\n",
        "        \n",
        "        new_pos = pos - self.lr * m_hat / (np.sqrt(v_hat) + self.eps)\n",
        "        self.path.append(new_pos.copy())\n",
        "        return new_pos\n",
        "\n",
        "# Run optimization\n",
        "def run_optimization(optimizer, start_pos, n_steps=100):\n",
        "    pos = start_pos.copy()\n",
        "    optimizer.path = [pos.copy()]\n",
        "    \n",
        "    for _ in range(n_steps):\n",
        "        grad = rosenbrock_gradient(pos[0], pos[1])\n",
        "        pos = optimizer.step(pos, grad)\n",
        "        \n",
        "        # Stop if converged\n",
        "        if np.linalg.norm(grad) < 1e-4:\n",
        "            break\n",
        "    \n",
        "    return np.array(optimizer.path)\n",
        "\n",
        "# Initialize optimizers\n",
        "start_pos = np.array([-1.5, -1.5])\n",
        "optimizers = {\n",
        "    'SGD': SGD(lr=0.001),\n",
        "    'Momentum': Momentum(lr=0.001, momentum=0.9),\n",
        "    'Adam': AdamSimple(lr=0.01)\n",
        "}\n",
        "\n",
        "paths = {}\n",
        "for name, opt in optimizers.items():\n",
        "    paths[name] = run_optimization(opt, start_pos, n_steps=500)\n",
        "\n",
        "# Visualization\n",
        "X, Y, Z = create_loss_landscape()\n",
        "\n",
        "fig = plt.figure(figsize=(15, 5))\n",
        "\n",
        "for i, (name, path) in enumerate(paths.items()):\n",
        "    ax = fig.add_subplot(1, 3, i+1)\n",
        "    \n",
        "    # Plot contour\n",
        "    contour = ax.contour(X, Y, Z, levels=np.logspace(-1, 3, 20), alpha=0.5)\n",
        "    \n",
        "    # Plot optimization path\n",
        "    ax.plot(path[:, 0], path[:, 1], 'r-', linewidth=2, alpha=0.8)\n",
        "    ax.plot(path[0, 0], path[0, 1], 'go', markersize=10, label='Start')\n",
        "    ax.plot(path[-1, 0], path[-1, 1], 'r*', markersize=15, label='End')\n",
        "    \n",
        "    ax.set_xlabel('X')\n",
        "    ax.set_ylabel('Y')\n",
        "    ax.set_title(f'{name} Optimization Path\\n({len(path)} steps)', fontweight='bold')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.set_xlim(-2, 2)\n",
        "    ax.set_ylim(-2, 2)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüìä Optimization Results:\")\n",
        "for name, path in paths.items():\n",
        "    final_pos = path[-1]\n",
        "    final_loss = (1 - final_pos[0])**2 + 100 * (final_pos[1] - final_pos[0]**2)**2\n",
        "    print(f\"{name:10} - Steps: {len(path):3d}, Final loss: {final_loss:.6f}, \"\n",
        "          f\"Final position: ({final_pos[0]:.3f}, {final_pos[1]:.3f})\")\n",
        "\n",
        "print(\"\\nüí° Key Insights:\")\n",
        "print(\"‚Ä¢ SGD struggles with the narrow valley and takes many steps\")\n",
        "print(\"‚Ä¢ Momentum accelerates through the valley more efficiently\")\n",
        "print(\"‚Ä¢ Adam adapts its learning rate and navigates most efficiently\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "### Exercise 4: Learning Rate Impact Analysis\n",
        "\n",
        "#### üìñ Concept\n",
        "\n",
        "The learning rate is arguably the most important hyperparameter:\n",
        "- **Too large**: Divergence, oscillation\n",
        "- **Too small**: Slow convergence, stuck in local minima\n",
        "- **Just right**: Efficient convergence to good minima\n",
        "\n",
        "#### üíª Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test different learning rates on a simple problem\n",
        "def train_with_lr(lr, X, y, epochs=200):\n",
        "    torch.manual_seed(42)\n",
        "    \n",
        "    # Simple linear model\n",
        "    model = nn.Sequential(\n",
        "        nn.Linear(X.shape[1], 32),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(32, 16),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(16, 1)\n",
        "    )\n",
        "    \n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "    \n",
        "    losses = []\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X)\n",
        "        loss = criterion(outputs, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        losses.append(loss.item())\n",
        "        \n",
        "        # Check for divergence\n",
        "        if loss.item() > 1e6 or np.isnan(loss.item()):\n",
        "            losses.extend([np.nan] * (epochs - epoch - 1))\n",
        "            break\n",
        "    \n",
        "    return losses\n",
        "\n",
        "# Generate regression data\n",
        "X_reg, y_reg = make_regression(n_samples=500, n_features=10, noise=10, random_state=42)\n",
        "X_reg = torch.FloatTensor(StandardScaler().fit_transform(X_reg))\n",
        "y_reg = torch.FloatTensor(y_reg.reshape(-1, 1))\n",
        "\n",
        "# Test different learning rates\n",
        "learning_rates = [0.0001, 0.001, 0.01, 0.1, 0.5]\n",
        "all_losses = {}\n",
        "\n",
        "for lr in learning_rates:\n",
        "    losses = train_with_lr(lr, X_reg, y_reg)\n",
        "    all_losses[lr] = losses\n",
        "\n",
        "# Visualization\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, lr in enumerate(learning_rates):\n",
        "    ax = axes[i]\n",
        "    losses = all_losses[lr]\n",
        "    \n",
        "    if not any(np.isnan(losses)):\n",
        "        ax.plot(losses, linewidth=2, color='steelblue')\n",
        "        ax.set_title(f'LR = {lr}\\nFinal Loss: {losses[-1]:.4f}', fontweight='bold')\n",
        "    else:\n",
        "        valid_losses = [l for l in losses if not np.isnan(l)]\n",
        "        ax.plot(valid_losses, linewidth=2, color='red')\n",
        "        ax.set_title(f'LR = {lr}\\n‚ö†Ô∏è DIVERGED!', fontweight='bold', color='red')\n",
        "    \n",
        "    ax.set_xlabel('Epoch')\n",
        "    ax.set_ylabel('Loss')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.set_yscale('log')\n",
        "\n",
        "# Summary plot\n",
        "ax = axes[5]\n",
        "for lr in learning_rates:\n",
        "    losses = all_losses[lr]\n",
        "    if not any(np.isnan(losses)):\n",
        "        ax.plot(losses, label=f'LR={lr}', linewidth=2, alpha=0.7)\n",
        "\n",
        "ax.set_xlabel('Epoch')\n",
        "ax.set_ylabel('Loss (log scale)')\n",
        "ax.set_title('All Learning Rates Comparison', fontweight='bold')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "ax.set_yscale('log')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüìä Learning Rate Analysis:\")\n",
        "for lr in learning_rates:\n",
        "    losses = all_losses[lr]\n",
        "    if not any(np.isnan(losses)):\n",
        "        print(f\"LR {lr:6.4f}: Final loss = {losses[-1]:8.4f}, \"\n",
        "              f\"Converged in ~{np.argmin(np.gradient(losses[:100]))} epochs\")\n",
        "    else:\n",
        "        print(f\"LR {lr:6.4f}: ‚ùå DIVERGED\")\n",
        "\n",
        "print(\"\\nüí° Key Insights:\")\n",
        "print(\"‚Ä¢ Too small LR (0.0001): Very slow convergence\")\n",
        "print(\"‚Ä¢ Optimal LR (0.01): Fast and stable convergence\")\n",
        "print(\"‚Ä¢ Too large LR (0.5): Training diverges immediately\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 4: Learning Rate Scheduling Strategies\n",
        "\n",
        "Adaptive learning rates can significantly improve training efficiency and final performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 5: Learning Rate Scheduling Comparison\n",
        "\n",
        "#### üìñ Concept\n",
        "\n",
        "Different scheduling strategies:\n",
        "- **Step Decay**: Reduce by factor at specific epochs\n",
        "- **Exponential Decay**: Smooth exponential reduction\n",
        "- **Cosine Annealing**: Cosine-shaped reduction\n",
        "- **Warm-up + Linear**: Start small, increase, then decrease\n",
        "\n",
        "#### üíª Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LRScheduler:\n",
        "    def __init__(self, initial_lr, total_epochs):\n",
        "        self.initial_lr = initial_lr\n",
        "        self.total_epochs = total_epochs\n",
        "        self.history = []\n",
        "    \n",
        "    def get_lr(self, epoch):\n",
        "        raise NotImplementedError\n",
        "    \n",
        "    def step(self, epoch):\n",
        "        lr = self.get_lr(epoch)\n",
        "        self.history.append(lr)\n",
        "        return lr\n",
        "\n",
        "class StepDecayScheduler(LRScheduler):\n",
        "    def __init__(self, initial_lr, total_epochs, step_size=30, gamma=0.1):\n",
        "        super().__init__(initial_lr, total_epochs)\n",
        "        self.step_size = step_size\n",
        "        self.gamma = gamma\n",
        "    \n",
        "    def get_lr(self, epoch):\n",
        "        return self.initial_lr * (self.gamma ** (epoch // self.step_size))\n",
        "\n",
        "class ExponentialDecayScheduler(LRScheduler):\n",
        "    def __init__(self, initial_lr, total_epochs, decay_rate=0.96):\n",
        "        super().__init__(initial_lr, total_epochs)\n",
        "        self.decay_rate = decay_rate\n",
        "    \n",
        "    def get_lr(self, epoch):\n",
        "        return self.initial_lr * (self.decay_rate ** epoch)\n",
        "\n",
        "class CosineAnnealingScheduler(LRScheduler):\n",
        "    def __init__(self, initial_lr, total_epochs, min_lr=0.00001):\n",
        "        super().__init__(initial_lr, total_epochs)\n",
        "        self.min_lr = min_lr\n",
        "    \n",
        "    def get_lr(self, epoch):\n",
        "        return self.min_lr + (self.initial_lr - self.min_lr) * \\\n",
        "               (1 + np.cos(np.pi * epoch / self.total_epochs)) / 2\n",
        "\n",
        "class WarmupLinearScheduler(LRScheduler):\n",
        "    def __init__(self, initial_lr, total_epochs, warmup_epochs=10):\n",
        "        super().__init__(initial_lr, total_epochs)\n",
        "        self.warmup_epochs = warmup_epochs\n",
        "    \n",
        "    def get_lr(self, epoch):\n",
        "        if epoch < self.warmup_epochs:\n",
        "            # Warmup phase\n",
        "            return self.initial_lr * (epoch + 1) / self.warmup_epochs\n",
        "        else:\n",
        "            # Linear decay phase\n",
        "            progress = (epoch - self.warmup_epochs) / (self.total_epochs - self.warmup_epochs)\n",
        "            return self.initial_lr * (1 - progress)\n",
        "\n",
        "class CyclicalScheduler(LRScheduler):\n",
        "    def __init__(self, initial_lr, total_epochs, min_lr=0.0001, cycle_length=20):\n",
        "        super().__init__(initial_lr, total_epochs)\n",
        "        self.min_lr = min_lr\n",
        "        self.cycle_length = cycle_length\n",
        "    \n",
        "    def get_lr(self, epoch):\n",
        "        cycle_progress = (epoch % self.cycle_length) / self.cycle_length\n",
        "        if cycle_progress < 0.5:\n",
        "            # Increasing phase\n",
        "            return self.min_lr + (self.initial_lr - self.min_lr) * (2 * cycle_progress)\n",
        "        else:\n",
        "            # Decreasing phase\n",
        "            return self.initial_lr - (self.initial_lr - self.min_lr) * (2 * (cycle_progress - 0.5))\n",
        "\n",
        "# Initialize schedulers\n",
        "total_epochs = 150\n",
        "initial_lr = 0.1\n",
        "\n",
        "schedulers = {\n",
        "    'Constant': LRScheduler(initial_lr, total_epochs),  # Base class acts as constant\n",
        "    'Step Decay': StepDecayScheduler(initial_lr, total_epochs, step_size=50),\n",
        "    'Exponential': ExponentialDecayScheduler(initial_lr, total_epochs, decay_rate=0.97),\n",
        "    'Cosine': CosineAnnealingScheduler(initial_lr, total_epochs),\n",
        "    'Warmup-Linear': WarmupLinearScheduler(initial_lr, total_epochs, warmup_epochs=20),\n",
        "    'Cyclical': CyclicalScheduler(initial_lr, total_epochs, cycle_length=30)\n",
        "}\n",
        "\n",
        "# For constant scheduler, override get_lr\n",
        "schedulers['Constant'].get_lr = lambda epoch: initial_lr\n",
        "\n",
        "# Generate learning rate schedules\n",
        "for name, scheduler in schedulers.items():\n",
        "    for epoch in range(total_epochs):\n",
        "        scheduler.step(epoch)\n",
        "\n",
        "# Visualization\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "colors = ['gray', 'blue', 'green', 'red', 'purple', 'orange']\n",
        "\n",
        "for i, (name, scheduler) in enumerate(schedulers.items()):\n",
        "    ax = axes[i]\n",
        "    ax.plot(scheduler.history, linewidth=2.5, color=colors[i])\n",
        "    ax.set_xlabel('Epoch', fontsize=11)\n",
        "    ax.set_ylabel('Learning Rate', fontsize=11)\n",
        "    ax.set_title(f'{name} Schedule', fontweight='bold', fontsize=12)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Add annotations for key features\n",
        "    if name == 'Step Decay':\n",
        "        for step in range(50, total_epochs, 50):\n",
        "            ax.axvline(x=step, color='red', linestyle='--', alpha=0.3)\n",
        "    elif name == 'Warmup-Linear':\n",
        "        ax.axvline(x=20, color='red', linestyle='--', alpha=0.3)\n",
        "        ax.text(20, ax.get_ylim()[1]*0.9, 'Warmup End', rotation=90, \n",
        "               verticalalignment='top', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüìä Learning Rate Schedule Statistics:\")\n",
        "print(\"-\" * 60)\n",
        "for name, scheduler in schedulers.items():\n",
        "    lr_values = scheduler.history\n",
        "    print(f\"{name:15} | Initial: {lr_values[0]:.4f} | \"\n",
        "          f\"Final: {lr_values[-1]:.4f} | \"\n",
        "          f\"Mean: {np.mean(lr_values):.4f}\")\n",
        "\n",
        "print(\"\\nüí° Key Insights:\")\n",
        "print(\"‚Ä¢ Step Decay: Simple and effective, sudden drops can cause loss spikes\")\n",
        "print(\"‚Ä¢ Exponential: Smooth decay, may decrease too quickly\")\n",
        "print(\"‚Ä¢ Cosine: Smooth with slow start and end, good for fine-tuning\")\n",
        "print(\"‚Ä¢ Warmup: Prevents instability with large initial learning rates\")\n",
        "print(\"‚Ä¢ Cyclical: Helps escape local minima through exploration\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "### Exercise 6: Metric Learning - Contrastive and Triplet Loss\n",
        "\n",
        "#### üìñ Concept\n",
        "\n",
        "Metric learning losses learn embeddings where:\n",
        "- **Contrastive Loss**: Similar pairs are close, dissimilar pairs are far\n",
        "- **Triplet Loss**: Anchor is closer to positive than negative by a margin\n",
        "\n",
        "#### üíª Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ContrastiveLoss(nn.Module):\n",
        "    def __init__(self, margin=1.0):\n",
        "        super(ContrastiveLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "    \n",
        "    def forward(self, output1, output2, label):\n",
        "        \"\"\"label=1 for similar, label=0 for dissimilar\"\"\"\n",
        "        distance = torch.nn.functional.pairwise_distance(output1, output2)\n",
        "        loss = label * distance.pow(2) + \\\n",
        "               (1 - label) * torch.clamp(self.margin - distance, min=0).pow(2)\n",
        "        return loss.mean()\n",
        "\n",
        "class TripletLoss(nn.Module):\n",
        "    def __init__(self, margin=1.0):\n",
        "        super(TripletLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "    \n",
        "    def forward(self, anchor, positive, negative):\n",
        "        pos_distance = torch.nn.functional.pairwise_distance(anchor, positive)\n",
        "        neg_distance = torch.nn.functional.pairwise_distance(anchor, negative)\n",
        "        loss = torch.clamp(pos_distance - neg_distance + self.margin, min=0)\n",
        "        return loss.mean()\n",
        "\n",
        "# Create embedding network\n",
        "class EmbeddingNet(nn.Module):\n",
        "    def __init__(self, input_dim=784, embedding_dim=32):\n",
        "        super(EmbeddingNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, embedding_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Generate synthetic data for demonstration\n",
        "np.random.seed(42)\n",
        "n_samples = 1000\n",
        "n_features = 50\n",
        "n_classes = 5\n",
        "embedding_dim = 2  # 2D for visualization\n",
        "\n",
        "# Create clustered data\n",
        "X_metric = []\n",
        "y_metric = []\n",
        "for class_id in range(n_classes):\n",
        "    center = np.random.randn(n_features) * 5\n",
        "    samples = center + np.random.randn(n_samples // n_classes, n_features)\n",
        "    X_metric.append(samples)\n",
        "    y_metric.extend([class_id] * (n_samples // n_classes))\n",
        "\n",
        "X_metric = np.vstack(X_metric)\n",
        "X_metric = torch.FloatTensor(X_metric)\n",
        "y_metric = torch.LongTensor(y_metric)\n",
        "\n",
        "# Train with contrastive loss\n",
        "model = EmbeddingNet(input_dim=n_features, embedding_dim=embedding_dim)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "contrastive_loss = ContrastiveLoss(margin=2.0)\n",
        "\n",
        "# Training loop for contrastive loss\n",
        "n_epochs = 50\n",
        "model.train()\n",
        "for epoch in range(n_epochs):\n",
        "    # Sample pairs\n",
        "    indices = torch.randperm(len(X_metric))\n",
        "    \n",
        "    total_loss = 0\n",
        "    for i in range(0, len(indices)-1, 2):\n",
        "        idx1, idx2 = indices[i], indices[i+1]\n",
        "        \n",
        "        x1, x2 = X_metric[idx1:idx1+1], X_metric[idx2:idx2+1]\n",
        "        y1, y2 = y_metric[idx1], y_metric[idx2]\n",
        "        \n",
        "        # Label: 1 if same class, 0 if different\n",
        "        label = torch.FloatTensor([1.0 if y1 == y2 else 0.0])\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        embed1 = model(x1)\n",
        "        embed2 = model(x2)\n",
        "        loss = contrastive_loss(embed1, embed2, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "\n",
        "# Get embeddings\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    embeddings = model(X_metric).numpy()\n",
        "\n",
        "# Visualization\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "# Original high-dimensional data (PCA projection)\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_metric.numpy())\n",
        "\n",
        "ax = axes[0]\n",
        "for class_id in range(n_classes):\n",
        "    mask = y_metric.numpy() == class_id\n",
        "    ax.scatter(X_pca[mask, 0], X_pca[mask, 1], \n",
        "              label=f'Class {class_id}', alpha=0.6, s=30)\n",
        "ax.set_xlabel('PCA Component 1')\n",
        "ax.set_ylabel('PCA Component 2')\n",
        "ax.set_title('Original Data (PCA Projection)', fontweight='bold')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Learned embeddings\n",
        "ax = axes[1]\n",
        "for class_id in range(n_classes):\n",
        "    mask = y_metric.numpy() == class_id\n",
        "    ax.scatter(embeddings[mask, 0], embeddings[mask, 1], \n",
        "              label=f'Class {class_id}', alpha=0.6, s=30)\n",
        "ax.set_xlabel('Embedding Dimension 1')\n",
        "ax.set_ylabel('Embedding Dimension 2')\n",
        "ax.set_title('Learned Embeddings (Contrastive Loss)', fontweight='bold')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Calculate inter and intra class distances\n",
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "intra_distances = []\n",
        "inter_distances = []\n",
        "\n",
        "for i in range(n_classes):\n",
        "    class_embeddings = embeddings[y_metric.numpy() == i]\n",
        "    # Intra-class distances\n",
        "    if len(class_embeddings) > 1:\n",
        "        intra_dist = cdist(class_embeddings, class_embeddings)\n",
        "        intra_distances.extend(intra_dist[np.triu_indices_from(intra_dist, k=1)])\n",
        "    \n",
        "    # Inter-class distances\n",
        "    for j in range(i+1, n_classes):\n",
        "        other_embeddings = embeddings[y_metric.numpy() == j]\n",
        "        inter_dist = cdist(class_embeddings, other_embeddings)\n",
        "        inter_distances.extend(inter_dist.flatten())\n",
        "\n",
        "print(\"\\nüìä Embedding Quality Metrics:\")\n",
        "print(f\"Average intra-class distance: {np.mean(intra_distances):.3f}\")\n",
        "print(f\"Average inter-class distance: {np.mean(inter_distances):.3f}\")\n",
        "print(f\"Separation ratio: {np.mean(inter_distances) / np.mean(intra_distances):.2f}\")\n",
        "\n",
        "print(\"\\nüí° Key Insights:\")\n",
        "print(\"‚Ä¢ Contrastive loss successfully separates different classes\")\n",
        "print(\"‚Ä¢ Similar samples are pulled together in embedding space\")\n",
        "print(\"‚Ä¢ Dissimilar samples are pushed apart by the margin\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "### Exercise 7: Advanced Optimizer Analysis - AdaGrad, RMSprop, AdamW\n",
        "\n",
        "#### üìñ Concept\n",
        "\n",
        "Adaptive optimizers adjust learning rates per parameter:\n",
        "- **AdaGrad**: Accumulates squared gradients (can stop learning)\n",
        "- **RMSprop**: Uses exponential moving average (fixes AdaGrad's issue)\n",
        "- **AdamW**: Adam with decoupled weight decay (better generalization)\n",
        "\n",
        "#### üíª Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare adaptive optimizers on a challenging problem\n",
        "def create_noisy_classification_problem():\n",
        "    \"\"\"Create a classification problem with noise and class imbalance\"\"\"\n",
        "    X, y = make_classification(n_samples=2000, n_features=20, n_informative=15,\n",
        "                              n_redundant=5, n_classes=3, n_clusters_per_class=2,\n",
        "                              weights=[0.5, 0.3, 0.2], flip_y=0.1, random_state=42)\n",
        "    return torch.FloatTensor(X), torch.LongTensor(y)\n",
        "\n",
        "X_opt, y_opt = create_noisy_classification_problem()\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_opt, y_opt, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Define model\n",
        "class ClassifierNet(nn.Module):\n",
        "    def __init__(self, input_dim=20, n_classes=3):\n",
        "        super(ClassifierNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 64)\n",
        "        self.fc2 = nn.Linear(64, 32)\n",
        "        self.fc3 = nn.Linear(32, 16)\n",
        "        self.fc4 = nn.Linear(16, n_classes)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "\n",
        "def train_model(optimizer_name, optimizer_class, **optimizer_kwargs):\n",
        "    torch.manual_seed(42)\n",
        "    model = ClassifierNet()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optimizer_class(model.parameters(), **optimizer_kwargs)\n",
        "    \n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "    train_accs = []\n",
        "    test_accs = []\n",
        "    \n",
        "    n_epochs = 100\n",
        "    for epoch in range(n_epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_train)\n",
        "        loss = criterion(outputs, y_train)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Evaluation\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            # Training metrics\n",
        "            train_outputs = model(X_train)\n",
        "            train_loss = criterion(train_outputs, y_train)\n",
        "            _, train_predicted = torch.max(train_outputs, 1)\n",
        "            train_acc = (train_predicted == y_train).float().mean()\n",
        "            \n",
        "            # Test metrics\n",
        "            test_outputs = model(X_test)\n",
        "            test_loss = criterion(test_outputs, y_test)\n",
        "            _, test_predicted = torch.max(test_outputs, 1)\n",
        "            test_acc = (test_predicted == y_test).float().mean()\n",
        "        \n",
        "        train_losses.append(train_loss.item())\n",
        "        test_losses.append(test_loss.item())\n",
        "        train_accs.append(train_acc.item())\n",
        "        test_accs.append(test_acc.item())\n",
        "    \n",
        "    return train_losses, test_losses, train_accs, test_accs\n",
        "\n",
        "# Compare optimizers\n",
        "optimizers_config = {\n",
        "    'SGD': (optim.SGD, {'lr': 0.01, 'momentum': 0.9}),\n",
        "    'AdaGrad': (optim.Adagrad, {'lr': 0.01}),\n",
        "    'RMSprop': (optim.RMSprop, {'lr': 0.001}),\n",
        "    'Adam': (optim.Adam, {'lr': 0.001}),\n",
        "    'AdamW': (optim.AdamW, {'lr': 0.001, 'weight_decay': 0.01})\n",
        "}\n",
        "\n",
        "results = {}\n",
        "for name, (opt_class, kwargs) in optimizers_config.items():\n",
        "    print(f\"Training with {name}...\")\n",
        "    results[name] = train_model(name, opt_class, **kwargs)\n",
        "\n",
        "# Visualization\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# Training loss\n",
        "ax = axes[0, 0]\n",
        "for name, (train_losses, _, _, _) in results.items():\n",
        "    ax.plot(train_losses, label=name, linewidth=2, alpha=0.8)\n",
        "ax.set_xlabel('Epoch')\n",
        "ax.set_ylabel('Loss')\n",
        "ax.set_title('Training Loss', fontweight='bold')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Test loss\n",
        "ax = axes[0, 1]\n",
        "for name, (_, test_losses, _, _) in results.items():\n",
        "    ax.plot(test_losses, label=name, linewidth=2, alpha=0.8)\n",
        "ax.set_xlabel('Epoch')\n",
        "ax.set_ylabel('Loss')\n",
        "ax.set_title('Test Loss', fontweight='bold')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Training accuracy\n",
        "ax = axes[1, 0]\n",
        "for name, (_, _, train_accs, _) in results.items():\n",
        "    ax.plot(train_accs, label=name, linewidth=2, alpha=0.8)\n",
        "ax.set_xlabel('Epoch')\n",
        "ax.set_ylabel('Accuracy')\n",
        "ax.set_title('Training Accuracy', fontweight='bold')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Test accuracy\n",
        "ax = axes[1, 1]\n",
        "for name, (_, _, _, test_accs) in results.items():\n",
        "    ax.plot(test_accs, label=name, linewidth=2, alpha=0.8)\n",
        "ax.set_xlabel('Epoch')\n",
        "ax.set_ylabel('Accuracy')\n",
        "ax.set_title('Test Accuracy', fontweight='bold')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Final performance comparison\n",
        "print(\"\\nüìä Final Performance Comparison:\")\n",
        "print(\"-\" * 70)\n",
        "print(f\"{'Optimizer':<12} | {'Train Loss':>10} | {'Test Loss':>10} | \"\n",
        "      f\"{'Train Acc':>10} | {'Test Acc':>10}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "for name, (train_losses, test_losses, train_accs, test_accs) in results.items():\n",
        "    print(f\"{name:<12} | {train_losses[-1]:>10.4f} | {test_losses[-1]:>10.4f} | \"\n",
        "          f\"{train_accs[-1]:>10.2%} | {test_accs[-1]:>10.2%}\")\n",
        "\n",
        "print(\"\\nüí° Key Insights:\")\n",
        "print(\"‚Ä¢ AdaGrad: Learning rate may decrease too quickly, causing early stopping\")\n",
        "print(\"‚Ä¢ RMSprop: Fixes AdaGrad's diminishing learning rate problem\")\n",
        "print(\"‚Ä¢ Adam: Combines momentum and adaptive learning rates effectively\")\n",
        "print(\"‚Ä¢ AdamW: Weight decay decoupling often improves generalization\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "### Exercise 8: Complete Training Pipeline with Best Practices\n",
        "\n",
        "#### üìñ Concept\n",
        "\n",
        "Combining everything we've learned:\n",
        "- Appropriate loss function\n",
        "- Optimal optimizer choice\n",
        "- Learning rate scheduling\n",
        "- Regularization\n",
        "\n",
        "#### üíª Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CompletePipeline:\n",
        "    def __init__(self, model, loss_fn, optimizer, scheduler=None, device='cpu'):\n",
        "        self.model = model.to(device)\n",
        "        self.loss_fn = loss_fn\n",
        "        self.optimizer = optimizer\n",
        "        self.scheduler = scheduler\n",
        "        self.device = device\n",
        "        self.history = {'train_loss': [], 'val_loss': [], \n",
        "                       'train_acc': [], 'val_acc': [], 'lr': []}\n",
        "    \n",
        "    def train_epoch(self, dataloader):\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        \n",
        "        for batch_X, batch_y in dataloader:\n",
        "            batch_X = batch_X.to(self.device)\n",
        "            batch_y = batch_y.to(self.device)\n",
        "            \n",
        "            self.optimizer.zero_grad()\n",
        "            outputs = self.model(batch_X)\n",
        "            loss = self.loss_fn(outputs, batch_y)\n",
        "            loss.backward()\n",
        "            \n",
        "            # Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
        "            \n",
        "            self.optimizer.step()\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == batch_y).sum().item()\n",
        "            total += batch_y.size(0)\n",
        "        \n",
        "        return total_loss / len(dataloader), correct / total\n",
        "    \n",
        "    def validate(self, dataloader):\n",
        "        self.model.eval()\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for batch_X, batch_y in dataloader:\n",
        "                batch_X = batch_X.to(self.device)\n",
        "                batch_y = batch_y.to(self.device)\n",
        "                \n",
        "                outputs = self.model(batch_X)\n",
        "                loss = self.loss_fn(outputs, batch_y)\n",
        "                \n",
        "                total_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                correct += (predicted == batch_y).sum().item()\n",
        "                total += batch_y.size(0)\n",
        "        \n",
        "        return total_loss / len(dataloader), correct / total\n",
        "    \n",
        "    def train(self, train_loader, val_loader, epochs, early_stopping_patience=10):\n",
        "        best_val_loss = float('inf')\n",
        "        patience_counter = 0\n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            # Training\n",
        "            train_loss, train_acc = self.train_epoch(train_loader)\n",
        "            \n",
        "            # Validation\n",
        "            val_loss, val_acc = self.validate(val_loader)\n",
        "            \n",
        "            # Learning rate scheduling\n",
        "            current_lr = self.optimizer.param_groups[0]['lr']\n",
        "            if self.scheduler:\n",
        "                if isinstance(self.scheduler, optim.lr_scheduler.ReduceLROnPlateau):\n",
        "                    self.scheduler.step(val_loss)\n",
        "                else:\n",
        "                    self.scheduler.step()\n",
        "                current_lr = self.optimizer.param_groups[0]['lr']\n",
        "            \n",
        "            # Record history\n",
        "            self.history['train_loss'].append(train_loss)\n",
        "            self.history['val_loss'].append(val_loss)\n",
        "            self.history['train_acc'].append(train_acc)\n",
        "            self.history['val_acc'].append(val_acc)\n",
        "            self.history['lr'].append(current_lr)\n",
        "            \n",
        "            # Early stopping\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                patience_counter = 0\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "                if patience_counter >= early_stopping_patience:\n",
        "                    print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                    break\n",
        "            \n",
        "            # Progress reporting\n",
        "            if (epoch + 1) % 10 == 0:\n",
        "                print(f\"Epoch {epoch+1}/{epochs} | \"\n",
        "                      f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2%} | \"\n",
        "                      f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2%} | \"\n",
        "                      f\"LR: {current_lr:.6f}\")\n",
        "\n",
        "# Prepare data\n",
        "iris = load_iris()\n",
        "X_iris = torch.FloatTensor(iris.data)\n",
        "y_iris = torch.LongTensor(iris.target)\n",
        "\n",
        "# Split and scale\n",
        "X_train_iris, X_test_iris, y_train_iris, y_test_iris = train_test_split(\n",
        "    X_iris, y_iris, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_iris = torch.FloatTensor(scaler.fit_transform(X_train_iris))\n",
        "X_test_iris = torch.FloatTensor(scaler.transform(X_test_iris))\n",
        "\n",
        "# Create data loaders\n",
        "train_dataset = TensorDataset(X_train_iris, y_train_iris)\n",
        "test_dataset = TensorDataset(X_test_iris, y_test_iris)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# Create model\n",
        "class IrisNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(IrisNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(4, 16)\n",
        "        self.fc2 = nn.Linear(16, 8)\n",
        "        self.fc3 = nn.Linear(8, 3)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Setup complete pipeline\n",
        "model = IrisNet()\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.01, weight_decay=0.01)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)\n",
        "\n",
        "pipeline = CompletePipeline(model, loss_fn, optimizer, scheduler)\n",
        "\n",
        "# Train\n",
        "print(\"üöÄ Starting training with complete pipeline...\\n\")\n",
        "pipeline.train(train_loader, test_loader, epochs=100, early_stopping_patience=15)\n",
        "\n",
        "# Visualization\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "# Loss curves\n",
        "ax = axes[0]\n",
        "ax.plot(pipeline.history['train_loss'], label='Training', linewidth=2)\n",
        "ax.plot(pipeline.history['val_loss'], label='Validation', linewidth=2)\n",
        "ax.set_xlabel('Epoch')\n",
        "ax.set_ylabel('Loss')\n",
        "ax.set_title('Loss Curves', fontweight='bold')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Accuracy curves\n",
        "ax = axes[1]\n",
        "ax.plot(pipeline.history['train_acc'], label='Training', linewidth=2)\n",
        "ax.plot(pipeline.history['val_acc'], label='Validation', linewidth=2)\n",
        "ax.set_xlabel('Epoch')\n",
        "ax.set_ylabel('Accuracy')\n",
        "ax.set_title('Accuracy Curves', fontweight='bold')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Learning rate schedule\n",
        "ax = axes[2]\n",
        "ax.plot(pipeline.history['lr'], linewidth=2, color='green')\n",
        "ax.set_xlabel('Epoch')\n",
        "ax.set_ylabel('Learning Rate')\n",
        "ax.set_title('Learning Rate Schedule', fontweight='bold')\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nüìä Final Results:\")\n",
        "print(f\"Best Validation Accuracy: {max(pipeline.history['val_acc']):.2%}\")\n",
        "print(f\"Final Training Accuracy: {pipeline.history['train_acc'][-1]:.2%}\")\n",
        "print(f\"Final Validation Accuracy: {pipeline.history['val_acc'][-1]:.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 5: Summary and Practice Exercises\n",
        "\n",
        "### üéØ Key Takeaways\n",
        "\n",
        "1. **Loss Functions**:\n",
        "   - Choose based on problem type and data characteristics\n",
        "   - Consider robustness requirements (outliers)\n",
        "   - Custom losses for domain-specific objectives\n",
        "\n",
        "2. **Optimization Algorithms**:\n",
        "   - Start with Adam/AdamW for rapid prototyping\n",
        "   - Fine-tune with SGD+Momentum for best generalization\n",
        "   - Adaptive methods handle different parameter scales well\n",
        "\n",
        "3. **Learning Rate Scheduling**:\n",
        "   - Always use some form of scheduling\n",
        "   - Warm-up for large models/batch sizes\n",
        "   - Cosine annealing works well in practice\n",
        "\n",
        "### üìù Practice Exercises\n",
        "\n",
        "#### Exercise A: Custom Loss Function\n",
        "Design and implement a custom loss function that:\n",
        "- Combines MSE with a penalty for prediction uncertainty\n",
        "- Includes a regularization term for model complexity\n",
        "- Adapts weights based on sample difficulty\n",
        "\n",
        "#### Exercise B: Optimizer Ensemble\n",
        "Create an ensemble optimizer that:\n",
        "- Switches between SGD and Adam based on training progress\n",
        "- Uses different optimizers for different layer types\n",
        "- Implements a voting mechanism for parameter updates\n",
        "\n",
        "#### Exercise C: Advanced Scheduling\n",
        "Implement a learning rate scheduler that:\n",
        "- Detects plateaus automatically\n",
        "- Combines multiple scheduling strategies\n",
        "- Adapts based on gradient statistics\n",
        "\n",
        "### üöÄ Next Steps\n",
        "\n",
        "1. Experiment with different combinations on your own datasets\n",
        "2. Profile and benchmark different approaches\n",
        "3. Implement advanced techniques like LARS, LAMB optimizers\n",
        "4. Explore meta-learning for automatic hyperparameter tuning\n",
        "\n",
        "---\n",
        "\n",
        "**Remember**: The best combination of loss, optimizer, and scheduler depends on your specific problem. Always validate empirically!\n",
        "\n",
        "Happy learning! üéâ"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}