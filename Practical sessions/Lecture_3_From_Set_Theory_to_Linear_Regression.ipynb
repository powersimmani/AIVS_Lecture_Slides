{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ From Math to ML: Essential Computational Practice\n",
    "\n",
    "## Table of Contents\n",
    "1. [Vectors and Dot Product Computation](#practice-1-vectors-and-dot-product-computation)\n",
    "2. [Matrix Operations and Transformations](#practice-2-matrix-operations-and-transformations)\n",
    "3. [Eigenvalues and Eigenvectors](#practice-3-eigenvalues-and-eigenvectors)\n",
    "4. [Probability Distributions and Sampling](#practice-4-probability-distributions-and-sampling)\n",
    "5. [Computing Expected Value, Variance, and Covariance](#practice-5-computing-expected-value-variance-and-covariance)\n",
    "6. [Direct Implementation of Least Squares Method](#practice-6-direct-implementation-of-least-squares-method)\n",
    "7. [Normal Equation Solution](#practice-7-normal-equation-solution)\n",
    "8. [Complete Implementation of Simple Linear Regression](#practice-8-complete-implementation-of-simple-linear-regression)\n",
    "9. [Multiple Linear Regression and Feature Engineering](#practice-9-multiple-linear-regression-and-feature-engineering)\n",
    "10. [Model Evaluation and Diagnostics](#practice-10-model-evaluation-and-diagnostics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing and Importing Essential Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization settings\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"‚úÖ All libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 1: Vectors and Dot Product Computation\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Understand the basics of vector operations\n",
    "- Grasp the geometric meaning of dot products\n",
    "- Learn how to calculate cosine similarity\n",
    "\n",
    "### üìñ Key Concepts\n",
    "**Inner Product (Dot Product):** $\\langle x, y \\rangle = x_1y_1 + x_2y_2 + ... + x_ny_n = ||x||||y||\\cos(\\theta)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Vector creation and basic operations\n",
    "def vector_operations():\n",
    "    \"\"\"Practice basic vector operations\"\"\"\n",
    "    # Create vectors\n",
    "    v1 = np.array([3, 4])\n",
    "    v2 = np.array([1, 2])\n",
    "    \n",
    "    # Vector operations\n",
    "    addition = v1 + v2\n",
    "    scalar_mult = 2 * v1\n",
    "    \n",
    "    # Manual dot product calculation\n",
    "    dot_manual = sum(v1[i] * v2[i] for i in range(len(v1)))\n",
    "    \n",
    "    # NumPy dot product\n",
    "    dot_numpy = np.dot(v1, v2)\n",
    "    \n",
    "    print(\"Vector v1:\", v1)\n",
    "    print(\"Vector v2:\", v2)\n",
    "    print(\"\\n--- Operation Results ---\")\n",
    "    print(f\"Addition: {addition}\")\n",
    "    print(f\"Scalar multiplication: {scalar_mult}\")\n",
    "    print(f\"Dot product (manual): {dot_manual}\")\n",
    "    print(f\"Dot product (NumPy): {dot_numpy}\")\n",
    "    print(f\"Results match: {dot_manual == dot_numpy}\")\n",
    "    \n",
    "    return v1, v2\n",
    "\n",
    "v1, v2 = vector_operations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 2: Matrix Operations and Transformations\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Understand and implement matrix multiplication\n",
    "- Visualize linear transformations\n",
    "- Grasp the meaning of inverse matrices and determinants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Direct implementation of matrix multiplication\n",
    "def matrix_multiplication_comparison():\n",
    "    \"\"\"Implement matrix multiplication manually and compare with NumPy\"\"\"\n",
    "    \n",
    "    # Create matrices\n",
    "    A = np.array([[1, 2], [3, 4]])\n",
    "    B = np.array([[5, 6], [7, 8]])\n",
    "    \n",
    "    # Manual implementation\n",
    "    def manual_matmul(A, B):\n",
    "        rows_A, cols_A = A.shape\n",
    "        rows_B, cols_B = B.shape\n",
    "        \n",
    "        if cols_A != rows_B:\n",
    "            raise ValueError(\"Matrix dimensions do not match\")\n",
    "        \n",
    "        C = np.zeros((rows_A, cols_B))\n",
    "        \n",
    "        for i in range(rows_A):\n",
    "            for j in range(cols_B):\n",
    "                for k in range(cols_A):\n",
    "                    C[i][j] += A[i][k] * B[k][j]\n",
    "        \n",
    "        return C\n",
    "    \n",
    "    # Compute\n",
    "    C_manual = manual_matmul(A, B)\n",
    "    C_numpy = A @ B  # NumPy matrix multiplication\n",
    "    \n",
    "    print(\"Matrix A:\")\n",
    "    print(A)\n",
    "    print(\"\\nMatrix B:\")\n",
    "    print(B)\n",
    "    print(\"\\nManual implementation A√óB:\")\n",
    "    print(C_manual)\n",
    "    print(\"\\nNumPy A√óB:\")\n",
    "    print(C_numpy)\n",
    "    print(f\"\\nResults match: {np.allclose(C_manual, C_numpy)}\")\n",
    "    \n",
    "    return A\n",
    "\n",
    "matrix_A = matrix_multiplication_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 7: Normal Equation Solution (Core Concept!)\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Calculate the normal equation $(X^T X)^{-1} X^T y$ directly\n",
    "- Implement efficiently using NumPy\n",
    "- Compare results with Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1 Derivation and implementation of normal equation\n",
    "def normal_equation_implementation():\n",
    "    \"\"\"Implement the normal equation step by step\"\"\"\n",
    "    \n",
    "    # Generate data\n",
    "    np.random.seed(42)\n",
    "    n_samples = 100\n",
    "    X = np.random.randn(n_samples, 1) * 3\n",
    "    y = 2 + 3 * X + np.random.randn(n_samples, 1) * 0.5\n",
    "    \n",
    "    # Create design matrix (add intercept term)\n",
    "    X_with_intercept = np.c_[np.ones((n_samples, 1)), X]\n",
    "    \n",
    "    print(\"Normal Equation: Œ≤ = (X^T X)^(-1) X^T y\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Step-by-step calculation\n",
    "    print(\"\\nStep 1: Calculate X^T\")\n",
    "    X_transpose = X_with_intercept.T\n",
    "    print(f\"   X shape: {X_with_intercept.shape}\")\n",
    "    print(f\"   X^T shape: {X_transpose.shape}\")\n",
    "    \n",
    "    print(\"\\nStep 2: Calculate X^T X\")\n",
    "    XtX = X_transpose @ X_with_intercept\n",
    "    print(f\"   X^T X shape: {XtX.shape}\")\n",
    "    print(f\"   X^T X:\\n{XtX}\")\n",
    "    \n",
    "    print(\"\\nStep 3: Calculate (X^T X)^(-1)\")\n",
    "    XtX_inv = np.linalg.inv(XtX)\n",
    "    print(f\"   (X^T X)^(-1):\\n{XtX_inv}\")\n",
    "    \n",
    "    print(\"\\nStep 4: Calculate X^T y\")\n",
    "    Xty = X_transpose @ y\n",
    "    print(f\"   X^T y shape: {Xty.shape}\")\n",
    "    \n",
    "    print(\"\\nStep 5: Œ≤ = (X^T X)^(-1) X^T y\")\n",
    "    beta = XtX_inv @ Xty\n",
    "    print(f\"   Œ≤:\\n{beta}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(f\"Final result: Œ≤‚ÇÄ = {beta[0, 0]:.4f}, Œ≤‚ÇÅ = {beta[1, 0]:.4f}\")\n",
    "    \n",
    "    return X, y, beta\n",
    "\n",
    "X_data, y_data, beta_normal = normal_equation_implementation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.2 Comparison of various calculation methods\n",
    "def compare_regression_methods():\n",
    "    \"\"\"Compare normal equation, NumPy, and Scikit-learn\"\"\"\n",
    "    \n",
    "    print(\"Comparison of Linear Regression Calculation Methods\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Method 1: Direct implementation of normal equation\n",
    "    X_with_intercept = np.c_[np.ones((len(X_data), 1)), X_data]\n",
    "    beta_manual = np.linalg.inv(X_with_intercept.T @ X_with_intercept) @ X_with_intercept.T @ y_data\n",
    "    print(f\"\\nMethod 1 - Direct Normal Equation Implementation:\")\n",
    "    print(f\"  Œ≤‚ÇÄ = {beta_manual[0, 0]:.6f}\")\n",
    "    print(f\"  Œ≤‚ÇÅ = {beta_manual[1, 0]:.6f}\")\n",
    "    \n",
    "    # Method 2: NumPy least squares (lstsq)\n",
    "    beta_numpy, residuals, rank, s = np.linalg.lstsq(X_with_intercept, y_data, rcond=None)\n",
    "    print(f\"\\nMethod 2 - NumPy lstsq:\")\n",
    "    print(f\"  Œ≤‚ÇÄ = {beta_numpy[0]:.6f}\")\n",
    "    print(f\"  Œ≤‚ÇÅ = {beta_numpy[1]:.6f}\")\n",
    "    \n",
    "    # Method 3: Scikit-learn\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    reg = LinearRegression()\n",
    "    reg.fit(X_data, y_data)\n",
    "    print(f\"\\nMethod 3 - Scikit-learn:\")\n",
    "    print(f\"  Œ≤‚ÇÄ = {reg.intercept_[0]:.6f}\")\n",
    "    print(f\"  Œ≤‚ÇÅ = {reg.coef_[0, 0]:.6f}\")\n",
    "    \n",
    "    # Compare results\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Results Comparison:\")\n",
    "    print(f\"  Difference between Method 1 and 2: Œ≤‚ÇÄ={abs(beta_manual[0, 0] - beta_numpy[0]):.10f}, \"\n",
    "          f\"Œ≤‚ÇÅ={abs(beta_manual[1, 0] - beta_numpy[1]):.10f}\")\n",
    "    print(f\"  Difference between Method 1 and 3: Œ≤‚ÇÄ={abs(beta_manual[0, 0] - reg.intercept_[0]):.10f}, \"\n",
    "          f\"Œ≤‚ÇÅ={abs(beta_manual[1, 0] - reg.coef_[0, 0]):.10f}\")\n",
    "    \n",
    "    print(\"\\n‚úÖ All methods produce identical results!\")\n",
    "    \n",
    "    return beta_manual, beta_numpy, reg\n",
    "\n",
    "beta1, beta2, sklearn_model = compare_regression_methods()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéØ Practice Complete!\n",
    "\n",
    "### Summary of What We Learned:\n",
    "\n",
    "1. **Vectors and Matrix Operations**: Basic operations like dot products and matrix multiplication\n",
    "2. **Normal Equation**: Finding analytical solutions - **(X^T X)^(-1) X^T y**\n",
    "3. **Implementation vs. Libraries**: Comparing direct implementation with Scikit-learn\n",
    "\n",
    "### Key Insights:\n",
    "- Understanding how mathematical concepts connect to machine learning\n",
    "- The normal equation is the foundation of all linear regression\n",
    "- Both NumPy and Scikit-learn use the same mathematical principles internally\n",
    "\n",
    "### Next Steps:\n",
    "- Implementing gradient descent\n",
    "- Regularization techniques (Ridge, Lasso)\n",
    "- Extending to multiple linear regression"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
