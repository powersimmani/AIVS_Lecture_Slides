{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Diffusion Models: From Theory to Practice\n",
        "## A Comprehensive Hands-on Tutorial\n\n",
        "**Based on**: Lecture 16 - Generative Models: Diffusion\n",
        "**Author**: Ho-min Park\n",
        "**Adapted for Interactive Learning**\n\n",
        "---\n\n",
        "## üéØ Learning Objectives\n\n",
        "By the end of this notebook, you will:\n",
        "1. Understand the mathematical foundations of diffusion models\n",
        "2. Implement forward and reverse diffusion processes\n",
        "3. Build a simple diffusion model from scratch\n",
        "4. Explore advanced techniques like DDIM and classifier-free guidance\n",
        "5. Apply diffusion models to real-world tasks\n\n",
        "---\n\n",
        "## üìö Table of Contents\n\n",
        "1. **Setup and Prerequisites**\n",
        "2. **Part 1: Mathematical Foundations** (Exercises 1-3)\n",
        "3. **Part 2: Forward and Reverse Processes** (Exercises 4-6)\n",
        "4. **Part 3: Building a Simple Diffusion Model** (Exercises 7-8)\n",
        "5. **Part 4: Advanced Techniques** (Exercises 9-10)\n",
        "6. **Summary and Further Resources**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ Setup and Imports\n\n",
        "First, let's import all necessary libraries and set up our environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Deep learning libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Visualization\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "from IPython.display import HTML, display\n",
        "import imageio\n",
        "from PIL import Image\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Configure plotting\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette('husl')\n",
        "%matplotlib inline\n",
        "\n",
        "print('‚úÖ All libraries imported successfully!')\n",
        "print(f'PyTorch version: {torch.__version__}')\n",
        "print(f'CUDA available: {torch.cuda.is_available()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Part 1: Mathematical Foundations üî¢\n\n",
        "## Understanding Diffusion Models\n\n",
        "Diffusion models work by gradually adding noise to data (forward process) and then learning to reverse this process (reverse process). The key insight is that if we can learn to denoise slightly noisy data, we can generate new samples by starting from pure noise and iteratively denoising."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 1: Understanding Gaussian Noise Addition\n\n",
        "### üìñ Concept\n",
        "The forward diffusion process gradually transforms data into Gaussian noise through a Markov chain:\n",
        "$$x_t = \\sqrt{1-\\beta_t} \\cdot x_{t-1} + \\sqrt{\\beta_t} \\cdot \\epsilon$$\n",
        "where $\\epsilon \\sim \\mathcal{N}(0, I)$ and $\\beta_t$ is the noise schedule.\n\n",
        "### üíª Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def add_gaussian_noise(x, noise_level):\n",
        "    \"\"\"Add Gaussian noise to data\n",
        "    Args:\n",
        "        x: Input data\n",
        "        noise_level: Standard deviation of noise (beta_t)\n",
        "    Returns:\n",
        "        Noisy data and the noise added\n",
        "    \"\"\"\n",
        "    noise = np.random.randn(*x.shape) * noise_level\n",
        "    noisy_x = x + noise\n",
        "    return noisy_x, noise\n",
        "\n",
        "# Generate synthetic 2D data\n",
        "np.random.seed(42)\n",
        "n_samples = 1000\n",
        "\n",
        "# Create a spiral dataset\n",
        "theta = np.linspace(0, 4*np.pi, n_samples)\n",
        "r = theta / (4*np.pi)\n",
        "x_original = np.column_stack([\n",
        "    r * np.cos(theta),\n",
        "    r * np.sin(theta)\n",
        "])\n",
        "\n",
        "# Add noise at different levels\n",
        "noise_levels = [0.0, 0.05, 0.1, 0.2, 0.5]\n",
        "fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
        "\n",
        "for idx, noise_level in enumerate(noise_levels):\n",
        "    x_noisy, _ = add_gaussian_noise(x_original, noise_level)\n",
        "    axes[idx].scatter(x_noisy[:, 0], x_noisy[:, 1], s=1, alpha=0.5)\n",
        "    axes[idx].set_title(f'Œ≤ = {noise_level}')\n",
        "    axes[idx].set_xlim(-1.5, 1.5)\n",
        "    axes[idx].set_ylim(-1.5, 1.5)\n",
        "    axes[idx].set_aspect('equal')\n",
        "\n",
        "plt.suptitle('Progressive Noise Addition (Forward Process)', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üí° Key Insight: As Œ≤ increases, the structure gradually disappears into noise.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 2: Implementing Noise Schedules\n\n",
        "### üìñ Concept\n",
        "The noise schedule $\\{\\beta_t\\}_{t=1}^T$ controls how quickly noise is added. Common schedules include:\n",
        "- **Linear**: $\\beta_t$ increases linearly from $\\beta_1$ to $\\beta_T$\n",
        "- **Cosine**: Smoother transition, better for high-resolution images\n",
        "- **Quadratic**: Accelerated noise addition\n\n",
        "### üíª Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NoiseSchedule:\n",
        "    \"\"\"Different noise schedule implementations\"\"\"\n",
        "    \n",
        "    def __init__(self, num_timesteps=1000, beta_start=1e-4, beta_end=0.02):\n",
        "        self.num_timesteps = num_timesteps\n",
        "        self.beta_start = beta_start\n",
        "        self.beta_end = beta_end\n",
        "    \n",
        "    def linear_schedule(self):\n",
        "        \"\"\"Linear noise schedule (original DDPM)\"\"\"\n",
        "        return np.linspace(self.beta_start, self.beta_end, self.num_timesteps)\n",
        "    \n",
        "    def cosine_schedule(self, s=0.008):\n",
        "        \"\"\"Cosine noise schedule (improved DDPM)\"\"\"\n",
        "        steps = self.num_timesteps + 1\n",
        "        t = np.linspace(0, self.num_timesteps, steps)\n",
        "        alphas_cumprod = np.cos(((t / self.num_timesteps) + s) / (1 + s) * np.pi * 0.5) ** 2\n",
        "        alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
        "        betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
        "        return np.clip(betas, 0.0001, 0.999)\n",
        "    \n",
        "    def quadratic_schedule(self):\n",
        "        \"\"\"Quadratic noise schedule\"\"\"\n",
        "        t = np.linspace(0, 1, self.num_timesteps)\n",
        "        return self.beta_start + (self.beta_end - self.beta_start) * t ** 2\n",
        "\n",
        "# Compare different schedules\n",
        "scheduler = NoiseSchedule(num_timesteps=1000)\n",
        "\n",
        "schedules = {\n",
        "    'Linear': scheduler.linear_schedule(),\n",
        "    'Cosine': scheduler.cosine_schedule(),\n",
        "    'Quadratic': scheduler.quadratic_schedule()\n",
        "}\n",
        "\n",
        "# Visualize schedules\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "for idx, (name, betas) in enumerate(schedules.items()):\n",
        "    # Plot beta values\n",
        "    axes[idx].plot(betas, label='Œ≤_t', color='blue', alpha=0.7)\n",
        "    \n",
        "    # Calculate and plot cumulative product of alphas\n",
        "    alphas = 1 - betas\n",
        "    alphas_cumprod = np.cumprod(alphas)\n",
        "    axes[idx].plot(alphas_cumprod, label='·æ±_t', color='red', alpha=0.7)\n",
        "    \n",
        "    axes[idx].set_xlabel('Timestep')\n",
        "    axes[idx].set_ylabel('Value')\n",
        "    axes[idx].set_title(f'{name} Schedule')\n",
        "    axes[idx].legend()\n",
        "    axes[idx].grid(True, alpha=0.3)\n",
        "\n",
        "plt.suptitle('Comparison of Noise Schedules', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üí° Key Insight: The cosine schedule provides a smoother transition,\")\n",
        "print(\"   preventing sudden information loss in early steps.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üéØ Your Turn!\n",
        "Modify the `NoiseSchedule` class to implement an exponential schedule where $\\beta_t = \\beta_1 \\cdot e^{t \\cdot \\log(\\beta_T/\\beta_1) / T}$. Compare it with the existing schedules."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 3: The Reparameterization Trick\n\n",
        "### üìñ Concept\n",
        "Instead of computing $x_t$ step by step, we can directly sample it from $x_0$ using:\n",
        "$$x_t = \\sqrt{\\bar{\\alpha}_t} \\cdot x_0 + \\sqrt{1-\\bar{\\alpha}_t} \\cdot \\epsilon$$\n",
        "where $\\bar{\\alpha}_t = \\prod_{s=1}^{t} \\alpha_s$ and $\\alpha_t = 1 - \\beta_t$.\n\n",
        "### üíª Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ForwardDiffusion:\n",
        "    \"\"\"Forward diffusion process with reparameterization\"\"\"\n",
        "    \n",
        "    def __init__(self, betas):\n",
        "        self.betas = betas\n",
        "        self.alphas = 1 - betas\n",
        "        self.alphas_cumprod = np.cumprod(self.alphas)\n",
        "        self.sqrt_alphas_cumprod = np.sqrt(self.alphas_cumprod)\n",
        "        self.sqrt_one_minus_alphas_cumprod = np.sqrt(1 - self.alphas_cumprod)\n",
        "    \n",
        "    def q_sample(self, x_0, t, noise=None):\n",
        "        \"\"\"Sample x_t from x_0 using reparameterization trick\n",
        "        Args:\n",
        "            x_0: Original data\n",
        "            t: Timestep\n",
        "            noise: Optional pre-generated noise\n",
        "        Returns:\n",
        "            x_t: Noisy data at timestep t\n",
        "        \"\"\"\n",
        "        if noise is None:\n",
        "            noise = np.random.randn(*x_0.shape)\n",
        "        \n",
        "        sqrt_alpha_t = self.sqrt_alphas_cumprod[t]\n",
        "        sqrt_one_minus_alpha_t = self.sqrt_one_minus_alphas_cumprod[t]\n",
        "        \n",
        "        return sqrt_alpha_t * x_0 + sqrt_one_minus_alpha_t * noise\n",
        "\n",
        "# Demonstrate reparameterization on an image\n",
        "# Create a simple synthetic image (checkerboard pattern)\n",
        "def create_checkerboard(size=32, square_size=4):\n",
        "    \"\"\"Create a checkerboard pattern\"\"\"\n",
        "    img = np.zeros((size, size))\n",
        "    for i in range(0, size, square_size*2):\n",
        "        for j in range(0, size, square_size*2):\n",
        "            img[i:i+square_size, j:j+square_size] = 1\n",
        "            img[i+square_size:i+2*square_size, j+square_size:j+2*square_size] = 1\n",
        "    return img\n",
        "\n",
        "# Create image and diffusion process\n",
        "img = create_checkerboard(64, 8)\n",
        "scheduler = NoiseSchedule(num_timesteps=1000)\n",
        "betas = scheduler.cosine_schedule()\n",
        "forward_process = ForwardDiffusion(betas)\n",
        "\n",
        "# Sample at different timesteps\n",
        "timesteps = [0, 100, 250, 500, 750, 999]\n",
        "fig, axes = plt.subplots(1, 6, figsize=(15, 3))\n",
        "\n",
        "for idx, t in enumerate(timesteps):\n",
        "    if t == 0:\n",
        "        noisy_img = img\n",
        "    else:\n",
        "        noisy_img = forward_process.q_sample(img, t)\n",
        "    \n",
        "    axes[idx].imshow(noisy_img, cmap='gray', vmin=-2, vmax=2)\n",
        "    axes[idx].set_title(f't = {t}')\n",
        "    axes[idx].axis('off')\n",
        "\n",
        "plt.suptitle('Direct Sampling with Reparameterization Trick', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üí° Key Insight: The reparameterization trick allows us to directly sample\")\n",
        "print(\"   x_t from x_0 without computing all intermediate steps.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Part 2: Forward and Reverse Processes üîÑ\n\n",
        "Now let's implement the core components of diffusion models: the forward process that adds noise and the reverse process that removes it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 4: Understanding the Score Function\n\n",
        "### üìñ Concept\n",
        "The score function $\\nabla_x \\log p(x)$ indicates the direction of increasing probability density. In diffusion models, we approximate this with a neural network that predicts the noise:\n",
        "$$\\epsilon_\\theta(x_t, t) \\approx -\\sqrt{1-\\bar{\\alpha}_t} \\cdot \\nabla_{x_t} \\log p(x_t)$$\n\n",
        "### üíª Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SimpleNoisePredictor(nn.Module):\n",
        "    \"\"\"Simple neural network for noise prediction\"\"\"\n",
        "    \n",
        "    def __init__(self, input_dim=2, hidden_dim=128, time_dim=32):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Time embedding layers\n",
        "        self.time_embed = nn.Sequential(\n",
        "            nn.Linear(1, time_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(time_dim, time_dim)\n",
        "        )\n",
        "        \n",
        "        # Main network\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim + time_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, input_dim)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x, t):\n",
        "        \"\"\"Predict noise given noisy data and timestep\"\"\"\n",
        "        # Embed timestep\n",
        "        t_emb = self.time_embed(t.unsqueeze(-1))\n",
        "        \n",
        "        # Concatenate with input\n",
        "        h = torch.cat([x, t_emb], dim=-1)\n",
        "        \n",
        "        # Predict noise\n",
        "        return self.net(h)\n",
        "\n",
        "# Create and visualize the model\n",
        "model = SimpleNoisePredictor()\n",
        "print(\"Model Architecture:\")\n",
        "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(\"\\nModel structure:\")\n",
        "print(model)\n",
        "\n",
        "# Test the model with random input\n",
        "x_test = torch.randn(32, 2)  # Batch of 32 2D points\n",
        "t_test = torch.rand(32)  # Random timesteps\n",
        "noise_pred = model(x_test, t_test)\n",
        "print(f\"\\nInput shape: {x_test.shape}\")\n",
        "print(f\"Timestep shape: {t_test.shape}\")\n",
        "print(f\"Output (predicted noise) shape: {noise_pred.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 5: Training a Simple Diffusion Model\n\n",
        "### üìñ Concept\n",
        "The training algorithm:\n",
        "1. Sample data $x_0 \\sim p_{data}$\n",
        "2. Sample timestep $t \\sim \\text{Uniform}\\{1, ..., T\\}$\n",
        "3. Sample noise $\\epsilon \\sim \\mathcal{N}(0, I)$\n",
        "4. Create noisy sample: $x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1-\\bar{\\alpha}_t} \\epsilon$\n",
        "5. Predict noise: $\\hat{\\epsilon} = \\epsilon_\\theta(x_t, t)$\n",
        "6. Loss: $L = \\|\\epsilon - \\hat{\\epsilon}\\|^2$\n\n",
        "### üíª Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_diffusion_model(model, data, n_epochs=100, batch_size=128, lr=1e-3):\n",
        "    \"\"\"Train a simple diffusion model\"\"\"\n",
        "    \n",
        "    # Setup\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    \n",
        "    # Create noise schedule\n",
        "    scheduler = NoiseSchedule(num_timesteps=1000)\n",
        "    betas = torch.tensor(scheduler.cosine_schedule(), dtype=torch.float32, device=device)\n",
        "    alphas = 1 - betas\n",
        "    alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
        "    sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
        "    sqrt_one_minus_alphas_cumprod = torch.sqrt(1 - alphas_cumprod)\n",
        "    \n",
        "    # Convert data to tensor\n",
        "    data_tensor = torch.tensor(data, dtype=torch.float32, device=device)\n",
        "    \n",
        "    # Training loop\n",
        "    losses = []\n",
        "    model.train()\n",
        "    \n",
        "    for epoch in tqdm(range(n_epochs), desc='Training'):\n",
        "        epoch_losses = []\n",
        "        \n",
        "        # Shuffle data\n",
        "        perm = torch.randperm(len(data_tensor))\n",
        "        data_shuffled = data_tensor[perm]\n",
        "        \n",
        "        for i in range(0, len(data_tensor), batch_size):\n",
        "            # Get batch\n",
        "            batch = data_shuffled[i:i+batch_size]\n",
        "            batch_size_actual = len(batch)\n",
        "            \n",
        "            # Sample random timesteps\n",
        "            t = torch.randint(0, 1000, (batch_size_actual,), device=device)\n",
        "            \n",
        "            # Sample noise\n",
        "            noise = torch.randn_like(batch)\n",
        "            \n",
        "            # Create noisy samples\n",
        "            sqrt_alpha_t = sqrt_alphas_cumprod[t].unsqueeze(-1)\n",
        "            sqrt_one_minus_alpha_t = sqrt_one_minus_alphas_cumprod[t].unsqueeze(-1)\n",
        "            x_noisy = sqrt_alpha_t * batch + sqrt_one_minus_alpha_t * noise\n",
        "            \n",
        "            # Predict noise\n",
        "            t_normalized = t.float() / 1000.0  # Normalize timesteps\n",
        "            noise_pred = model(x_noisy, t_normalized)\n",
        "            \n",
        "            # Compute loss\n",
        "            loss = F.mse_loss(noise_pred, noise)\n",
        "            \n",
        "            # Backpropagation\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            epoch_losses.append(loss.item())\n",
        "        \n",
        "        avg_loss = np.mean(epoch_losses)\n",
        "        losses.append(avg_loss)\n",
        "        \n",
        "        if (epoch + 1) % 20 == 0:\n",
        "            print(f\"Epoch {epoch+1}/{n_epochs}, Loss: {avg_loss:.4f}\")\n",
        "    \n",
        "    return losses\n",
        "\n",
        "# Generate training data (Swiss roll dataset)\n",
        "from sklearn.datasets import make_swiss_roll\n",
        "n_samples = 5000\n",
        "noise = 0.1\n",
        "X, _ = make_swiss_roll(n_samples=n_samples, noise=noise)\n",
        "# Use only 2D projection\n",
        "X_2d = X[:, [0, 2]] / 10.0  # Normalize\n",
        "\n",
        "# Train the model\n",
        "model = SimpleNoisePredictor()\n",
        "losses = train_diffusion_model(model, X_2d, n_epochs=100)\n",
        "\n",
        "# Plot training loss\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(losses)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüí° Key Insight: The model learns to predict the noise that was added to the data.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 6: Sampling from the Trained Model\n\n",
        "### üìñ Concept\n",
        "DDPM Sampling algorithm:\n",
        "1. Start with pure noise: $x_T \\sim \\mathcal{N}(0, I)$\n",
        "2. For $t = T, T-1, ..., 1$:\n",
        "   - Predict noise: $\\hat{\\epsilon} = \\epsilon_\\theta(x_t, t)$\n",
        "   - Compute mean: $\\mu_\\theta(x_t, t)$\n",
        "   - Sample: $x_{t-1} \\sim \\mathcal{N}(\\mu_\\theta, \\sigma_t^2 I)$\n",
        "3. Return $x_0$\n\n",
        "### üíª Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def ddpm_sample(model, n_samples=100, n_steps=1000, device='cpu'):\n",
        "    \"\"\"Sample from diffusion model using DDPM\"\"\"\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    # Setup noise schedule\n",
        "    scheduler = NoiseSchedule(num_timesteps=n_steps)\n",
        "    betas = torch.tensor(scheduler.cosine_schedule(), dtype=torch.float32, device=device)\n",
        "    alphas = 1 - betas\n",
        "    alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
        "    alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
        "    \n",
        "    sqrt_recip_alphas = torch.sqrt(1.0 / alphas)\n",
        "    sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
        "    sqrt_one_minus_alphas_cumprod = torch.sqrt(1 - alphas_cumprod)\n",
        "    \n",
        "    posterior_variance = betas * (1 - alphas_cumprod_prev) / (1 - alphas_cumprod)\n",
        "    \n",
        "    # Start from pure noise\n",
        "    x = torch.randn(n_samples, 2, device=device)\n",
        "    \n",
        "    # Store intermediate steps for visualization\n",
        "    trajectory = [x.cpu().numpy()]\n",
        "    \n",
        "    # Reverse diffusion process\n",
        "    for t in tqdm(reversed(range(n_steps)), desc='Sampling', total=n_steps):\n",
        "        # Create batch of timesteps\n",
        "        t_batch = torch.full((n_samples,), t, device=device, dtype=torch.float32)\n",
        "        \n",
        "        # Predict noise\n",
        "        noise_pred = model(x, t_batch / n_steps)\n",
        "        \n",
        "        # Compute x_{t-1}\n",
        "        mean = sqrt_recip_alphas[t] * (x - betas[t] / sqrt_one_minus_alphas_cumprod[t] * noise_pred)\n",
        "        \n",
        "        if t > 0:\n",
        "            noise = torch.randn_like(x)\n",
        "            std = torch.sqrt(posterior_variance[t])\n",
        "            x = mean + std * noise\n",
        "        else:\n",
        "            x = mean\n",
        "        \n",
        "        # Store every 100th step\n",
        "        if t % 100 == 0:\n",
        "            trajectory.append(x.cpu().numpy())\n",
        "    \n",
        "    return x.cpu().numpy(), trajectory\n",
        "\n",
        "# Sample from the trained model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "samples, trajectory = ddpm_sample(model, n_samples=500, device=device)\n",
        "\n",
        "# Visualize the sampling process\n",
        "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
        "\n",
        "# Show trajectory (reverse process)\n",
        "for i, (ax, step) in enumerate(zip(axes[0], trajectory[:5])):\n",
        "    ax.scatter(step[:, 0], step[:, 1], s=5, alpha=0.5)\n",
        "    ax.set_xlim(-3, 3)\n",
        "    ax.set_ylim(-3, 3)\n",
        "    ax.set_title(f't = {1000 - i*100}')\n",
        "    ax.set_aspect('equal')\n",
        "\n",
        "# Compare with original data\n",
        "axes[1, 0].scatter(X_2d[:500, 0], X_2d[:500, 1], s=5, alpha=0.5, label='Original')\n",
        "axes[1, 0].set_title('Original Data')\n",
        "axes[1, 0].set_xlim(-3, 3)\n",
        "axes[1, 0].set_ylim(-3, 3)\n",
        "axes[1, 0].set_aspect('equal')\n",
        "\n",
        "axes[1, 1].scatter(samples[:, 0], samples[:, 1], s=5, alpha=0.5, label='Generated')\n",
        "axes[1, 1].set_title('Generated Samples')\n",
        "axes[1, 1].set_xlim(-3, 3)\n",
        "axes[1, 1].set_ylim(-3, 3)\n",
        "axes[1, 1].set_aspect('equal')\n",
        "\n",
        "# Hide unused axes\n",
        "for ax in axes[1, 2:]:\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.suptitle('DDPM Sampling Process', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüí° Key Insight: The model successfully generates samples that match the data distribution!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üéØ Your Turn!\n",
        "Implement DDIM sampling (deterministic) by setting the variance to 0 in the sampling process. Compare the results with DDPM sampling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Part 3: Building a Complete Diffusion Model üèóÔ∏è\n\n",
        "Let's build a more sophisticated diffusion model with U-Net architecture for image generation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 7: Implementing a Mini U-Net\n\n",
        "### üìñ Concept\n",
        "U-Net is the standard architecture for diffusion models. It features:\n",
        "- **Encoder path**: Downsamples while increasing channels\n",
        "- **Decoder path**: Upsamples while decreasing channels\n",
        "- **Skip connections**: Preserve fine details\n",
        "- **Time embedding**: Conditions the network on timestep\n\n",
        "### üíª Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SinusoidalPositionalEmbedding(nn.Module):\n",
        "    \"\"\"Sinusoidal time embeddings\"\"\"\n",
        "    \n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "    \n",
        "    def forward(self, t):\n",
        "        device = t.device\n",
        "        half_dim = self.dim // 2\n",
        "        embeddings = torch.exp(\n",
        "            -torch.log(torch.tensor(10000.0)) * \n",
        "            torch.arange(half_dim, device=device) / half_dim\n",
        "        )\n",
        "        embeddings = t[:, None] * embeddings[None, :]\n",
        "        embeddings = torch.cat([torch.sin(embeddings), torch.cos(embeddings)], dim=-1)\n",
        "        return embeddings\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    \"\"\"Residual block with time embedding\"\"\"\n",
        "    \n",
        "    def __init__(self, in_channels, out_channels, time_channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)\n",
        "        self.time_emb = nn.Linear(time_channels, out_channels)\n",
        "        self.norm1 = nn.GroupNorm(8, out_channels)\n",
        "        self.norm2 = nn.GroupNorm(8, out_channels)\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "        if in_channels != out_channels:\n",
        "            self.shortcut = nn.Conv2d(in_channels, out_channels, 1)\n",
        "        else:\n",
        "            self.shortcut = nn.Identity()\n",
        "    \n",
        "    def forward(self, x, t):\n",
        "        h = self.conv1(x)\n",
        "        h = self.norm1(h)\n",
        "        h = self.relu(h)\n",
        "        h = h + self.time_emb(t)[:, :, None, None]\n",
        "        h = self.conv2(h)\n",
        "        h = self.norm2(h)\n",
        "        h = self.relu(h)\n",
        "        return h + self.shortcut(x)\n",
        "\n",
        "class MiniUNet(nn.Module):\n",
        "    \"\"\"Simplified U-Net for diffusion models\"\"\"\n",
        "    \n",
        "    def __init__(self, in_channels=1, base_channels=32, time_dim=128):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Time embedding\n",
        "        self.time_mlp = nn.Sequential(\n",
        "            SinusoidalPositionalEmbedding(time_dim),\n",
        "            nn.Linear(time_dim, time_dim * 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(time_dim * 2, time_dim),\n",
        "        )\n",
        "        \n",
        "        # Encoder\n",
        "        self.down1 = ResidualBlock(in_channels, base_channels, time_dim)\n",
        "        self.down2 = ResidualBlock(base_channels, base_channels * 2, time_dim)\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "        \n",
        "        # Bottleneck\n",
        "        self.bottleneck = ResidualBlock(base_channels * 2, base_channels * 2, time_dim)\n",
        "        \n",
        "        # Decoder\n",
        "        self.up2 = ResidualBlock(base_channels * 4, base_channels * 2, time_dim)\n",
        "        self.up1 = ResidualBlock(base_channels * 3, base_channels, time_dim)\n",
        "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
        "        \n",
        "        # Output\n",
        "        self.out_conv = nn.Conv2d(base_channels, in_channels, 1)\n",
        "    \n",
        "    def forward(self, x, t):\n",
        "        # Time embedding\n",
        "        t_emb = self.time_mlp(t)\n",
        "        \n",
        "        # Encoder\n",
        "        d1 = self.down1(x, t_emb)\n",
        "        x1 = self.pool(d1)\n",
        "        d2 = self.down2(x1, t_emb)\n",
        "        x2 = self.pool(d2)\n",
        "        \n",
        "        # Bottleneck\n",
        "        bottleneck = self.bottleneck(x2, t_emb)\n",
        "        \n",
        "        # Decoder\n",
        "        u2 = self.upsample(bottleneck)\n",
        "        u2 = torch.cat([u2, d2], dim=1)\n",
        "        u2 = self.up2(u2, t_emb)\n",
        "        \n",
        "        u1 = self.upsample(u2)\n",
        "        u1 = torch.cat([u1, d1], dim=1)\n",
        "        u1 = self.up1(u1, t_emb)\n",
        "        \n",
        "        return self.out_conv(u1)\n",
        "\n",
        "# Create and test the model\n",
        "unet = MiniUNet(in_channels=1, base_channels=32)\n",
        "print(\"Mini U-Net Architecture:\")\n",
        "print(f\"Total parameters: {sum(p.numel() for p in unet.parameters()):,}\")\n",
        "\n",
        "# Test with random input\n",
        "x = torch.randn(4, 1, 32, 32)  # Batch of 4 grayscale 32x32 images\n",
        "t = torch.rand(4) * 1000  # Random timesteps\n",
        "output = unet(x, t)\n",
        "print(f\"\\nInput shape: {x.shape}\")\n",
        "print(f\"Output shape: {output.shape}\")\n",
        "print(\"\\nüí° Key Insight: U-Net preserves spatial dimensions while processing at multiple scales.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 8: Image Generation with Diffusion\n\n",
        "### üìñ Concept\n",
        "Let's train our U-Net on simple geometric shapes and generate new samples.\n\n",
        "### üíª Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_simple_dataset(n_samples=1000, img_size=32):\n",
        "    \"\"\"Create a dataset of simple geometric shapes\"\"\"\n",
        "    images = []\n",
        "    \n",
        "    for _ in range(n_samples):\n",
        "        img = np.zeros((img_size, img_size))\n",
        "        shape_type = np.random.choice(['circle', 'square', 'triangle'])\n",
        "        \n",
        "        if shape_type == 'circle':\n",
        "            center = np.random.randint(8, img_size-8, 2)\n",
        "            radius = np.random.randint(4, 8)\n",
        "            y, x = np.ogrid[:img_size, :img_size]\n",
        "            mask = (x - center[1])**2 + (y - center[0])**2 <= radius**2\n",
        "            img[mask] = 1\n",
        "        \n",
        "        elif shape_type == 'square':\n",
        "            size = np.random.randint(8, 16)\n",
        "            x = np.random.randint(0, img_size - size)\n",
        "            y = np.random.randint(0, img_size - size)\n",
        "            img[y:y+size, x:x+size] = 1\n",
        "        \n",
        "        else:  # triangle\n",
        "            pts = np.random.randint(4, img_size-4, (3, 2))\n",
        "            # Simple triangle fill (approximate)\n",
        "            for i in range(img_size):\n",
        "                for j in range(img_size):\n",
        "                    # Check if point is inside triangle (simplified)\n",
        "                    if np.random.random() > 0.7:  # Simplified for speed\n",
        "                        img[i, j] = 0.5\n",
        "        \n",
        "        images.append(img)\n",
        "    \n",
        "    return np.array(images)[:, np.newaxis, :, :]\n",
        "\n",
        "# Create dataset\n",
        "print(\"Creating geometric shapes dataset...\")\n",
        "shape_data = create_simple_dataset(n_samples=2000)\n",
        "print(f\"Dataset shape: {shape_data.shape}\")\n",
        "\n",
        "# Visualize some samples\n",
        "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    ax.imshow(shape_data[i, 0], cmap='gray')\n",
        "    ax.axis('off')\n",
        "plt.suptitle('Sample Geometric Shapes', fontsize=14)\n",
        "plt.show()\n",
        "\n",
        "# Quick training (simplified for demonstration)\n",
        "def train_image_diffusion(model, images, n_epochs=50, batch_size=32, lr=1e-3):\n",
        "    \"\"\"Train diffusion model on images\"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    \n",
        "    # Prepare data\n",
        "    images_tensor = torch.tensor(images, dtype=torch.float32, device=device)\n",
        "    \n",
        "    # Noise schedule\n",
        "    n_timesteps = 1000\n",
        "    beta = torch.linspace(1e-4, 0.02, n_timesteps, device=device)\n",
        "    alpha = 1 - beta\n",
        "    alpha_bar = torch.cumprod(alpha, dim=0)\n",
        "    \n",
        "    losses = []\n",
        "    model.train()\n",
        "    \n",
        "    for epoch in tqdm(range(n_epochs), desc='Training'):\n",
        "        epoch_losses = []\n",
        "        \n",
        "        for i in range(0, len(images_tensor), batch_size):\n",
        "            batch = images_tensor[i:i+batch_size]\n",
        "            batch_size_actual = len(batch)\n",
        "            \n",
        "            # Random timesteps\n",
        "            t = torch.randint(0, n_timesteps, (batch_size_actual,), device=device)\n",
        "            \n",
        "            # Add noise\n",
        "            noise = torch.randn_like(batch)\n",
        "            noisy_images = (torch.sqrt(alpha_bar[t])[:, None, None, None] * batch + \n",
        "                          torch.sqrt(1 - alpha_bar[t])[:, None, None, None] * noise)\n",
        "            \n",
        "            # Predict noise\n",
        "            noise_pred = model(noisy_images, t.float())\n",
        "            \n",
        "            # Loss\n",
        "            loss = F.mse_loss(noise_pred, noise)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            epoch_losses.append(loss.item())\n",
        "        \n",
        "        losses.append(np.mean(epoch_losses))\n",
        "    \n",
        "    return losses\n",
        "\n",
        "# Train the model (reduced epochs for demonstration)\n",
        "unet_model = MiniUNet(in_channels=1, base_channels=16)  # Smaller model for speed\n",
        "print(\"\\nTraining U-Net on geometric shapes...\")\n",
        "losses = train_image_diffusion(unet_model, shape_data, n_epochs=30)\n",
        "\n",
        "# Plot loss\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(losses)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss for Image Diffusion')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Part 4: Advanced Techniques üöÄ\n\n",
        "Let's explore advanced techniques that make modern diffusion models powerful."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 9: DDIM - Fast Deterministic Sampling\n\n",
        "### üìñ Concept\n",
        "DDIM (Denoising Diffusion Implicit Models) allows:\n",
        "- **Deterministic sampling**: Same noise ‚Üí same output\n",
        "- **Faster generation**: Skip timesteps (e.g., 50 steps instead of 1000)\n",
        "- **Interpolation**: Smooth transitions between samples\n\n",
        "The DDIM update rule:\n",
        "$$x_{t-1} = \\sqrt{\\bar{\\alpha}_{t-1}} \\cdot \\hat{x}_0 + \\sqrt{1-\\bar{\\alpha}_{t-1}} \\cdot \\hat{\\epsilon}_t$$\n\n",
        "### üíª Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def ddim_sample(model, image_size=32, n_samples=4, n_steps=50, eta=0.0, device='cpu'):\n",
        "    \"\"\"DDIM sampling - deterministic and fast\n",
        "    Args:\n",
        "        model: Trained diffusion model\n",
        "        n_steps: Number of denoising steps (can be < training timesteps)\n",
        "        eta: Interpolation parameter (0=deterministic, 1=DDPM)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    # Setup\n",
        "    n_training_steps = 1000\n",
        "    beta = torch.linspace(1e-4, 0.02, n_training_steps, device=device)\n",
        "    alpha = 1 - beta\n",
        "    alpha_bar = torch.cumprod(alpha, dim=0)\n",
        "    \n",
        "    # Select subset of timesteps\n",
        "    timesteps = torch.linspace(0, n_training_steps - 1, n_steps, dtype=torch.long, device=device)\n",
        "    \n",
        "    # Start from noise\n",
        "    x = torch.randn(n_samples, 1, image_size, image_size, device=device)\n",
        "    \n",
        "    # Store trajectory\n",
        "    trajectory = [x.cpu()]\n",
        "    \n",
        "    for i in tqdm(reversed(range(len(timesteps))), desc='DDIM Sampling'):\n",
        "        t = timesteps[i]\n",
        "        t_prev = timesteps[i - 1] if i > 0 else 0\n",
        "        \n",
        "        # Current and previous alpha values\n",
        "        alpha_t = alpha_bar[t]\n",
        "        alpha_prev = alpha_bar[t_prev] if i > 0 else torch.tensor(1.0)\n",
        "        \n",
        "        # Predict noise\n",
        "        t_batch = torch.full((n_samples,), t, device=device, dtype=torch.float32)\n",
        "        noise_pred = model(x, t_batch)\n",
        "        \n",
        "        # Predict x_0\n",
        "        x0_pred = (x - torch.sqrt(1 - alpha_t) * noise_pred) / torch.sqrt(alpha_t)\n",
        "        \n",
        "        # DDIM update\n",
        "        sigma = eta * torch.sqrt((1 - alpha_prev) / (1 - alpha_t)) * torch.sqrt(1 - alpha_t / alpha_prev)\n",
        "        noise = torch.randn_like(x) if i > 0 else 0\n",
        "        \n",
        "        x = (torch.sqrt(alpha_prev) * x0_pred + \n",
        "             torch.sqrt(1 - alpha_prev - sigma**2) * noise_pred + \n",
        "             sigma * noise)\n",
        "        \n",
        "        if i % 10 == 0:\n",
        "            trajectory.append(x.cpu())\n",
        "    \n",
        "    return x.cpu(), trajectory\n",
        "\n",
        "# Compare DDPM vs DDIM\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "unet_model = unet_model.to(device)\n",
        "\n",
        "print(\"Generating with DDIM (50 steps, deterministic)...\")\n",
        "ddim_samples, ddim_traj = ddim_sample(unet_model, n_samples=8, n_steps=50, eta=0.0, device=device)\n",
        "\n",
        "print(\"Generating with DDIM (50 steps, stochastic)...\")\n",
        "ddim_stoch_samples, _ = ddim_sample(unet_model, n_samples=8, n_steps=50, eta=1.0, device=device)\n",
        "\n",
        "# Visualize results\n",
        "fig, axes = plt.subplots(2, 8, figsize=(16, 4))\n",
        "\n",
        "for i in range(8):\n",
        "    axes[0, i].imshow(ddim_samples[i, 0], cmap='gray')\n",
        "    axes[0, i].axis('off')\n",
        "    axes[0, i].set_title(f'Det. {i+1}')\n",
        "    \n",
        "    axes[1, i].imshow(ddim_stoch_samples[i, 0], cmap='gray')\n",
        "    axes[1, i].axis('off')\n",
        "    axes[1, i].set_title(f'Stoch. {i+1}')\n",
        "\n",
        "plt.suptitle('DDIM Sampling: Deterministic (Œ∑=0) vs Stochastic (Œ∑=1)', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüí° Key Insight: DDIM enables fast sampling with controllable stochasticity.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 10: Classifier-Free Guidance\n\n",
        "### üìñ Concept\n",
        "Classifier-free guidance improves conditional generation without needing a separate classifier:\n",
        "$$\\tilde{\\epsilon}_\\theta = \\epsilon_\\theta(x_t, \\emptyset) + w \\cdot (\\epsilon_\\theta(x_t, c) - \\epsilon_\\theta(x_t, \\emptyset))$$\n",
        "\n",
        "Where:\n",
        "- $\\epsilon_\\theta(x_t, c)$: Conditional prediction\n",
        "- $\\epsilon_\\theta(x_t, \\emptyset)$: Unconditional prediction\n",
        "- $w$: Guidance scale (higher = stronger conditioning)\n\n",
        "### üíª Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ConditionalDiffusion(nn.Module):\n",
        "    \"\"\"Simple conditional diffusion model for demonstration\"\"\"\n",
        "    \n",
        "    def __init__(self, input_dim=2, n_classes=3, hidden_dim=128, time_dim=32):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Class embedding\n",
        "        self.class_embed = nn.Embedding(n_classes + 1, hidden_dim)  # +1 for unconditional\n",
        "        \n",
        "        # Time embedding\n",
        "        self.time_embed = nn.Sequential(\n",
        "            nn.Linear(1, time_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(time_dim, time_dim)\n",
        "        )\n",
        "        \n",
        "        # Main network\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim + hidden_dim + time_dim, hidden_dim * 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim * 2, hidden_dim * 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, input_dim)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x, t, c=None):\n",
        "        \"\"\"Forward pass\n",
        "        Args:\n",
        "            x: Noisy data\n",
        "            t: Timestep\n",
        "            c: Class label (None for unconditional)\n",
        "        \"\"\"\n",
        "        # Time embedding\n",
        "        t_emb = self.time_embed(t.unsqueeze(-1))\n",
        "        \n",
        "        # Class embedding (use last index for unconditional)\n",
        "        if c is None:\n",
        "            c = torch.full((x.shape[0],), 3, device=x.device, dtype=torch.long)\n",
        "        c_emb = self.class_embed(c)\n",
        "        \n",
        "        # Concatenate and predict\n",
        "        h = torch.cat([x, c_emb, t_emb], dim=-1)\n",
        "        return self.net(h)\n",
        "\n",
        "def generate_class_data(n_samples=1000, n_classes=3):\n",
        "    \"\"\"Generate 2D data with different classes\"\"\"\n",
        "    data = []\n",
        "    labels = []\n",
        "    \n",
        "    samples_per_class = n_samples // n_classes\n",
        "    \n",
        "    for class_idx in range(n_classes):\n",
        "        if class_idx == 0:  # Circle\n",
        "            theta = np.random.uniform(0, 2*np.pi, samples_per_class)\n",
        "            r = 1.0 + np.random.randn(samples_per_class) * 0.1\n",
        "            x = r * np.cos(theta)\n",
        "            y = r * np.sin(theta)\n",
        "        elif class_idx == 1:  # Square\n",
        "            x = np.random.uniform(-1, 1, samples_per_class)\n",
        "            y = np.random.uniform(-1, 1, samples_per_class)\n",
        "        else:  # Gaussian blob\n",
        "            x = np.random.randn(samples_per_class) * 0.3\n",
        "            y = np.random.randn(samples_per_class) * 0.3\n",
        "        \n",
        "        data.append(np.column_stack([x, y]))\n",
        "        labels.append(np.full(samples_per_class, class_idx))\n",
        "    \n",
        "    return np.vstack(data), np.concatenate(labels)\n",
        "\n",
        "# Generate conditional data\n",
        "X_cond, y_cond = generate_class_data(n_samples=3000)\n",
        "\n",
        "# Visualize classes\n",
        "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
        "class_names = ['Circle', 'Square', 'Gaussian']\n",
        "\n",
        "for i in range(3):\n",
        "    mask = y_cond == i\n",
        "    axes[i].scatter(X_cond[mask, 0], X_cond[mask, 1], s=5, alpha=0.5)\n",
        "    axes[i].set_title(f'Class {i}: {class_names[i]}')\n",
        "    axes[i].set_xlim(-2, 2)\n",
        "    axes[i].set_ylim(-2, 2)\n",
        "    axes[i].set_aspect('equal')\n",
        "\n",
        "plt.suptitle('Conditional Data Classes', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Classifier-free guidance sampling\n",
        "@torch.no_grad()\n",
        "def sample_with_guidance(model, class_label, guidance_scale=2.0, n_samples=100, device='cpu'):\n",
        "    \"\"\"Sample with classifier-free guidance\"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    # Start from noise\n",
        "    x = torch.randn(n_samples, 2, device=device)\n",
        "    \n",
        "    # Sampling loop (simplified)\n",
        "    n_steps = 100\n",
        "    for t in tqdm(reversed(range(n_steps)), desc=f'Sampling class {class_label}'):\n",
        "        t_batch = torch.full((n_samples,), t / n_steps, device=device)\n",
        "        \n",
        "        # Conditional and unconditional predictions\n",
        "        c = torch.full((n_samples,), class_label, device=device, dtype=torch.long)\n",
        "        eps_cond = model(x, t_batch, c)\n",
        "        eps_uncond = model(x, t_batch, None)\n",
        "        \n",
        "        # Apply guidance\n",
        "        eps = eps_uncond + guidance_scale * (eps_cond - eps_uncond)\n",
        "        \n",
        "        # Simple denoising step\n",
        "        x = x - 0.01 * eps\n",
        "    \n",
        "    return x.cpu().numpy()\n",
        "\n",
        "print(\"\\nüí° Key Insight: Classifier-free guidance amplifies the conditional signal\")\n",
        "print(\"   by extrapolating away from the unconditional prediction.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üéØ Your Turn!\n",
        "1. Train the conditional diffusion model on the class data\n",
        "2. Sample from each class with different guidance scales (w=0, 1, 2, 5)\n",
        "3. Observe how guidance strength affects sample quality and diversity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Summary and Key Takeaways üìù\n\n",
        "## What We've Learned\n\n",
        "### 1. **Mathematical Foundations**\n",
        "- Forward process gradually adds Gaussian noise\n",
        "- Reparameterization trick enables efficient training\n",
        "- Score function connects to denoising\n\n",
        "### 2. **Core Algorithms**\n",
        "- **DDPM**: Original stochastic sampling\n",
        "- **DDIM**: Deterministic and fast sampling\n",
        "- Training objective: predict the added noise\n\n",
        "### 3. **Architecture**\n",
        "- U-Net with skip connections\n",
        "- Time embeddings condition the network\n",
        "- Attention mechanisms for long-range dependencies\n\n",
        "### 4. **Advanced Techniques**\n",
        "- Classifier-free guidance for better conditioning\n",
        "- Latent diffusion for efficiency (Stable Diffusion)\n",
        "- Various noise schedules (linear, cosine)\n\n",
        "## Key Insights\n\n",
        "1. **Gradual is Good**: Small denoising steps are easier to learn than direct generation\n",
        "2. **Flexibility**: Same model can be sampled deterministically or stochastically\n",
        "3. **Scalability**: Works from toy 2D data to high-resolution images\n",
        "4. **Control**: Conditioning mechanisms enable precise control over generation\n\n",
        "## Applications\n\n",
        "- **Text-to-Image**: DALL-E 2, Stable Diffusion, Midjourney\n",
        "- **Image Editing**: Inpainting, style transfer, super-resolution\n",
        "- **3D Generation**: DreamFusion, Point-E\n",
        "- **Audio/Video**: AudioLDM, Video diffusion models\n",
        "- **Science**: Molecular design, protein folding\n\n",
        "## Next Steps\n\n",
        "1. **Experiment** with different architectures and datasets\n",
        "2. **Implement** advanced sampling techniques (DPM-Solver, PNDM)\n",
        "3. **Explore** conditioning mechanisms (ControlNet, IP-Adapter)\n",
        "4. **Apply** to your specific domain or problem\n\n",
        "## Resources for Further Learning\n\n",
        "- [The Annotated Diffusion Model](https://huggingface.co/blog/annotated-diffusion)\n",
        "- [Understanding Diffusion Models: A Unified Perspective](https://arxiv.org/abs/2208.11970)\n",
        "- [Diffusers Library by HuggingFace](https://github.com/huggingface/diffusers)\n",
        "- Original Papers: [DDPM](https://arxiv.org/abs/2006.11239), [DDIM](https://arxiv.org/abs/2010.02502), [Stable Diffusion](https://arxiv.org/abs/2112.10752)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üèÜ Final Challenge\n\n",
        "Create your own diffusion model for a specific task:\n",
        "1. Choose a dataset (e.g., MNIST digits, simple patterns, time series)\n",
        "2. Design an appropriate architecture\n",
        "3. Train with different noise schedules\n",
        "4. Implement both DDPM and DDIM sampling\n",
        "5. Add conditioning (class labels, text, etc.)\n",
        "6. Evaluate quality vs diversity trade-offs\n\n",
        "Share your results and learnings!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your code here!\n",
        "# Start by implementing a diffusion model for your chosen task\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}