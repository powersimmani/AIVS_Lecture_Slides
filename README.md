Please visit the website [AIVS Lecture Slides](http://powersimmani.github.io/AIVS_Lecture_Slides/)

# Machine Learning & Deep Learning

## ðŸ“š Course Overview

Comprehensive summaries of a 20-lecture series on Machine Learning and Deep Learning.  
**Instructor:** Ho-min Park (homin.park@ghent.ac.kr)

Each lecture has been condensed to a 2-page summary extracting only the core content from the original materials.  
For detailed code, examples, and slides, please refer to the original files.

---

## ðŸ“– Lecture List

### Part 1: Foundations and Infrastructure

- **[Lecture 1: Computer Structure and Networks for ML](Lecture01/readme.md)**  
  Data representation, CPU/GPU architecture, memory management, distributed training

- **[Lecture 2: Data Visualization](Lecture02/readme.md)**  
  Visualization principles, chart types, ML workflow visualization, dashboard design

### Part 2: Mathematical Foundations and Basic Models

- **[Lecture 3: From Set Theory to Linear Regression](Lecture03/readme.md)**  
  Set theory, linear algebra, calculus, probability, linear regression

- **[Lecture 4: From Linear to Logistic Regression](Lecture04/readme.md)**  
  Regression to classification, logistic regression, regularization (Ridge, Lasso, Elastic Net)

- **[Lecture 5: From Logistic Regression to Multi-layer Perceptrons](Lecture05/readme.md)**  
  Linear model limitations (XOR), MLP, Backpropagation

### Part 3: Model Evaluation and Advanced Training Techniques

- **[Lecture 6: Supervised Learning Evaluation](Lecture06/readme.md)**  
  Data splitting, metrics (MSE, F1, AUC), cross-validation, hyperparameter tuning

- **[Lecture 7: Deep Learning Fundamentals](Lecture07/readme.md)**  
  Importance of depth, activation functions (ReLU, GELU, Swish), hierarchical representation

- **[Lecture 8: Loss, Optimization and Scheduling](Lecture08/readme.md)**  
  Loss functions, optimization (SGD, Adam, AdamW), learning rate scheduling

- **[Lecture 9: Initialization and Normalization](Lecture09/readme.md)**  
  Weight initialization (Xavier, He), normalization (Batch Norm, Layer Norm)

### Part 4: Data Modalities

- **[Lecture 10: Data Modality and Feature Extraction](Lecture10/readme.md)**  
  Text (BoW, TF-IDF), Image (SIFT, HOG), Audio (MFCC), Graph

### Part 5: Sequence Models

- **[Lecture 11: Sequence Models](Lecture11/readme.md)**  
  Time series analysis (ARIMA), RNN, LSTM, GRU, Seq2Seq

- **[Lecture 12: Advanced Sequence Models](Lecture12/readme.md)**  
  RNN limitations, Attention mechanism, Bidirectional RNN

### Part 6: Transformers and LLMs

- **[Lecture 13: Transformer Architecture](Lecture13/readme.md)**  
  Self-Attention, Multi-Head Attention, Positional Encoding

- **[Lecture 14: Pre-trained Language Models & LLM Era](Lecture14/readme.md)**  
  BERT, GPT, T5, Transfer Learning, Fine-tuning, LLM

### Part 7: Generative Models

- **[Lecture 15: Generative Models - GAN](Lecture15/readme.md)**  
  GAN theory, Minimax, DCGAN, StyleGAN, training stabilization

- **[Lecture 16: Generative Models - Diffusion](Lecture16/readme.md)**  
  Diffusion Models, Forward/Reverse processes, DDPM, DDIM

### Part 8: Unsupervised Learning

- **[Lecture 17: Clustering and Unsupervised Learning Fundamentals](Lecture17/readme.md)**  
  K-Means, DBSCAN, GMM, Hierarchical clustering, PCA, t-SNE

- **[Lecture 18: Advanced Unsupervised Learning](Lecture18/readme.md)**  
  Self-Supervised Learning, Contrastive Learning (SimCLR, MoCo), DTW

### Part 9: Model Explainability

- **[Lecture 19: Model Explainability - XAI Fundamentals](Lecture19/readme.md)**  
  XAI motivation, Feature Importance, LIME, interpretability

- **[Lecture 20: Model Explainability - SHAP and Deep Learning XAI](Lecture20/readme.md)**  
  Shapley Values, SHAP, GradCAM, Integrated Gradients, Attention Visualization

---

## ðŸ’¡ How to Use

1. **Quick Review**: Quickly grasp the core concepts of each lecture
2. **Lecture Selection**: Find and study topics of interest in depth
3. **Pre-study**: Preview main content before lectures
4. **Checklist**: Check learning objectives and assess understanding

---


## ðŸŽ“ Recommended Learning Path

### Beginner â†’ Intermediate
1. Lectures 1-2 (Foundation Infrastructure)
2. Lectures 3-5 (Mathematical Foundations and Basic Models)
3. Lecture 6 (Model Evaluation)

### Intermediate â†’ Advanced
4. Lectures 7-9 (Advanced Training Techniques)
5. Lectures 10-12 (Data Modalities and Sequences)
6. Lectures 13-14 (Transformers and LLMs)

### Advanced Topics
7. Lectures 15-16 (Generative Models)
8. Lectures 17-18 (Unsupervised Learning)
9. Lectures 19-20 (Model Explainability)

---

## ðŸ“ž Contact

**Instructor:** Ho-min Park  
**Email:** homin.park@ghent.ac.kr | powersimmani@gmail.com  
**Institution:** Ghent University

---

**Note:** These documents are summaries of the original lecture materials. For detailed content, code examples, and practice materials, please refer to the original files.

---

## ðŸ“¦ Available Versions

- **Korean Version**: `Lecture##_Summary.md` files
- **English Version**: `Lecture##/readme.md` files (this version)
- **Index Files**: 
  - Korean: `LECTURES_INDEX.md`
  - English: `LECTURES_INDEX_EN.md` (this file)
