# Machine Learning Course: Complete Lecture Series

**Instructor:** Ho-min Park  
**Email:** homin.park@ghent.ac.kr | powersimmani@gmail.com  
**Course Level:** Intermediate to Advanced  
**Total Lectures:** 20

---

## üìö Course Overview

This comprehensive machine learning course covers the complete pipeline from fundamental computer architecture to advanced deep learning models and explainability techniques. Students will gain both theoretical understanding and practical implementation skills across supervised learning, unsupervised learning, deep learning, and state-of-the-art generative models.

---

## üéØ Course Structure

### **Module 1: Foundations** (Lectures 1-3)

**Lecture 1: Computer Structure and Networks for ML**  
Hardware components (CPU, GPU, TPU), memory hierarchy, distributed computing, and cloud platforms for machine learning deployment.

**Lecture 2: Data Visualization**  
Visual encoding principles, chart types, Gestalt principles, color theory, accessibility, and ML-specific visualizations for EDA and model evaluation.

**Lecture 3: From Set Theory to Linear Regression**  
Mathematical foundations including set theory, functions, vectors, matrices, probability distributions, and introduction to linear regression.

### **Module 2: Classical Machine Learning** (Lectures 4-6)

**Lecture 4: From Linear to Logistic Regression**  
Binary classification, sigmoid function, maximum likelihood estimation, gradient descent, and decision boundaries.

**Lecture 5: From Logistic Regression to Multi-layer Perceptrons**  
Limitations of linear models, XOR problem, introduction to neural networks, activation functions, and backpropagation basics.

**Lecture 6: Supervised Learning Evaluation**  
Confusion matrices, ROC curves, precision-recall analysis, cross-validation, bias-variance tradeoff, and model selection strategies.

### **Module 3: Deep Learning Fundamentals** (Lectures 7-10)

**Lecture 7: Multi-layer Perceptrons Deep Dive**  
Network architecture design, universal approximation theorem, forward/backward propagation, gradient flow, and implementation strategies.

**Lecture 8: Loss, Optimization and Scheduling**  
Loss functions (MSE, cross-entropy, custom losses), optimizers (SGD, Adam, RMSprop), learning rate scheduling, and convergence analysis.

**Lecture 9: Initialization and Normalization**  
Weight initialization techniques (Xavier, He), batch normalization, layer normalization, instance normalization, and training stability.

**Lecture 10: Data Modality and Feature Extraction**  
Convolutional Neural Networks (CNNs) for images, feature maps, pooling operations, famous architectures (AlexNet, VGG, ResNet), and transfer learning.

### **Module 4: Sequential Data and NLP** (Lectures 11-14)

**Lecture 11: Sequence Models**  
Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM), Gated Recurrent Units (GRU), bidirectional architectures, and sequence-to-sequence models.

**Lecture 12: Advanced Sequence Models**  
Encoder-decoder architectures, attention mechanisms, teacher forcing, beam search, and applications in machine translation and text generation.

**Lecture 13: Transformer Architecture**  
Self-attention mechanism, multi-head attention, positional encoding, transformer blocks, parallelization advantages, and scalability.

**Lecture 14: Pre-trained Language Models & LLM Era**  
BERT, GPT series, T5, instruction tuning, fine-tuning strategies, prompt engineering, and the evolution to large language models.

### **Module 5: Generative Models** (Lectures 15-16)

**Lecture 15: Generative Adversarial Networks (GANs)**  
Generator-discriminator framework, adversarial training, mode collapse, training stabilization techniques, and GAN variants (DCGAN, StyleGAN, CycleGAN).

**Lecture 16: Diffusion Models**  
Forward/reverse diffusion processes, denoising diffusion probabilistic models (DDPM), score-based models, applications in image generation (DALL-E, Stable Diffusion).

### **Module 6: Unsupervised Learning** (Lectures 17-18)

**Lecture 17: Clustering and Unsupervised Learning Fundamentals**  
K-means, hierarchical clustering, DBSCAN, dimensionality reduction (PCA, t-SNE, UMAP), and clustering evaluation metrics.

**Lecture 18: Advanced Unsupervised Learning**  
Autoencoders, variational autoencoders (VAE), self-supervised learning, contrastive learning, and representation learning techniques.

### **Module 7: Explainability and Interpretability** (Lectures 19-20)

**Lecture 19: XAI Fundamentals and Traditional Methods**  
Feature importance, permutation importance, partial dependence plots (PDP), LIME, model-agnostic interpretation techniques.

**Lecture 20: SHAP and Deep Learning XAI**  
SHAP values, game theory foundations, Grad-CAM, saliency maps, attention visualization, and interpretability in neural networks.

---

## üõ†Ô∏è Key Skills Acquired

- **Data Processing & Visualization**: Transform raw data into insights through effective visualization and exploratory data analysis
- **Classical ML**: Master supervised learning algorithms, evaluation metrics, and model selection
- **Deep Learning**: Build and train neural networks for various tasks including computer vision and NLP
- **Advanced Architectures**: Implement state-of-the-art models including Transformers, GANs, and Diffusion models
- **Unsupervised Learning**: Extract patterns and structure from unlabeled data
- **Model Interpretation**: Explain and interpret complex model predictions for stakeholders
- **Practical Implementation**: Deploy models considering computational resources and scalability

---

## üìñ Prerequisites

- Python programming proficiency
- Basic linear algebra and calculus
- Probability and statistics fundamentals
- Familiarity with NumPy, Pandas, and Matplotlib

---

## üíª Tools and Technologies

- **Languages**: Python
- **Deep Learning Frameworks**: PyTorch, TensorFlow/Keras
- **Data Processing**: NumPy, Pandas, Scikit-learn
- **Visualization**: Matplotlib, Seaborn, Plotly
- **Cloud Platforms**: AWS, Google Cloud, Azure
- **Version Control**: Git/GitHub

---

## üéì Course Outcomes

Upon completing this course, students will be able to:

1. Design and implement end-to-end machine learning pipelines
2. Select appropriate models and architectures for diverse problem domains
3. Train, optimize, and evaluate deep learning models effectively
4. Work with multiple data modalities (tabular, image, text, time-series)
5. Build and deploy generative AI applications
6. Interpret and explain model predictions to non-technical stakeholders
7. Stay current with state-of-the-art developments in machine learning

---

**Course Materials**: All lecture slides, code examples, and assignments are available in the respective lecture folders.  
**Questions**: Contact the instructor via email for course-related inquiries.