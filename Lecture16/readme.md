# Lecture 16: Generative Models - Diffusion

## ğŸ“‹ Overview

**Instructor:** Ho-min Park  
**Email:** homin.park@ghent.ac.kr | powersimmani@gmail.com

This lecture covers mathematical foundations and practical implementation of Diffusion Models.

---

## ğŸ¯ Learning Objectives

1. Understand Forward/Reverse processes
2. Noise scheduling strategies
3. Implement U-Net architecture
4. Compare DDPM and DDIM algorithms
5. Apply Conditional Diffusion

---

## ğŸ“š Key Topics

**Forward Process**: Gradual noise addition, Markov chain
**Reverse Process**: Noise removal, learnable
**Noise schedule**: Linear, Cosine schedule
**U-Net**: Encoder-Decoder, Skip connections, Attention
**DDIM**: Fast sampling, Deterministic

---

## ğŸ’¡ Key Concepts

- Diffusion enables stable generation
- Forward is fixed, Reverse is learned
- U-Net predicts noise
- Score matching is theoretical foundation
- DDIM accelerates sampling

---

## ğŸ› ï¸ Prerequisites

- Basic Python programming
- Understanding of previous lecture content
- Basic machine learning concepts

---

## ğŸ“– Additional Resources

For detailed code examples, practice materials, and slides, please refer to the original lecture files.
Lecture materials: HTML-based interactive slides provided
