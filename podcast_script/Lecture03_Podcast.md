# Lecture 03 Podcast: ML을 위한 수학 기초 - 선형대수, 확률, 회귀

## 에피소드 정보
- **주제**: 집합론, 선형대수, 확률통계, 선형회귀
- **예상 시간**: 15분
- **대상**: ML 입문자, 수학 기초를 다지려는 분들

---

## 스크립트

**[인트로 - 0:00]**

**Host A**: 안녕하세요! AI 비전 시스템 팟캐스트입니다. 오늘은 ML의 기반이 되는 수학을 다뤄볼 거예요.

**Host B**: 수학이라고 하면 많이들 겁먹는데요, 핵심만 잘 잡으면 괜찮아요! 선형대수, 확률, 그리고 이걸 활용한 선형회귀까지 가볼게요.

**Host A**: 코드만 돌릴 줄 알아도 되지 않나요?

**Host B**: 단순히 쓰기만 하면 가능하죠. 하지만 왜 작동하는지, 언제 어떤 방법이 좋은지 이해하려면 수학이 필수예요. 그리고 문제가 생겼을 때 디버깅하려면요!

---

**[섹션 1: 선형대수 기초 - 벡터와 행렬 - 1:30]**

**Host A**: 선형대수부터 시작해볼까요? 벡터 공간이 뭔가요?

**Host B**: 벡터 공간은 벡터의 덧셈과 스칼라 곱셈이 정의된 집합이에요. ML에서 데이터 포인트 하나가 피처 벡터로 표현되잖아요? 그게 바로 벡터 공간의 원소예요.

**Host A**: 기저(Basis)는요?

**Host B**: 기저는 공간을 표현하는 최소한의 선형 독립 벡터 집합이에요. 3차원 공간이면 3개의 기저 벡터가 필요하죠. 모든 벡터를 기저 벡터의 선형 결합으로 표현할 수 있어요.

**Host A**: 내적(Inner Product)은 어디 쓰이나요?

**Host B**: 내적은 두 벡터의 각 성분을 곱해서 더한 거예요. 중요한 건 코사인 유사도를 구할 수 있다는 거죠! cos(θ) = u·v / (||u|| ||v||). 추천 시스템에서 유저 벡터와 아이템 벡터 유사도 계산할 때 많이 써요.

**Host A**: 직교(Orthogonal)가 뭐죠?

**Host B**: 내적이 0이면 두 벡터가 직교해요. 수직이라는 뜻이죠. PCA에서 주성분들이 서로 직교하거든요. 그래서 정보가 중복되지 않아요.

---

**[섹션 2: 행렬 연산과 고유값 - 3:30]**

**Host A**: 행렬 연산으로 넘어가볼까요?

**Host B**: 행렬 곱셈이 핵심이에요. 뉴럴 네트워크의 레이어가 본질적으로 행렬 곱셈이거든요. (AB)ij = Σk Aik Bkj, 행과 열의 내적이죠.

**Host A**: 역행렬은요?

**Host B**: A⁻¹은 AA⁻¹ = I가 되는 행렬이에요. 정방행렬이고 행렬식(determinant)이 0이 아니어야 존재해요. 선형회귀의 정규방정식에서 쓰이죠.

**Host A**: 고유값과 고유벡터! 이게 PCA의 핵심이라면서요?

**Host B**: 맞아요! Av = λv를 만족하는 λ가 고유값, v가 고유벡터예요. 행렬이 벡터를 변환할 때 방향은 유지하고 크기만 λ배 바꾸는 특별한 벡터죠. PCA는 공분산 행렬의 고유벡터를 찾는 거예요.

**Host A**: SVD도 들어봤어요.

**Host B**: Singular Value Decomposition! A = UΣVᵀ로 어떤 행렬이든 분해할 수 있어요. 추천 시스템, 차원 축소, 노이즈 제거 등 엄청 많이 쓰여요.

---

**[섹션 3: 미분과 그래디언트 - 5:30]**

**Host A**: 미분은 딥러닝의 핵심이죠!

**Host B**: 그래디언트가 핵심이에요. ∇f는 편미분들의 벡터인데, 함수가 가장 가파르게 증가하는 방향을 가리켜요.

**Host A**: 그래서 Gradient Descent가 그래디언트의 반대 방향으로 가는 거군요!

**Host B**: 정확해요! 최소값을 찾으려면 가장 가파르게 내려가는 방향, 즉 -∇f 방향으로 가면 되죠. θ = θ - α∇L(θ)가 업데이트 규칙이에요.

**Host A**: Chain Rule은요?

**Host B**: 합성 함수의 미분이에요. d/dx[f(g(x))] = f'(g(x)) · g'(x). 역전파(Backpropagation)가 바로 Chain Rule의 연속 적용이에요! 각 레이어에서 그래디언트가 곱해지면서 전파되죠.

---

**[섹션 4: 확률론 기초 - 7:00]**

**Host A**: 이제 확률로 넘어가볼까요?

**Host B**: 확률 공간부터 시작해요. 표본 공간 Ω, 사건 공간 F, 확률 함수 P의 삼총사예요. 확률 변수는 결과를 실수로 매핑하는 함수고요.

**Host A**: 분포는 어떤 게 있나요?

**Host B**: 이산형으로는 베르누이, 이항, 포아송이 대표적이에요. 연속형은 균등, 정규(가우시안), 지수 분포가 있고요. 정규분포가 특히 중요한데, 평균 μ와 분산 σ²로 정의되는 종 모양 곡선이에요.

**Host A**: 기댓값과 분산은요?

**Host B**: 기댓값 E[X]는 평균이에요. 분산 Var(X)는 평균에서 얼마나 퍼져있는지를 나타내죠. Var(X) = E[(X-μ)²] = E[X²] - (E[X])². 공분산은 두 변수가 함께 어떻게 변하는지를 측정해요.

---

**[섹션 5: 베이즈 정리와 추정 - 9:00]**

**Host A**: 베이즈 정리! ML에서 정말 많이 나오죠.

**Host B**: P(A|B) = P(B|A) × P(A) / P(B). 사전확률 P(A)에 새 증거 B를 반영해서 사후확률 P(A|B)를 계산하는 거예요.

**Host A**: ML에서 어떻게 쓰이나요?

**Host B**: Naive Bayes 분류기가 대표적이에요. 그리고 MLE와 MAP 추정에서도 중요해요. MLE는 데이터가 주어졌을 때 가장 가능성 높은 파라미터를 찾는 거고, MAP는 여기에 사전 확률까지 고려해요.

**Host A**: MLE가 L2 정규화랑 관련 있다던데요?

**Host B**: 네! MAP에서 가우시안 사전분포를 가정하면 L2 정규화와 같아져요. 라플라시안 사전분포면 L1 정규화고요. 수학적으로 연결되어 있어요.

---

**[섹션 6: 중심극한정리 - 10:30]**

**Host A**: 중심극한정리(CLT)도 중요하다고 했죠?

**Host B**: 엄청 중요해요! 독립적인 랜덤 변수들의 합은 원래 분포가 뭐든 정규분포에 가까워져요. n이 커질수록요.

**Host A**: 왜 중요한 거예요?

**Host B**: 그래서 정규분포가 어디서나 나타나는 거예요! 측정 오차, 생물학적 변이, 금융 수익률... 모두 많은 작은 요인들의 합이니까요. 그리고 신뢰구간 계산의 수학적 근거가 되죠.

**Host A**: 큰 수의 법칙도 있잖아요.

**Host B**: 표본 평균이 모집단 평균으로 수렴한다는 거예요. 샘플 많이 모으면 추정이 정확해진다는 직관적인 사실의 수학적 표현이죠.

---

**[섹션 7: 선형회귀 - 11:30]**

**Host A**: 이제 이 수학을 적용한 선형회귀를 알아볼까요?

**Host B**: 선형회귀의 목표는 y = Xβ + ε에서 최적의 β를 찾는 거예요. 잔차 제곱합(SSR)을 최소화하죠.

**Host A**: 최소제곱법이요?

**Host B**: L(β) = Σ(yi - ŷi)² = ||y - Xβ||²를 최소화해요. 미분해서 0으로 놓으면 정규방정식 β = (XᵀX)⁻¹Xᵀy가 나와요!

**Host A**: 기하학적 해석도 있다면서요?

**Host B**: 네! ŷ = Xβ는 y를 X의 열 공간에 정사영(projection)한 거예요. 잔차 벡터 e = y - ŷ는 X의 열 공간에 직교해요. 그래서 Xᵀe = 0이 성립하죠.

**Host A**: 다중회귀에서 주의할 점은요?

**Host B**: 다중공선성(Multicollinearity)이에요! 독립변수들이 서로 강하게 상관되면 추정이 불안정해져요. VIF(Variance Inflation Factor)로 체크할 수 있어요.

---

**[섹션 8: 회귀 진단 - 13:30]**

**Host A**: 회귀 모델이 잘 맞는지 어떻게 확인해요?

**Host B**: LINE 가정을 체크해요. Linearity(선형성), Independence(독립성), Normality(정규성), Equal variance(등분산성).

**Host A**: 어떤 그래프로 확인하나요?

**Host B**: Residuals vs Fitted 플롯으로 선형성과 등분산성을, Q-Q 플롯으로 정규성을 확인해요. 잔차가 0 주변에 랜덤하게 흩어져야 하고, Q-Q 플롯은 대각선에 가까워야 해요.

**Host A**: R²만 보면 안 되나요?

**Host B**: R²가 높아도 잔차 플롯에서 패턴이 보이면 문제가 있는 거예요. 비선형 관계를 선형으로 억지로 맞추면 R²는 높을 수 있지만 예측이 나빠요. 항상 시각적으로도 확인해야 해요!

---

**[아웃트로 - 14:30]**

**Host A**: 오늘 많은 수학을 다뤘네요! 정리해볼까요?

**Host B**: 선형대수는 데이터를 표현하고 변환하는 언어예요. 벡터, 행렬, 고유값 분해가 핵심이죠.

**Host A**: 확률은 불확실성을 다루는 도구예요. 분포, 베이즈 정리, 추정 방법을 알아야 해요.

**Host B**: 미분과 그래디언트는 최적화의 기반이에요. 딥러닝의 역전파가 Chain Rule이라는 것!

**Host A**: 선형회귀는 이 모든 게 합쳐진 예시예요. 행렬 연산, 미분, 통계적 가정이 다 들어가죠.

**Host B**: 수학이 어렵게 느껴져도, 핵심 개념만 잡으면 ML을 훨씬 깊이 이해할 수 있어요. 다음 에피소드에서 만나요!

**Host A**: 감사합니다!

---

## 핵심 키워드
- Vector Space, Basis, Inner Product, Orthogonality
- Matrix Multiplication, Inverse, Eigenvalue, SVD
- Gradient, Chain Rule, Backpropagation
- Probability Distribution, Expectation, Variance
- Bayes' Theorem, MLE, MAP
- Central Limit Theorem, Law of Large Numbers
- Linear Regression, Normal Equation, R², Residual Analysis
