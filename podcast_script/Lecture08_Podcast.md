# Lecture 08 Podcast: 손실 함수와 최적화 알고리즘의 완벽 가이드

## 에피소드 정보
- **주제**: 손실 함수 설계, 최적화 알고리즘, 학습률 스케줄링
- **예상 시간**: 15분
- **대상**: 딥러닝 모델 학습의 핵심을 이해하고 싶은 모든 분들

---

## 스크립트

**[인트로 - 0:00]**

**Host A**: 안녕하세요! AI 비전 시스템 팟캐스트입니다. 오늘은 모델 학습의 핵심인 손실 함수와 최적화에 대해 다뤄볼 거예요.

**Host B**: 네! 손실 함수를 뭘 쓰느냐, 옵티마이저를 어떻게 설정하느냐에 따라 학습 결과가 완전히 달라질 수 있어요.

**Host A**: Adam이랑 SGD 중에 뭘 써야 하는지, 학습률은 어떻게 조절해야 하는지 항상 고민이었는데요.

**Host B**: 오늘 그 모든 의문을 해결해 드릴게요! 각 방법의 원리와 언제 뭘 써야 하는지 알아봅시다.

---

**[섹션 1: 손실 함수의 역할 - 1:30]**

**Host A**: 먼저 손실 함수가 뭔지부터 설명해주세요.

**Host B**: 손실 함수는 모델의 예측이 실제 값과 얼마나 다른지 수치화하는 거예요. 모델 학습의 나침반이라고 할 수 있죠. 이 값을 최소화하는 방향으로 파라미터를 업데이트해요.

**Host A**: 좋은 손실 함수의 조건은요?

**Host B**: 첫째, 미분 가능해야 해요. 그래디언트 기반 최적화를 쓰니까요. 둘째, 태스크에 적합해야 해요. 회귀면 MSE, 분류면 Cross-Entropy처럼요. 셋째, 스케일이 적절해야 학습이 안정적이에요.

**Host A**: 손실 함수 선택이 성능에 영향을 주나요?

**Host B**: 크게요! 같은 모델이라도 손실 함수에 따라 수렴 속도, 최종 성능, 학습 안정성이 달라져요.

---

**[섹션 2: 회귀 손실 함수 - 3:00]**

**Host A**: 회귀 문제에서 쓰는 손실 함수들을 알아볼까요?

**Host B**: 가장 기본은 MSE, Mean Squared Error예요. 예측과 실제의 차이를 제곱해서 평균 내는 거죠. 큰 오차에 더 큰 페널티를 줘요.

**Host A**: 단점은요?

**Host B**: 이상치에 민감해요! 하나의 큰 오차가 전체 손실을 지배할 수 있어요. 그래서 MAE, Mean Absolute Error도 있어요. 절댓값을 쓰니까 이상치에 덜 민감하죠.

**Host A**: MAE는 왜 항상 쓰지 않나요?

**Host B**: 0에서 미분이 안 돼요. 그래디언트가 불연속이라 최적화가 좀 불안정할 수 있어요. 그래서 Huber Loss가 나왔어요!

**Host A**: Huber Loss는 뭐예요?

**Host B**: 작은 오차에서는 MSE처럼 부드럽게, 큰 오차에서는 MAE처럼 선형으로 동작해요. 델타 파라미터로 전환점을 조절하고요. 둘의 장점을 합친 거죠!

---

**[섹션 3: 분류 손실 함수 - 5:00]**

**Host A**: 분류 문제의 표준은 Cross-Entropy죠?

**Host B**: 네! Binary Cross-Entropy는 마이너스 y log p 더하기 (1-y) log (1-p) 형태예요. 확률 출력과 정답의 차이를 측정해요.

**Host A**: 왜 MSE 대신 이걸 써요?

**Host B**: Cross-Entropy는 틀린 예측에 훨씬 더 큰 페널티를 줘요. 특히 자신 있게 틀렸을 때요! 확률이 0.99인데 정답이 0이면 손실이 엄청 커져요. 학습 신호가 더 명확하죠.

**Host A**: Hinge Loss도 있다던데요?

**Host B**: SVM에서 쓰는 손실이에요. max(0, 1 - y*f(x)) 형태로, 마진을 최대화하는 방향으로 학습해요. 분류 경계 근처의 샘플들에 집중하죠.

---

**[섹션 4: 특수 목적 손실 함수 - 6:30]**

**Host A**: 클래스 불균형 문제에서 쓰는 손실 함수도 있죠?

**Host B**: Focal Loss가 대표적이에요! Cross-Entropy에 (1-p)^gamma 가중치를 곱해요. 쉬운 샘플의 손실을 줄이고 어려운 샘플에 집중하게 해요.

**Host A**: 어디서 처음 나왔어요?

**Host B**: RetinaNet 객체 검출 모델에서요. 배경 vs 객체 불균형이 심한데, Focal Loss로 해결했어요.

**Host A**: Metric Learning에서 쓰는 손실도 있죠?

**Host B**: Contrastive Loss와 Triplet Loss가 있어요! Contrastive는 비슷한 쌍은 가깝게, 다른 쌍은 멀게. Triplet Loss는 앵커, 포지티브, 네거티브 세 샘플을 써서 상대적 거리를 학습해요.

**Host A**: 어디서 쓰여요?

**Host B**: 얼굴 인식이 대표적이에요. FaceNet이 Triplet Loss로 유명하죠. 임베딩 공간에서 같은 사람은 가깝게, 다른 사람은 멀게 배치해요.

---

**[섹션 5: 경사 하강법과 변형들 - 8:00]**

**Host A**: 이제 최적화 알고리즘으로 넘어가볼까요?

**Host B**: 기본은 Gradient Descent예요. theta를 theta 빼기 학습률 곱하기 그래디언트로 업데이트해요. 손실을 줄이는 방향으로 파라미터를 조금씩 움직이는 거죠.

**Host A**: Batch, Mini-batch, Stochastic의 차이는요?

**Host B**: Batch는 전체 데이터로 그래디언트를 계산해요. 정확하지만 느리고 메모리를 많이 먹어요. Stochastic은 샘플 하나씩 처리해서 빠르지만 노이즈가 많아요.

**Host A**: Mini-batch가 절충안이죠?

**Host B**: 네! 보통 32-256 샘플 정도로 배치를 만들어요. 정확도와 속도의 균형이 좋고, 실제로 가장 많이 써요.

---

**[섹션 6: 모멘텀과 NAG - 9:30]**

**Host A**: 모멘텀은 뭐예요?

**Host B**: 물리의 관성 개념이에요! 이전 업데이트 방향을 기억해서 같은 방향이면 가속하고, 방향이 바뀌면 완충해줘요. 수렴이 빨라지고 진동이 줄어요.

**Host A**: 공식으로 설명해주세요.

**Host B**: 속도 v에 이전 속도의 0.9배를 더하고, 현재 그래디언트를 빼요. 그다음 파라미터에 이 속도를 더해요. 언덕을 내려가는 공이 관성으로 계속 굴러가는 것처럼요.

**Host A**: NAG는 뭐가 다른 거예요?

**Host B**: Nesterov Accelerated Gradient는 "미리 보고 점프"해요. 모멘텀 방향으로 먼저 이동한 위치에서 그래디언트를 계산해요. 최솟값 근처에서 더 빠르게 멈출 수 있어요.

---

**[섹션 7: 적응형 학습률 - 11:00]**

**Host A**: AdaGrad, RMSprop, Adam 같은 적응형 방법들이 있죠?

**Host B**: AdaGrad는 파라미터마다 다른 학습률을 적용해요. 많이 업데이트된 파라미터는 학습률을 낮추고, 적게 업데이트된 건 높여요.

**Host A**: 문제가 있다던데요?

**Host B**: 학습률이 계속 감소해서 나중에 거의 0이 돼요. 학습이 멈춰버리죠. RMSprop이 이걸 해결했어요. 지수이동평균을 써서 최근 그래디언트에 더 가중치를 줘요.

**Host A**: Adam은요?

**Host B**: Adam은 모멘텀과 RMSprop을 합친 거예요! 1차 모멘트(평균 방향)와 2차 모멘트(분산)를 모두 추적해요. 바이어스 보정도 하고요. 기본값으로 거의 항상 잘 작동해서 가장 인기 있어요.

**Host A**: AdamW는요?

**Host B**: Weight decay를 Adam에 제대로 적용한 버전이에요. L2 정규화를 그래디언트에 섞지 않고 분리해서 적용해요. 일반화 성능이 더 좋아서 요즘은 AdamW를 많이 써요.

---

**[섹션 8: 학습률 스케줄링 - 12:30]**

**Host A**: 학습률이 왜 그렇게 중요해요?

**Host B**: 딥러닝에서 가장 중요한 하이퍼파라미터예요! 너무 크면 발산하거나 진동하고, 너무 작으면 학습이 너무 느리거나 지역 최솟값에 갇혀요.

**Host A**: 고정 학습률만 쓰면 안 돼요?

**Host B**: 초반에는 큰 학습률로 빠르게 탐색하고, 후반에는 작은 학습률로 세밀하게 조정하는 게 좋아요. 그래서 스케줄링이 필요해요!

**Host A**: 어떤 방법들이 있어요?

**Host B**: Step Decay는 일정 에폭마다 10분의 1로 줄여요. Cosine Annealing은 코사인 곡선을 따라 부드럽게 감소해요. 웜업은 처음에 학습률을 0에서 시작해서 서서히 올려요. Transformer 학습에 필수죠!

**Host A**: 1cycle Policy도 있다던데요?

**Host B**: 학습률을 올렸다가 내리는 사이클을 한 번 돌아요. fast.ai에서 유명해진 방법인데, 빠르게 좋은 결과를 낼 때 좋아요.

---

**[아웃트로 - 14:30]**

**Host A**: 오늘 내용을 정리해볼까요?

**Host B**: 첫째, 손실 함수는 태스크에 맞게 선택해요. 회귀는 MSE나 Huber, 분류는 Cross-Entropy!

**Host A**: 둘째, 불균형 데이터에는 Focal Loss, 유사도 학습에는 Triplet Loss 같은 특수 손실을 고려하세요.

**Host B**: 셋째, Adam이 기본 선택으로 좋고, AdamW가 정규화까지 잘 처리해요. SGD+Momentum도 특정 상황에서 더 좋을 수 있어요.

**Host A**: 마지막으로, 학습률 스케줄링은 필수예요! 웜업, Cosine, Step Decay 중 태스크에 맞게 선택하세요!

**Host B**: 다음 시간에는 초기화와 정규화 기법에 대해 다룰 거예요!

**Host A**: 감사합니다!

---

## 핵심 키워드
- Loss Function, MSE, MAE, Huber Loss
- Cross-Entropy, Focal Loss, Hinge Loss
- Contrastive Loss, Triplet Loss, Metric Learning
- L1/L2 Regularization, Weight Decay
- Gradient Descent, Mini-batch, Stochastic
- Momentum, Nesterov Accelerated Gradient
- AdaGrad, RMSprop, Adam, AdamW
- Learning Rate Scheduling, Warm-up, Step Decay
- Cosine Annealing, 1cycle Policy
