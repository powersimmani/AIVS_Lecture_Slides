# Lecture 13 Podcast: Transformer - 어텐션이 전부다

## 에피소드 정보
- **주제**: Transformer 아키텍처, 셀프 어텐션, 멀티헤드 어텐션, 포지셔널 인코딩
- **예상 시간**: 15분
- **대상**: ML/DL을 공부하는 학생 및 실무자

---

## 스크립트

**[인트로 - 0:00]**

**Host A**: 안녕하세요! AI 비전 시스템 팟캐스트에 오신 것을 환영합니다. 저는 호스트 A이고요.

**Host B**: 안녕하세요, 호스트 B입니다! 오늘은 드디어 현대 AI의 핵심, Transformer에 대해 다뤄볼 거예요. BERT, GPT, ChatGPT, Claude... 이 모든 모델의 기반이 되는 아키텍처죠!

**Host A**: 맞아요. 2017년에 나온 "Attention Is All You Need" 논문이 AI 역사를 완전히 바꿔놓았어요. 오늘 그 비밀을 파헤쳐봅시다!

---

**[섹션 1: Transformer의 등장 배경 - 1:00]**

**Host B**: Transformer가 왜 등장했는지부터 알아볼까요? RNN 기반 Seq2Seq의 한계점들이 있었어요.

**Host A**: 첫째, 순차적 연산 문제예요. RNN은 한 타임스텝씩 처리해야 해서 병렬화가 안 돼요. 시퀀스 길이에 비례해서 학습 시간이 늘어나죠.

**Host B**: 둘째, 여전히 장기 의존성 문제가 있어요. LSTM, GRU로 개선했지만 아주 긴 시퀀스에서는 여전히 어려워요. 정보가 여러 단계를 거쳐야 하니까요.

**Host A**: 셋째, 어텐션을 추가해도 여전히 RNN 백본을 사용해요. 속도 향상에 한계가 있죠.

**Host B**: Transformer의 핵심 아이디어는 "어텐션만으로 충분하다"예요. RNN을 완전히 제거하고 어텐션만으로 시퀀스를 처리하는 거죠!

---

**[섹션 2: 셀프 어텐션 - 2:30]**

**Host A**: 지난 시간에 배운 어텐션과 셀프 어텐션의 차이가 뭔가요?

**Host B**: 지난 시간 어텐션은 "교차 어텐션"이에요. 디코더가 인코더 출력에 어텐션하죠. Query는 디코더에서, Key와 Value는 인코더에서 왔어요.

**Host A**: 셀프 어텐션은요?

**Host B**: 셀프 어텐션은 같은 시퀀스 내에서 어텐션해요. Query, Key, Value가 모두 같은 시퀀스에서 나와요. 각 위치가 시퀀스 내 다른 모든 위치에 어텐션하는 거죠.

**Host A**: 어떻게 계산하나요?

**Host B**: 입력 임베딩 X에서 세 개의 행렬을 만들어요. Q = X * W_Q, K = X * W_K, V = X * W_V. 여기서 W_Q, W_K, W_V는 학습되는 가중치 행렬이에요.

**Host A**: 그다음은요?

**Host B**: 어텐션 공식이에요. Attention(Q, K, V) = softmax(QK^T / sqrt(d_k)) * V. Q와 K의 점곱으로 스코어를 계산하고, 루트 d_k로 스케일링하고, 소프트맥스 적용 후 V와 곱해요.

---

**[섹션 3: 스케일링의 중요성 - 4:30]**

**Host A**: 왜 sqrt(d_k)로 나눠야 하나요?

**Host B**: 좋은 질문이에요! 스케일링을 안 하면 점곱 값이 차원에 따라 커져요. d_k가 64면 값이 꽤 커질 수 있죠.

**Host A**: 값이 크면 왜 문제인가요?

**Host B**: 소프트맥스가 매우 뾰족해져요. 하나의 위치에 거의 모든 가중치가 몰리고 나머지는 0에 가까워져요. 그러면 기울기가 거의 0이 되어서 학습이 안 돼요.

**Host A**: 스케일링하면요?

**Host B**: 분산을 대략 1로 유지해서 소프트맥스가 적당히 부드러워져요. 기울기가 잘 흘러서 학습이 안정적으로 되죠.

---

**[섹션 4: 멀티헤드 어텐션 - 5:30]**

**Host A**: 멀티헤드 어텐션은 왜 필요한가요?

**Host B**: 단일 어텐션은 한 가지 유형의 관계만 포착해요. 하지만 언어에는 여러 종류의 관계가 있죠. 구문적 관계, 의미적 관계, 위치적 관계, 대명사 해소 같은 것들이요.

**Host A**: 그래서요?

**Host B**: 여러 개의 어텐션을 병렬로 실행해요. 각 "헤드"가 다른 종류의 관계를 학습하도록요. 마지막에 모든 헤드 출력을 연결하고 선형 변환해요.

**Host A**: 수식으로 어떻게 되나요?

**Host B**: head_i = Attention(Q*W_i^Q, K*W_i^K, V*W_i^V)로 각 헤드를 계산하고, MultiHead = Concat(head_1, ..., head_h) * W_O로 합쳐요.

**Host A**: 파라미터 수는 어떻게 되나요?

**Host B**: 기본 설정이 d_model=512, 8개 헤드면 각 헤드는 d_k = 512/8 = 64 차원이에요. 전체 연산량은 단일 512차원 어텐션과 비슷하지만, 8가지 다른 패턴을 포착할 수 있어요!

---

**[섹션 5: 포지셔널 인코딩 - 7:30]**

**Host A**: 셀프 어텐션에 큰 문제가 하나 있다고 했죠?

**Host B**: 네! 셀프 어텐션은 순서를 모른다는 거예요. 순서 불변성이라고 하는데, "cat sat mat"이든 "mat cat sat"이든 같은 결과가 나와요.

**Host A**: 그러면 "Dog bites man"과 "Man bites dog"이 구분이 안 되겠네요?

**Host B**: 바로 그거예요! RNN은 순차 처리하면서 자연스럽게 순서 정보가 들어가는데, Transformer는 명시적으로 위치 정보를 넣어줘야 해요.

**Host A**: 어떻게 넣어주나요?

**Host B**: 포지셔널 인코딩을 토큰 임베딩에 더해줘요. 원래 Transformer는 사인/코사인 함수를 사용한 고정 인코딩을 썼어요.

**Host A**: 수식이 어떻게 되나요?

**Host B**: PE(pos, 2i) = sin(pos / 10000^(2i/d_model)), PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))이에요. 짝수 차원은 사인, 홀수 차원은 코사인이고, 각 차원마다 다른 주파수를 사용해요.

**Host A**: 왜 이 방식이 좋은가요?

**Host B**: 몇 가지 좋은 특성이 있어요. 각 위치가 고유한 인코딩을 갖고, 값이 -1과 1 사이로 제한되고, 상대적 위치를 선형 변환으로 표현할 수 있어요. 그리고 학습 때 본 것보다 긴 시퀀스도 처리할 수 있어요!

---

**[섹션 6: Transformer 전체 구조 - 9:30]**

**Host A**: 이제 Transformer 전체 구조를 살펴볼까요?

**Host B**: Transformer는 인코더와 디코더로 구성돼요. 인코더는 입력을 처리하고, 디코더는 출력을 생성해요. 각각 N개의 동일한 레이어를 쌓아요. 보통 N=6이에요.

**Host A**: 인코더 레이어 구조는요?

**Host B**: 두 부분으로 구성돼요. 첫째, 멀티헤드 셀프 어텐션. 각 위치가 모든 위치에 어텐션해요. 둘째, Feed-Forward Network, 즉 FFN. 위치별로 독립적으로 적용되는 완전연결층이에요.

**Host A**: FFN이 뭐 하는 건가요?

**Host B**: FFN(x) = ReLU(xW_1 + b_1)W_2 + b_2예요. 차원을 확장했다가 다시 축소해요. 보통 d_ff = 4 * d_model이에요. 비선형성을 추가하고 위치별 변환을 학습해요.

**Host A**: 각 서브레이어 주변에 뭔가 더 있죠?

**Host B**: 잔차 연결과 레이어 정규화가 있어요. output = LayerNorm(x + SubLayer(x)) 형태예요. 잔차 연결은 기울기 흐름을 돕고, 레이어 정규화는 학습을 안정화시켜요.

---

**[섹션 7: 디코더와 마스킹 - 11:00]**

**Host A**: 디코더는 인코더와 어떻게 다른가요?

**Host B**: 디코더 레이어는 세 부분이에요. 첫째, 마스크드 멀티헤드 셀프 어텐션. 둘째, 인코더-디코더 교차 어텐션. 셋째, FFN.

**Host A**: 마스크드 셀프 어텐션이 뭔가요?

**Host B**: 디코더는 자기회귀적이에요. 출력을 하나씩 생성하니까 현재 위치에서 미래 위치를 보면 안 돼요. 인과적 마스크로 미래를 가려요.

**Host A**: 마스크가 어떻게 생겼어요?

**Host B**: 상삼각 행렬이에요. 위치 i는 위치 1부터 i까지만 볼 수 있어요. 미래 위치의 스코어를 음의 무한대로 설정하면 소프트맥스 후에 0이 돼요.

**Host A**: 교차 어텐션은요?

**Host B**: 지난 시간에 배운 그 어텐션이에요! Query는 디코더에서, Key와 Value는 인코더 출력에서 와요. 디코더가 입력 시퀀스 전체를 참조할 수 있게 해줘요.

---

**[섹션 8: 학습과 추론 - 12:30]**

**Host A**: Transformer 학습은 어떻게 하나요?

**Host B**: 학습할 때는 Teacher Forcing을 사용하고, 마스킹 덕분에 모든 디코더 위치를 병렬로 계산할 수 있어요. 전체 시퀀스가 한 번의 순전파로 처리돼요!

**Host A**: 추론할 때는요?

**Host B**: 추론은 자기회귀적이에요. 한 토큰씩 생성해야 해요. 각 단계에서 이전 출력들을 입력으로 사용해요. 여기서 KV-캐시가 중요해요.

**Host A**: KV-캐시가 뭔가요?

**Host B**: 이전 위치들의 Key와 Value를 저장해두는 거예요. 새 토큰 생성할 때 이전 것들을 다시 계산할 필요 없이 캐시에서 가져와요. 추론 속도가 훨씬 빨라져요.

---

**[섹션 9: 구현 팁과 응용 - 13:30]**

**Host A**: 실제 구현할 때 주의할 점들이 있나요?

**Host B**: 많아요! 먼저 학습률 스케줄이 중요해요. 웜업 단계에서 학습률을 점진적으로 올리고, 그 후에 역제곱근으로 감소시켜요. 웜업 없이 바로 시작하면 학습이 불안정해요.

**Host A**: 다른 팁은요?

**Host B**: 그래디언트 클리핑 필수고요, 드롭아웃은 어텐션 가중치 후, FFN 활성화 후, 임베딩 후에 적용해요. 레이블 스무딩도 일반화에 도움돼요.

**Host A**: Transformer는 어디에 쓰이나요?

**Host B**: 처음에는 기계 번역용이었지만 지금은 모든 곳에 쓰여요! NLP에서는 BERT, GPT 같은 모델의 기반이고, 비전에서는 ViT로 이미지를 처리하고, 음성, 멀티모달, 심지어 단백질 구조 예측에도 쓰여요.

---

**[아웃트로 - 14:30]**

**Host A**: 오늘 정말 많은 내용을 다뤘네요! 정리해볼까요?

**Host B**: 첫째, Transformer는 RNN을 완전히 제거하고 어텐션만으로 시퀀스를 처리해요. 덕분에 완전 병렬화가 가능해요.

**Host A**: 둘째, 셀프 어텐션은 시퀀스 내 모든 위치 쌍 사이의 관계를 직접 모델링하고, 멀티헤드로 다양한 패턴을 포착해요.

**Host B**: 셋째, 포지셔널 인코딩으로 순서 정보를 주입하고, 인코더는 양방향, 디코더는 인과적 마스킹을 사용해요.

**Host A**: 넷째, 잔차 연결과 레이어 정규화가 깊은 네트워크 학습을 가능하게 해요. 이게 현대 AI의 기반이에요!

**Host B**: 다음 시간에는 BERT, GPT 같은 사전학습 언어 모델을 배워요. Transformer가 어떻게 혁명을 일으켰는지 더 깊이 알게 될 거예요!

**Host A**: 감사합니다! 다음 시간에 만나요!

---

## 핵심 키워드
- Transformer, Attention Is All You Need
- Self-Attention, Cross-Attention
- Query, Key, Value, Scaled Dot-Product Attention
- Multi-Head Attention, Attention Head
- Positional Encoding, Sinusoidal Encoding, Learned Positional Embedding
- Encoder, Decoder, Encoder-Decoder
- Causal Mask, Padding Mask
- Feed-Forward Network (FFN), Residual Connection, Layer Normalization
- KV-Cache, Learning Rate Warmup
- BERT, GPT, ViT
