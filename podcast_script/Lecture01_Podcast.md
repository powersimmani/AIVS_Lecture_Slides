# Lecture 01 Podcast: ML 하드웨어 기초와 분산 학습 입문

## 에피소드 정보
- **주제**: 데이터 표현, 메모리 구조, 네트워크와 분산 ML
- **예상 시간**: 15분
- **대상**: ML/DL을 공부하는 학생 및 실무자

---

## 스크립트

**[인트로 - 0:00]**

**Host A**: 안녕하세요! AI 비전 시스템 팟캐스트에 오신 것을 환영합니다. 저는 호스트 A이고요.

**Host B**: 안녕하세요, 호스트 B입니다! 오늘은 정말 중요한 주제를 다룰 거예요. 머신러닝을 하면서 누구나 한 번쯤 궁금해했을 "왜 GPU가 필요한지", "메모리는 어떻게 관리해야 하는지" 같은 하드웨어 기초에 대해 얘기해볼 겁니다.

**Host A**: 맞아요. 솔직히 처음 ML 공부할 때 코드만 돌리면 되는 줄 알았는데, 실제로 큰 모델 학습시키려면 하드웨어 이해가 필수더라고요.

---

**[섹션 1: 데이터 타입과 비트 - 1:00]**

**Host B**: 자, 그럼 가장 기본부터 시작해볼까요? 컴퓨터에서 데이터가 어떻게 표현되는지 아시나요?

**Host A**: 네, 1 바이트가 8 비트라는 건 알죠. 근데 ML에서 이게 왜 중요한 거예요?

**Host B**: 좋은 질문이에요! ML에서는 데이터 타입 선택이 메모리 사용량과 직결되거든요. 예를 들어볼게요. FP32, 즉 32비트 부동소수점은 4바이트를 차지해요. PyTorch나 TensorFlow의 기본 타입이죠.

**Host A**: 아, 그래서 10억 개 파라미터 모델이면 4GB가 필요한 거군요! 10억 곱하기 4바이트.

**Host B**: 정확해요! 그런데 FP16으로 바꾸면 2바이트니까 2GB로 줄어들어요. 메모리 절반 절약이죠. INT8이면? 1바이트니까 1GB만 있으면 돼요.

**Host A**: 와, 75%나 줄일 수 있네요. 근데 정밀도가 떨어지지 않나요?

**Host B**: 그게 바로 quantization, 양자화의 핵심 trade-off예요. 다행히 제대로 된 양자화 기법을 쓰면 정확도 손실이 1-2% 정도밖에 안 돼요. 모바일이나 엣지 디바이스 배포할 때 정말 유용하죠.

---

**[섹션 2: CPU vs GPU - 3:30]**

**Host A**: 자, 이제 모든 ML 입문자의 질문으로 넘어가볼까요? "왜 GPU가 필요해요?"

**Host B**: 하하, 클래식한 질문이죠! 핵심은 병렬 처리예요. CPU는 강력한 코어가 4개에서 64개 정도 있어요. 복잡한 순차 작업에 최적화되어 있죠.

**Host A**: 반면 GPU는요?

**Host B**: GPU는 수천에서 수만 개의 간단한 코어가 있어요. RTX 4090만 해도 16,384개의 CUDA 코어가 있거든요! 행렬 연산에서 10배에서 100배 더 빨라요.

**Host A**: 딥러닝이 본질적으로 행렬 연산의 연속이니까요. 행렬 곱셈, 컨볼루션, 어텐션 다 병렬화 가능하고.

**Host B**: 바로 그거예요! NVIDIA의 CUDA 플랫폼이 2007년에 나왔는데, 그게 딥러닝 혁명의 기반이 됐죠. PyTorch랑 TensorFlow가 자동으로 CUDA를 활용해서 GPU 가속을 해줘요.

**Host A**: FLOPS라는 단위도 자주 보이는데, 이게 뭐예요?

**Host B**: 대문자 FLOPS는 초당 부동소수점 연산 횟수로, 하드웨어 성능 지표예요. A100이 312 TFLOPS고, H100은 1000 TFLOPS니까 3배 이상 빠른 거죠. 소문자 FLOPs는 모델 복잡도를 나타내고요.

---

**[섹션 3: 메모리 계층 구조 - 6:00]**

**Host A**: GPU 얘기 나왔으니까 메모리 얘기도 해야겠네요. VRAM이 항상 부족하다고 하잖아요.

**Host B**: 메모리 계층 구조를 이해하면 왜 그런지 알 수 있어요. 가장 빠른 건 레지스터, 1 사이클이면 접근 가능해요. 그다음 L1 캐시가 4 사이클, L2가 10 사이클, L3가 40 사이클...

**Host A**: RAM은요?

**Host B**: RAM은 100 사이클 정도 걸려요. SSD 스토리지는 무려 10,000 사이클이고요. 그래서 데이터를 가능한 프로세서 가까이 두는 게 중요해요.

**Host A**: VRAM이랑 일반 RAM 차이는 뭐예요?

**Host B**: VRAM은 GPU 전용 메모리인데, 대역폭이 훨씬 높아요. HBM3 VRAM은 초당 900GB 정도 전송 가능한데, DDR5 RAM은 50GB 정도밖에 안 돼요.

**Host A**: LLaMA-70B 같은 대형 모델은 어떻게 돌려요? FP16으로도 140GB나 필요한데, 단일 GPU VRAM으로는 안 되잖아요.

**Host B**: 세 가지 방법이 있어요. 첫째, 모델 샤딩으로 여러 GPU에 분산시키기. 둘째, 오프로딩으로 일부를 RAM에 두고 필요할 때 로딩하기. 셋째, 양자화로 8비트나 4비트로 줄이기. Hugging Face Accelerate나 DeepSpeed ZeRO 같은 도구들이 이걸 쉽게 해줘요.

---

**[섹션 4: 배치 사이즈와 메모리 - 8:30]**

**Host A**: OOM 에러, 다들 한 번쯤 겪어봤을 거예요. "CUDA out of memory"...

**Host B**: 트라우마죠! 학습 중 메모리 사용량은 네 가지로 구성돼요. 모델 파라미터, 옵티마이저 상태, 그래디언트, 그리고 활성화 값.

**Host A**: 배치 사이즈가 커지면 어디가 늘어나요?

**Host B**: 활성화 메모리가 배치 사이즈와 시퀀스 길이에 비례해서 늘어나요. BERT-base로 배치 32, 시퀀스 512 쓰면 활성화 메모리만 8GB 정도 필요해요.

**Host A**: 해결책은요?

**Host B**: Gradient Accumulation이 있어요. 배치를 작은 마이크로배치로 나눠서 처리하는 거죠. 메모리는 적게 쓰면서 큰 배치 효과를 낼 수 있어요. Gradient Checkpointing도 있는데, 이건 활성화 값을 저장 안 하고 나중에 다시 계산하는 거예요. 메모리와 연산을 교환하는 셈이죠.

**Host A**: 일단 OOM 나면 배치 사이즈 절반으로 줄이라는 룰도 있더라고요.

**Host B**: 네, 실용적인 팁이에요. nvidia-smi나 torch.cuda.memory_allocated()로 모니터링하면서 조절하면 돼요.

---

**[섹션 5: 혼합 정밀도 학습 - 10:30]**

**Host A**: Mixed Precision Training도 요즘 기본이 됐죠?

**Host B**: 네! 아이디어는 간단해요. FP16으로 속도를 높이고, FP32로 안정성을 유지하는 거예요. 메모리 50% 절약에 Tensor Core 활용하면 2-3배 속도 향상도 가능해요.

**Host A**: Loss Scaling이 필요하다던데요?

**Host B**: FP16은 표현 범위가 좁아서 작은 그래디언트가 언더플로우 날 수 있어요. Loss를 크게 스케일업했다가 나중에 다시 스케일다운하는 방식으로 해결해요. PyTorch의 AMP, Automatic Mixed Precision을 쓰면 autocast context manager 안에서 자동으로 처리돼요.

---

**[섹션 6: 분산 학습 기초 - 12:00]**

**Host A**: 이제 네트워크랑 분산 학습 얘기로 넘어가볼까요?

**Host B**: 분산 학습은 크게 두 가지 패러다임이 있어요. Data Parallelism과 Model Parallelism.

**Host A**: Data Parallelism은 같은 모델을 복제해서 다른 데이터를 처리하는 거죠?

**Host B**: 맞아요! 각 GPU가 미니배치를 처리하고, 그래디언트를 동기화해요. PyTorch에서는 DDP, Distributed Data Parallel을 추천해요. 모델이 단일 GPU에 들어갈 때 쓰면 돼요.

**Host A**: Model Parallelism은요?

**Host B**: 모델 자체를 여러 GPU에 나누는 거예요. 모델이 너무 커서 한 GPU에 안 들어갈 때 필요하죠. Pipeline Parallelism은 레이어를 순차적으로 나누고, Tensor Parallelism은 연산 자체를 분할해요.

**Host A**: 네트워크 대역폭도 중요하겠네요?

**Host B**: 매우 중요해요! 노드 내에서는 NVLink가 초당 600GB로 빠르지만, 노드 간에는 InfiniBand가 200-400Gbps 정도고, 일반 이더넷은 더 느려요. Ring-AllReduce 같은 효율적인 그래디언트 집계 알고리즘도 중요하고요.

---

**[섹션 7: 실용적인 도구들 - 13:30]**

**Host A**: 실제로 원격 서버에서 학습할 때 알아야 할 것들은요?

**Host B**: SSH가 기본이에요! ssh username@hostname으로 접속하고, 키 기반 인증 설정해두면 편하죠. SCP나 rsync로 파일 전송하고요.

**Host A**: TensorBoard 보려면요?

**Host B**: 포트 포워딩이요! ssh -L 6006:localhost:6006 이런 식으로 원격 서버의 TensorBoard를 로컬에서 볼 수 있어요. tmux는 필수예요. 긴 학습 돌릴 때 세션 끊겨도 계속 실행되니까요.

**Host A**: Docker도 많이 쓰죠?

**Host B**: 환경 재현성 때문에 거의 필수가 됐어요. nvidia/cuda 이미지 위에 필요한 라이브러리 올리면 어디서든 똑같은 환경으로 돌릴 수 있어요. --gpus all 플래그로 GPU 접근도 간단하고요.

---

**[아웃트로 - 14:30]**

**Host A**: 오늘 정말 많은 내용을 다뤘네요! 정리하자면요?

**Host B**: 첫째, 데이터 타입 선택이 메모리와 속도에 큰 영향을 줘요. 양자화로 75%까지 메모리를 줄일 수 있고요.

**Host A**: 둘째, GPU가 ML에 필수인 이유는 병렬 처리 때문이에요. 수천 개의 코어가 행렬 연산을 동시에 처리하니까요.

**Host B**: 셋째, 메모리 계층 구조를 이해하고, 배치 사이즈 조절이나 Mixed Precision으로 효율적으로 사용하는 게 중요해요.

**Host A**: 마지막으로, 분산 학습과 원격 서버 활용 기술은 대규모 모델 학습의 필수 스킬이에요!

**Host B**: 다음 에피소드에서는 리눅스 기초와 환경 설정에 대해 다룰 예정이에요. 구독과 좋아요 부탁드려요!

**Host A**: 감사합니다! 다음 시간에 만나요!

---

## 핵심 키워드
- FP32, FP16, INT8, Quantization
- GPU, CUDA, Tensor Cores, FLOPS
- Memory Hierarchy, VRAM, Bandwidth
- Batch Size, Gradient Accumulation, Mixed Precision
- Data Parallelism, Model Parallelism, DDP
- SSH, Docker, tmux
