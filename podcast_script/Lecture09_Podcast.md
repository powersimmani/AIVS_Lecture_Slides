# Lecture 09 Podcast: 초기화, 정규화, 그리고 일반화 전략

## 에피소드 정보
- **주제**: 가중치 초기화, 정규화 기법, 드롭아웃과 데이터 증강
- **예상 시간**: 15분
- **대상**: 딥러닝 모델의 학습 안정성과 일반화를 개선하고 싶은 분들

---

## 스크립트

**[인트로 - 0:00]**

**Host A**: 안녕하세요! AI 비전 시스템 팟캐스트입니다. 오늘은 딥러닝 학습의 숨은 영웅들에 대해 이야기해볼 거예요.

**Host B**: 숨은 영웅이라니, 뭔가요?

**Host A**: 초기화, 정규화, 드롭아웃 같은 기법들이요! 화려하진 않지만 이것들 없이는 깊은 네트워크 학습이 불가능해요.

**Host B**: 아, 맞아요! 처음 딥러닝 할 때 왜 초기화가 중요한지, 왜 BatchNorm을 쓰는지 잘 몰랐는데요.

**Host A**: 오늘 그 모든 것을 파헤쳐 봅시다!

---

**[섹션 1: 초기화가 왜 중요한가 - 1:30]**

**Host A**: 가중치를 왜 신경 써서 초기화해야 해요? 그냥 0으로 시작하면 안 돼요?

**Host B**: 절대 안 돼요! 0으로 초기화하면 대칭성 문제가 생겨요. 모든 뉴런이 똑같은 그래디언트를 받아서 똑같이 업데이트되거든요.

**Host A**: 그러면 뭐가 문제예요?

**Host B**: 100개 뉴런이 있어도 실제로는 1개랑 같아져요. 다양한 특징을 학습할 수 없게 되죠. 그래서 랜덤 초기화로 대칭을 깨줘야 해요.

**Host A**: 랜덤이면 아무 값이나 괜찮아요?

**Host B**: 아니요! 스케일이 중요해요. 너무 크면 활성화 값이 폭발하고, 너무 작으면 신호가 사라져요. 그래서 Xavier와 He 초기화 같은 방법이 나온 거예요.

---

**[섹션 2: 기울기 소실과 폭발 - 3:00]**

**Host A**: Vanishing Gradient, 기울기 소실 문제를 설명해주세요.

**Host B**: 역전파할 때 그래디언트는 체인 룰로 곱해지잖아요. 각 레이어의 그래디언트가 1보다 작으면, 깊어질수록 기하급수적으로 작아져요.

**Host A**: 예를 들면요?

**Host B**: Sigmoid의 최대 그래디언트가 0.25예요. 10개 레이어면 0.25의 10승, 거의 0에 가까워요! 초기 레이어는 거의 학습이 안 돼요.

**Host A**: 폭발하는 경우도 있죠?

**Host B**: 네, 반대로 그래디언트가 1보다 크면 기하급수적으로 커져요. NaN이 뜨거나 손실이 무한대로 가죠. RNN에서 특히 흔해요.

---

**[섹션 3: Xavier와 He 초기화 - 4:30]**

**Host A**: Xavier 초기화는 어떻게 작동해요?

**Host B**: 입력 뉴런 수와 출력 뉴런 수를 고려해서 분산을 설정해요. 2 나누기 (입력 + 출력)의 분산을 가진 정규분포에서 샘플링하죠.

**Host A**: 왜 이렇게 하는 거예요?

**Host B**: 각 레이어를 지나도 활성화 값의 분산이 유지되도록요! 분산이 일정하면 그래디언트도 안정적으로 흐를 수 있어요.

**Host A**: He 초기화는 뭐가 다른 거예요?

**Host B**: Xavier는 Sigmoid나 Tanh용이에요. ReLU는 음수를 다 0으로 만들어서 절반이 사라지잖아요. 그래서 He 초기화는 분산을 2 나누기 입력 뉴런 수로, 2배 더 크게 잡아요.

**Host A**: CNN에서는 뭘 써요?

**Host B**: ReLU 쓰면 He 초기화가 기본이에요. PyTorch의 kaiming_normal이 바로 He 초기화예요.

---

**[섹션 4: Batch Normalization - 6:00]**

**Host A**: BatchNorm은 왜 거의 모든 CNN에 들어가 있어요?

**Host B**: Internal Covariate Shift 문제를 해결해요. 학습 중에 각 레이어의 입력 분포가 계속 바뀌는데, 이러면 학습이 불안정해져요.

**Host A**: BatchNorm이 어떻게 해결해요?

**Host B**: 미니배치의 평균과 분산으로 정규화해요. 평균 빼고 표준편차로 나누는 거죠. 그다음 학습 가능한 감마와 베타로 스케일과 시프트를 조정해요.

**Host A**: 효과가 뭐예요?

**Host B**: 학습률을 더 크게 쓸 수 있어요. 초기화에 덜 민감해지고요. 정규화 효과도 있어서 드롭아웃이 필요 없는 경우도 있어요. 수렴도 훨씬 빨라지고요!

**Host A**: 단점은요?

**Host B**: 배치 사이즈에 의존해요. 배치가 작으면 통계가 불안정해져요. 그리고 추론 시에는 훈련 중 계산한 이동평균을 써야 해서 관리가 필요해요.

---

**[섹션 5: 다양한 정규화 기법들 - 7:30]**

**Host A**: BatchNorm 외에 다른 정규화 기법도 있죠?

**Host B**: Layer Normalization이 있어요! 배치가 아니라 특징 차원에서 정규화해요. 배치 사이즈와 무관해서 RNN이나 Transformer에 적합해요.

**Host A**: BERT나 GPT가 LayerNorm을 쓰죠?

**Host B**: 네! Transformer의 표준이에요. 시퀀스 길이가 다양해도 괜찮고, 배치 사이즈 1에서도 작동해요.

**Host A**: Instance Normalization은요?

**Host B**: 각 샘플의 각 채널을 독립적으로 정규화해요. 스타일 트랜스퍼에서 많이 써요. 인스턴스별 대비 정보를 제거하거든요.

**Host A**: Group Normalization도 있다던데요?

**Host B**: 채널을 그룹으로 나눠서 그룹 내에서 정규화해요. 배치 사이즈가 작을 때 BatchNorm 대안으로 좋아요. 객체 검출 같은 태스크에서 유용해요.

---

**[섹션 6: 드롭아웃의 원리 - 9:00]**

**Host A**: 드롭아웃이 왜 효과적이에요?

**Host B**: 학습 중에 랜덤하게 뉴런을 끄는 거예요! 보통 20-50% 정도요. 네트워크가 특정 뉴런에 의존하지 못하게 해요.

**Host A**: 어떤 효과가 있어요?

**Host B**: 첫째, 뉴런들의 공동 적응을 방지해요. 특정 뉴런 조합에만 의존하면 일반화가 안 되거든요. 둘째, 앙상블 효과가 있어요. 매번 다른 서브네트워크로 학습하는 셈이니까요.

**Host A**: 추론할 때는요?

**Host B**: 모든 뉴런을 쓰고, 출력에 (1-드롭아웃률)을 곱해요. 또는 학습 때 살아남은 뉴런 출력을 스케일업하는 방법도 있어요.

**Host A**: 언제 쓰면 좋아요?

**Host B**: 주로 FC 레이어 뒤에요. CNN의 특징 추출부에는 잘 안 쓰고, 분류기 부분에 많이 써요. BatchNorm이랑 같이 쓰면 효과가 중복될 수 있어서 주의가 필요해요.

---

**[섹션 7: 드롭아웃 변형들 - 10:30]**

**Host A**: 드롭아웃의 변형도 있죠?

**Host B**: DropBlock이 있어요! 랜덤 뉴런 대신 연속된 영역을 끄는 거예요. CNN에서 공간적 상관관계 때문에 일반 드롭아웃이 덜 효과적인데, 이걸 해결해요.

**Host A**: Spatial Dropout은요?

**Host B**: 채널 전체를 끄는 거예요. 특정 피처맵을 통째로 제거해서 공간 구조를 보존하면서 정규화해요.

**Host A**: DropPath도 있다던데요?

**Host B**: Stochastic Depth라고도 해요. ResNet에서 전체 레이어나 블록을 스킵하는 거예요. 1000층 이상의 초깊은 네트워크 학습을 가능하게 했죠!

---

**[섹션 8: 데이터 증강 - 12:00]**

**Host A**: 데이터 증강도 중요한 정규화 기법이죠?

**Host B**: 정말 중요해요! 학습 데이터를 변형해서 양을 늘리는 거예요. 회전, 반전, 크기 조절, 자르기 같은 기하학적 변환이 기본이에요.

**Host A**: 색상 변환도 있죠?

**Host B**: 밝기, 대비, 채도, 색조 변경이요. 모델이 이런 변화에 불변하게 만들어줘요.

**Host A**: 최근에 나온 고급 기법은요?

**Host B**: Cutout은 이미지 일부를 검은색으로 가려요. Random Erasing은 랜덤 영역을 지우고요. 모델이 부분 정보만으로도 판단할 수 있게 해요.

**Host A**: Mixup이랑 CutMix는요?

**Host B**: Mixup은 두 이미지를 블렌딩하고 레이블도 섞어요. CutMix는 한 이미지의 일부를 다른 이미지로 교체해요. 둘 다 결정 경계를 부드럽게 만들어서 일반화를 개선해요.

---

**[섹션 9: Early Stopping과 앙상블 - 13:30]**

**Host A**: Early Stopping은 간단하지만 효과적이죠?

**Host B**: 네! 검증 성능이 개선되지 않으면 학습을 멈추는 거예요. Patience를 설정해서 몇 에폭 동안 개선이 없으면 종료해요. 과적합을 자연스럽게 방지하죠.

**Host A**: 앙상블도 일반화에 도움이 되죠?

**Host B**: 여러 모델의 예측을 결합하면 분산이 줄어요! 평균, 투표, 스태킹 같은 방법이 있어요. 보통 1-3%의 정확도 향상을 얻을 수 있어요.

**Host A**: 다양성은 어떻게 확보해요?

**Host B**: 다른 랜덤 시드, 다른 아키텍처, 다른 하이퍼파라미터, 다른 데이터 서브셋으로 학습하면 돼요.

---

**[아웃트로 - 14:30]**

**Host A**: 오늘 배운 내용을 정리해볼까요?

**Host B**: 첫째, 초기화는 매우 중요해요! ReLU에는 He 초기화, Sigmoid/Tanh에는 Xavier를 쓰세요.

**Host A**: 둘째, BatchNorm은 CNN의 기본이고, LayerNorm은 Transformer의 필수예요. 상황에 맞는 정규화를 선택하세요.

**Host B**: 셋째, 드롭아웃과 데이터 증강은 과적합 방지의 핵심이에요. Mixup과 CutMix도 시도해보세요!

**Host A**: 마지막으로, Early Stopping과 앙상블로 추가적인 성능 향상을 노려보세요!

**Host B**: 다음 시간에는 딥 네트워크의 필요성과 현대 아키텍처 패턴에 대해 다룰 거예요!

**Host A**: 감사합니다!

---

## 핵심 키워드
- Weight Initialization, Zero Initialization, Symmetry Breaking
- Vanishing/Exploding Gradients
- Xavier/Glorot Initialization, He/Kaiming Initialization
- LSUV, Pre-trained Weights
- Batch Normalization, Layer Normalization
- Instance Normalization, Group Normalization
- Weight Normalization, Spectral Normalization
- Dropout, DropConnect, DropBlock, Spatial Dropout
- Stochastic Depth, DropPath
- Data Augmentation, Cutout, Random Erasing
- Mixup, CutMix
- Early Stopping, Ensemble Methods
