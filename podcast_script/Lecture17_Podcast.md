# Lecture 17 Podcast: 비지도 학습 - 클러스터링과 차원 축소

## 에피소드 정보
- **주제**: 비지도 학습 개념, 클러스터링 알고리즘, 차원 축소, 이상 탐지
- **예상 시간**: 15분
- **대상**: 머신러닝을 공부하는 학생, 레이블 없는 데이터를 다루려는 분들

---

## 스크립트

**[인트로 - 0:00]**

**Host A**: 안녕하세요! AI 비전 시스템 팟캐스트입니다. 오늘은 레이블 없이 데이터에서 패턴을 찾는 비지도 학습을 다룹니다!

**Host B**: 네! 지금까지 분류, 회귀 같은 지도 학습을 많이 봤는데, 현실에서는 레이블이 없는 데이터가 훨씬 많아요.

**Host A**: 레이블 없이 뭘 할 수 있어요?

**Host B**: 생각보다 많아요! 비슷한 데이터를 그룹으로 묶거나, 고차원 데이터를 시각화하거나, 이상한 데이터를 찾아낼 수 있어요. 오늘 이 모든 걸 다뤄볼게요!

---

**[섹션 1: 비지도 학습이란? - 1:30]**

**Host A**: 비지도 학습을 정의해주세요.

**Host B**: 입력 데이터 X만 있고, 정답 레이블 y가 없을 때 데이터의 숨겨진 구조를 찾는 거예요. 지도 학습이 "이건 고양이야"라고 알려주면서 학습시킨다면, 비지도 학습은 "비슷한 것끼리 알아서 모아봐"예요.

**Host A**: 구체적으로 어떤 종류가 있어요?

**Host B**: 크게 네 가지요! 클러스터링은 비슷한 데이터 그룹화, 차원 축소는 특징 압축, 이상 탐지(Anomaly Detection)는 이상치 발견, 연관 규칙(Association)은 관계 발견이에요.

**Host A**: 언제 쓰이나요?

**Host B**: 고객 세분화에서 구매 패턴으로 고객 그룹을 나누거나, 사기 탐지에서 비정상 거래를 찾거나, 유전자 데이터 분석에서 유사한 유전자 패턴을 찾을 때요. 레이블이 없거나 레이블링 비용이 클 때 유용해요!

---

**[섹션 2: K-Means 클러스터링 - 3:30]**

**Host A**: 가장 기본적인 클러스터링 알고리즘부터 알려주세요.

**Host B**: K-Means예요! 아이디어가 정말 직관적이에요. K개의 중심점(centroid)을 정하고, 각 데이터를 가장 가까운 중심점에 할당해요.

**Host A**: 알고리즘을 단계별로 설명해주세요.

**Host B**: 네 단계예요! 첫째, K개의 중심점을 랜덤하게 초기화해요. 둘째, 각 점을 가장 가까운 중심점에 할당해요. 셋째, 각 클러스터의 평균으로 중심점을 업데이트해요. 넷째, 중심점이 안 움직일 때까지 반복해요!

**Host A**: 항상 잘 작동해요?

**Host B**: 초기화에 민감해요! 운 나쁘게 시작하면 안 좋은 결과가 나와요. 그래서 K-Means++가 나왔는데, 중심점을 서로 멀리 떨어지게 초기화해요. 처음 중심점은 랜덤, 다음 중심점은 기존 중심점에서 먼 점을 확률적으로 선택해요.

**Host A**: K는 어떻게 정해요?

**Host B**: Elbow Method를 많이 써요! K를 1부터 늘려가면서 클러스터 내 분산(inertia)을 그래프로 그려요. 감소 속도가 급격히 줄어드는 "팔꿈치" 지점이 적절한 K예요.

---

**[섹션 3: 다른 클러스터링 알고리즘 - 5:30]**

**Host A**: K-Means의 한계가 있나요?

**Host B**: 네! 원형(spherical) 클러스터만 잘 찾아요. 길쭉하거나 복잡한 모양은 못 찾죠. 그리고 K를 미리 정해야 해요. 이상치에도 민감하고요.

**Host A**: 대안은요?

**Host B**: DBSCAN이 있어요! Density-Based Spatial Clustering. 밀도가 높은 영역을 클러스터로 보고, 밀도가 낮은 곳에 있는 점은 이상치(noise)로 처리해요.

**Host A**: 어떻게 작동해요?

**Host B**: 두 파라미터가 있어요. epsilon은 이웃 반경, minPts는 핵심점이 되려면 필요한 최소 이웃 수. 핵심점들과 그 이웃을 연결해서 클러스터를 형성해요. K를 정할 필요 없고, 복잡한 모양도 찾아요!

**Host A**: 계층적 클러스터링도 있죠?

**Host B**: 네! Hierarchical Clustering. Agglomerative는 각 점에서 시작해서 가까운 것들을 합쳐가요. Dendrogram이라는 트리 구조로 시각화할 수 있어서, 어느 레벨에서 자르느냐에 따라 다른 클러스터 수를 얻을 수 있어요.

---

**[섹션 4: 차원의 저주와 PCA - 7:30]**

**Host A**: 이제 차원 축소로 넘어가볼까요?

**Host B**: 먼저 왜 필요한지부터요! 차원의 저주(Curse of Dimensionality)라고, 차원이 높아지면 문제가 생겨요. 데이터가 희박해지고, 거리가 의미 없어지고, 계산이 느려져요.

**Host A**: 거리가 왜 의미 없어져요?

**Host B**: 고차원에서는 모든 점 사이의 거리가 비슷해져요! 직관과 다르죠. 1000차원에서 가장 가까운 점과 가장 먼 점의 거리 차이가 거의 없어요. 그래서 K-NN 같은 알고리즘이 잘 안 작동해요.

**Host A**: PCA가 해결책인가요?

**Host B**: 가장 기본적인 선형 차원 축소예요! Principal Component Analysis. 데이터의 분산이 가장 큰 방향을 찾아서 그 축으로 투영해요.

**Host A**: 어떻게 계산해요?

**Host B**: 공분산 행렬의 고유벡터(eigenvector)를 구해요! 가장 큰 고유값에 해당하는 고유벡터가 첫 번째 주성분이에요. 상위 k개 주성분으로 데이터를 투영하면 k차원으로 축소돼요. 보통 95% 분산을 보존하는 k를 선택해요.

---

**[섹션 5: t-SNE와 UMAP - 9:30]**

**Host A**: PCA의 한계는요?

**Host B**: 선형 관계만 잡아요! 데이터가 곡선 형태로 분포하면 PCA가 잘 안 돼요. 그래서 비선형 방법이 필요해요.

**Host A**: t-SNE가 유명하죠?

**Host B**: 네! t-Distributed Stochastic Neighbor Embedding. 고차원에서 가까운 점들이 저차원에서도 가깝게 유지되도록 해요. 시각화에 정말 좋아요!

**Host A**: 어떻게 작동해요?

**Host B**: 고차원에서 점 쌍의 유사도를 가우시안으로 계산하고, 저차원에서는 t-분포로 계산해요. 두 분포의 KL Divergence를 최소화하도록 저차원 좌표를 최적화해요.

**Host A**: 단점은요?

**Host B**: 느려요! O(n^2) 복잡도라서 데이터가 크면 힘들어요. 그리고 매번 결과가 달라요. 전역 구조보다 지역 구조를 보존하는 경향이 있어서, 멀리 떨어진 클러스터 사이 거리는 의미가 없어요.

**Host A**: UMAP은 어때요?

**Host B**: t-SNE의 대안이에요! 훨씬 빠르고(O(n)), 전역 구조도 더 잘 보존해요. 새로운 점도 임베딩할 수 있어요. 최근에는 t-SNE보다 UMAP을 많이 써요!

---

**[섹션 6: 오토인코더와 차원 축소 - 11:00]**

**Host A**: 딥러닝으로 차원 축소할 수도 있죠?

**Host B**: 오토인코더(Autoencoder)요! 입력을 압축했다가 다시 복원하도록 학습해요. 입력 → 인코더 → 잠재 벡터(bottleneck) → 디코더 → 출력. 출력이 입력과 같아지도록 학습해요.

**Host A**: PCA와 뭐가 달라요?

**Host B**: PCA는 선형 변환인데, 오토인코더는 비선형 활성화 함수가 있어서 복잡한 관계를 잡을 수 있어요! 그리고 원하는 대로 아키텍처를 설계할 수 있죠.

**Host A**: VAE도 있잖아요?

**Host B**: Variational Autoencoder요! 잠재 공간을 확률 분포로 모델링해요. 보통 오토인코더의 잠재 벡터가 점인데, VAE는 평균과 분산을 출력해서 그 분포에서 샘플링해요. 덕분에 새로운 샘플을 생성할 수도 있어요!

---

**[섹션 7: 클러스터 평가 - 12:30]**

**Host A**: 클러스터링 결과가 좋은지 어떻게 알아요?

**Host B**: 어려운 문제예요! 레이블이 없으니까요. 내부 평가 지표(Internal Metrics)를 써요. Silhouette Score가 대표적이에요.

**Host A**: Silhouette Score가 뭐예요?

**Host B**: 각 점에 대해 같은 클러스터 내 평균 거리 a와 가장 가까운 다른 클러스터까지 평균 거리 b를 계산해요. (b-a)/max(a,b)가 그 점의 실루엣 값이에요. -1에서 1 사이고, 1에 가까울수록 좋아요!

**Host A**: 다른 지표도 있나요?

**Host B**: Davies-Bouldin Index는 낮을수록 좋아요. 만약 실제 레이블이 있다면 외부 지표도 쓸 수 있어요. NMI(Normalized Mutual Information), ARI(Adjusted Rand Index) 같은 거요.

---

**[섹션 8: 이상 탐지 - 13:30]**

**Host A**: 이상 탐지도 비지도 학습이죠?

**Host B**: 네! 대부분의 데이터와 다른 이상치(outlier)를 찾는 거예요. 사기 탐지, 네트워크 침입 탐지, 제조 결함 탐지에 쓰여요.

**Host A**: 어떤 방법들이 있어요?

**Host B**: 통계적 방법으로는 Z-score가 있어요. 평균에서 표준편차의 3배 이상 떨어지면 이상치로 보죠. IQR 방법은 1사분위수 - 1.5*IQR 아래이거나 3사분위수 + 1.5*IQR 위면 이상치예요.

**Host A**: 머신러닝 방법은요?

**Host B**: Isolation Forest가 인기 있어요! 아이디어가 재밌어요. 이상치는 고립시키기 쉽다는 거예요. 랜덤하게 특징을 선택하고 분할하는 트리를 만들면, 이상치는 빨리 고립되서 트리에서 경로가 짧아요.

**Host A**: 다른 방법도요?

**Host B**: One-Class SVM은 정상 데이터 주변에 경계를 만들어요. Local Outlier Factor(LOF)는 지역 밀도를 비교해요. 오토인코더는 정상 데이터 재구성을 학습하고, 재구성 에러가 크면 이상치로 봐요!

---

**[아웃트로 - 14:30]**

**Host A**: 오늘 내용을 정리해볼까요?

**Host B**: 첫째, 비지도 학습은 레이블 없이 데이터 구조를 찾아요. 클러스터링, 차원 축소, 이상 탐지가 대표적이에요!

**Host A**: 둘째, K-Means는 간단하지만 원형 클러스터에 적합하고, DBSCAN은 복잡한 모양과 이상치에 강해요.

**Host B**: 셋째, PCA는 선형 차원 축소의 기본이고, t-SNE와 UMAP은 시각화에 좋은 비선형 방법이에요.

**Host A**: 넷째, Silhouette Score로 클러스터 품질을 평가하고, Isolation Forest 같은 방법으로 이상치를 탐지해요!

**Host B**: 비지도 학습은 데이터 탐색과 전처리에 정말 유용해요. 다음 시간에도 흥미로운 주제로 돌아올게요!

**Host A**: 감사합니다!

---

## 핵심 키워드
- Unsupervised Learning, Supervised Learning
- Clustering: K-Means, K-Means++, DBSCAN, Hierarchical
- Curse of Dimensionality
- Dimensionality Reduction: PCA, Kernel PCA
- Visualization: t-SNE, UMAP
- Autoencoder, VAE (Variational Autoencoder)
- Cluster Evaluation: Silhouette Score, Elbow Method
- Anomaly Detection: Z-score, IQR, Isolation Forest, LOF
- Gaussian Mixture Model (GMM)
