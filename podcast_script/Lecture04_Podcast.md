# Lecture 04 Podcast: 정규화와 로지스틱 회귀 - 회귀에서 분류로

## 에피소드 정보
- **주제**: 다항 회귀, 정규화(Ridge/Lasso), 로지스틱 회귀
- **예상 시간**: 15분
- **대상**: ML 입문자, 분류 알고리즘을 배우려는 분들

---

## 스크립트

**[인트로 - 0:00]**

**Host A**: 안녕하세요! AI 비전 시스템 팟캐스트입니다. 지난 시간에 선형회귀를 배웠는데, 오늘은 여기서 한 걸음 더 나아가볼게요.

**Host B**: 네! 오늘은 세 가지 핵심을 다룰 거예요. 첫째, 비선형 패턴을 잡는 다항 회귀. 둘째, 과적합을 막는 정규화. 셋째, 분류 문제를 위한 로지스틱 회귀!

**Host A**: 선형회귀가 직선만 그릴 수 있다면, 곡선은 어떻게 그려요?

**Host B**: 좋은 질문이에요! 바로 다항 회귀로 넘어가죠.

---

**[섹션 1: 다항 회귀와 기저 확장 - 1:30]**

**Host B**: 다항 회귀의 핵심 아이디어는 x를 x, x², x³ 등으로 확장하는 거예요. y = β₀ + β₁x + β₂x² + β₃x³ + ...

**Host A**: 그러면 파라미터에 대해서는 여전히 선형이네요?

**Host B**: 바로 그거예요! 입력 x에 대해서는 비선형이지만, β들에 대해서는 선형이라 같은 최적화 방법을 쓸 수 있어요. 이걸 기저 확장(Basis Expansion)이라고 해요.

**Host A**: sklearn에서는 어떻게 해요?

**Host B**: PolynomialFeatures로 피처를 변환하고 일반 LinearRegression을 적용하면 돼요. 간단하죠! 근데 차수를 너무 높이면 문제가 생겨요.

**Host A**: 과적합(Overfitting)이요?

**Host B**: 맞아요! 훈련 데이터에는 완벽히 맞지만, 새 데이터에서는 엉망이 되죠. 곡선이 너무 구불구불해져서요.

---

**[섹션 2: Ridge 회귀 (L2 정규화) - 3:30]**

**Host A**: 과적합을 어떻게 막아요?

**Host B**: 정규화(Regularization)예요! Ridge 회귀는 손실 함수에 L2 패널티를 추가해요. L = Σ(y - ŷ)² + λΣβ²

**Host A**: λ가 뭐예요?

**Host B**: 정규화 강도예요. λ가 커지면 계수들이 작아지도록 강하게 제약하는 거죠. 계수가 크면 모델이 입력의 작은 변화에도 출력이 크게 변하거든요.

**Host A**: 수학적으로 어떻게 풀어요?

**Host B**: 정규방정식이 약간 바뀌어요. β = (XᵀX + λI)⁻¹Xᵀy. λI를 더하면 항상 역행렬이 존재해서 다중공선성 문제도 해결돼요!

**Host A**: 모든 계수가 작아지나요?

**Host B**: 네, 0에 가까워지지만 정확히 0이 되진 않아요. 모든 피처를 유지하면서 크기만 줄이는 거죠.

---

**[섹션 3: Lasso 회귀 (L1 정규화) - 5:30]**

**Host A**: Lasso는 뭐가 다른가요?

**Host B**: Lasso는 L1 패널티를 써요. L = Σ(y - ŷ)² + λΣ|β|. 절댓값이죠!

**Host A**: 제곱이랑 절댓값이 뭐가 다른데요?

**Host B**: 핵심 차이예요! L1은 계수를 정확히 0으로 만들 수 있어요. 즉, 자동으로 피처 선택(Feature Selection)을 해줘요!

**Host A**: 왜 그렇게 되는 거예요?

**Host B**: 기하학적으로 설명하면, L1의 제약 영역이 마름모 모양이라 꼭짓점에서 최적해가 나올 확률이 높아요. 꼭짓점은 일부 계수가 0인 지점이죠.

**Host A**: 그럼 언제 Ridge, 언제 Lasso를 써요?

**Host B**: 피처가 다 중요할 것 같으면 Ridge, 일부만 중요할 것 같으면 Lasso예요. 해석 가능성이 중요하면 Lasso가 좋고요.

---

**[섹션 4: Elastic Net과 피처 선택 - 7:30]**

**Host A**: 둘을 합칠 수는 없나요?

**Host B**: Elastic Net이 바로 그거예요! L = Σ(y - ŷ)² + λ₁Σ|β| + λ₂Σβ². L1과 L2를 동시에 써요.

**Host A**: 언제 유용해요?

**Host B**: 상관된 피처가 많을 때요. Lasso는 상관된 피처 중 하나만 선택하는 경향이 있는데, Elastic Net은 그룹으로 함께 선택해요. L2의 안정성 덕분이죠.

**Host A**: 피처 중요도는 어떻게 알 수 있어요?

**Host B**: 표준화 후 계수 크기 |β|를 보면 돼요. Lasso에서 0이 아닌 계수가 중요한 피처고요. RFE(Recursive Feature Elimination)같은 방법도 있어요.

---

**[섹션 5: 회귀에서 분류로 - 9:00]**

**Host A**: 이제 분류로 넘어가볼까요? 왜 선형회귀로 분류를 못 해요?

**Host B**: 출력이 제한이 없어서요! 선형회귀 출력은 -∞에서 +∞까지 가능한데, 분류에서는 0과 1 사이 확률이 필요해요.

**Host A**: 그냥 0.5 기준으로 자르면 안 되나요?

**Host B**: 문제가 있어요. 아웃라이어가 있으면 결정 경계가 크게 밀려요. 그리고 결과를 확률로 해석할 수가 없죠.

**Host A**: 퍼셉트론(Perceptron)은요?

**Host B**: 역사적으로 중요한 알고리즘이에요! sign(wᵀx + b)로 분류하고, 오분류된 샘플마다 가중치를 업데이트해요. 근데 선형 분리 가능한 데이터에서만 수렴이 보장돼요.

---

**[섹션 6: 시그모이드와 로지스틱 회귀 - 10:30]**

**Host A**: 그래서 시그모이드 함수가 등장하는 거군요!

**Host B**: 맞아요! σ(z) = 1 / (1 + e⁻ᶻ). 어떤 실수든 0과 1 사이로 매핑해요. z가 크면 1에 가깝고, z가 작으면 0에 가깝죠.

**Host A**: 로지스틱 회귀 모델은요?

**Host B**: P(Y=1|X) = σ(wᵀx + b)예요. 선형 조합을 시그모이드에 통과시켜서 확률을 얻는 거죠. 결정 경계는 wᵀx + b = 0, 여기서 확률이 정확히 0.5예요.

**Host A**: 왜 "로지스틱"이에요?

**Host B**: 오즈(Odds) 개념에서 와요. p/(1-p)가 오즈인데, log(p/(1-p))를 로짓(logit)이라고 해요. 로지스틱 회귀는 로짓이 선형이라고 가정하는 거예요!

---

**[섹션 7: 손실 함수와 최적화 - 12:00]**

**Host A**: 로지스틱 회귀는 어떻게 학습해요?

**Host B**: 최대 우도 추정(MLE)을 써요. 관측된 데이터의 확률을 최대화하는 파라미터를 찾죠. 이걸 뒤집으면 Binary Cross-Entropy 손실이 돼요.

**Host A**: 수식으로는요?

**Host B**: L = -1/n Σ[y log(p) + (1-y) log(1-p)]. y=1일 때는 p가 커야 손실이 작고, y=0일 때는 p가 작아야 손실이 작아요.

**Host A**: 정규방정식처럼 한 번에 풀 수 있어요?

**Host B**: 아니요, 닫힌 형태 해가 없어서 Gradient Descent를 써야 해요. 다행히 볼록(convex) 함수라 전역 최솟값을 찾을 수 있어요.

---

**[섹션 8: 다중 분류와 정규화 - 13:30]**

**Host A**: 클래스가 3개 이상이면요?

**Host B**: 두 가지 방법이 있어요. One-vs-Rest는 K개의 이진 분류기를 만드는 거고, Softmax 회귀는 모든 클래스를 동시에 모델링해요.

**Host A**: Softmax가 뭐예요?

**Host B**: P(Y=k|X) = exp(wₖᵀx) / Σⱼ exp(wⱼᵀx). 모든 클래스 확률의 합이 1이 되도록 정규화하는 함수예요. 뉴럴넷 출력층에서 많이 쓰이죠.

**Host A**: 로지스틱 회귀에도 정규화를 적용해요?

**Host B**: 당연히요! L2 정규화가 기본이에요. sklearn의 LogisticRegression은 기본적으로 L2가 켜져 있어요. C 파라미터가 1/λ라서, C가 작을수록 정규화가 강해요.

**Host A**: 실제 적용할 때 팁은요?

**Host B**: 피처 스케일링은 필수! 클래스 불균형이 있으면 class_weight='balanced' 옵션을 쓰세요. 그리고 threshold가 항상 0.5일 필요 없어요, 상황에 맞게 조절하세요.

---

**[아웃트로 - 14:30]**

**Host A**: 오늘 정리해볼까요?

**Host B**: 첫째, 다항 회귀로 비선형 패턴을 잡을 수 있지만, 과적합 주의!

**Host A**: 둘째, Ridge는 계수를 작게, Lasso는 0으로 만들어요. Elastic Net은 둘의 조합!

**Host B**: 셋째, 분류에서는 시그모이드를 써서 확률을 모델링해요. 이게 로지스틱 회귀!

**Host A**: 넷째, Cross-Entropy 손실과 Gradient Descent로 학습하고, Softmax로 다중 분류도 가능해요!

**Host B**: 로지스틱 회귀는 딥러닝의 기초이기도 해요. 다음 시간에 더 깊이 들어가볼게요!

**Host A**: 감사합니다!

---

## 핵심 키워드
- Polynomial Regression, Basis Expansion
- Ridge (L2), Lasso (L1), Elastic Net
- Feature Selection, Regularization Strength
- Perceptron, Linear Separability
- Sigmoid Function, Logit, Odds
- Logistic Regression, Binary Cross-Entropy
- Maximum Likelihood Estimation, Gradient Descent
- Softmax, Categorical Cross-Entropy
