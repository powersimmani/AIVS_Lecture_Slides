# Lecture 05 Podcast: 신경망의 탄생 - 퍼셉트론에서 MLP까지

## 에피소드 정보
- **주제**: 신경망 동기, MLP 구조, 순전파, 역전파
- **예상 시간**: 15분
- **대상**: 딥러닝 입문자, 신경망 원리를 이해하려는 분들

---

## 스크립트

**[인트로 - 0:00]**

**Host A**: 안녕하세요! AI 비전 시스템 팟캐스트입니다. 드디어 딥러닝의 핵심, 신경망 이야기를 할 시간이에요!

**Host B**: 네! 오늘은 왜 신경망이 필요한지부터 시작해서, 어떻게 구조화되고, 어떻게 학습하는지까지 다룰 거예요.

**Host A**: 로지스틱 회귀도 좋은데, 왜 신경망이 필요해요?

**Host B**: 바로 그 질문이 시작점이에요! 유명한 XOR 문제부터 보죠.

---

**[섹션 1: XOR 문제와 선형 분리 불가능 - 1:30]**

**Host B**: XOR 진리표를 보면요, (0,0)→0, (0,1)→1, (1,0)→1, (1,1)→0이에요. 그래프에 점을 찍으면 대각선 끼리 같은 클래스죠.

**Host A**: 직선 하나로는 절대 분리할 수 없네요!

**Host B**: 정확해요! 1969년에 Minsky와 Papert가 퍼셉트론은 XOR을 풀 수 없다고 증명했어요. 이게 AI 겨울의 원인 중 하나였죠.

**Host A**: 그래서 어떻게 해결해요?

**Host B**: 핵심은 피처 공간 변환이에요! 원래 공간에서 선형 분리 불가능해도, 적절히 변환하면 가능해져요. 신경망이 이 변환을 자동으로 학습하는 거예요.

**Host A**: 수동으로 피처 엔지니어링하는 게 아니라?

**Host B**: 네! 그게 표현 학습(Representation Learning)의 핵심이에요. 네트워크가 좋은 표현을 스스로 찾아요.

---

**[섹션 2: 다층 퍼셉트론 구조 - 3:30]**

**Host A**: 그래서 다층 퍼셉트론, MLP가 등장하는 거군요!

**Host B**: 맞아요. 구조는 입력층 → 은닉층(들) → 출력층이에요. 각 층에서 선형 변환과 비선형 활성화를 거쳐요.

**Host A**: 수식으로는요?

**Host B**: h⁽ˡ⁾ = σ(W⁽ˡ⁾h⁽ˡ⁻¹⁾ + b⁽ˡ⁾). 이전 층의 출력에 가중치 행렬을 곱하고, 편향을 더하고, 활성화 함수를 적용해요.

**Host A**: 예를 들어 MNIST 분류라면요?

**Host B**: 입력 784개(28x28 이미지), 은닉층 256→128→64, 출력 10개(0-9 숫자)로 구성할 수 있어요. 파라미터 수는 각 층 연결의 가중치와 편향을 다 합한 거예요.

**Host A**: 파라미터가 꽤 많겠네요.

**Host B**: 네! 784×256 + 256 + 256×128 + 128 + ... 식으로요. 그래서 초기화가 중요해요. Xavier 초기화나 He 초기화를 써요.

---

**[섹션 3: 활성화 함수의 중요성 - 5:30]**

**Host A**: 활성화 함수가 왜 필요해요?

**Host B**: 핵심 질문이에요! 활성화 함수 없이 여러 층을 쌓으면, 결국 하나의 선형 변환이 돼요. W₂(W₁x + b₁) + b₂ = W₂W₁x + 상수, 이건 그냥 선형 모델이에요!

**Host A**: 비선형성이 있어야 복잡한 함수를 표현할 수 있군요.

**Host B**: 맞아요! 시그모이드는 (0,1) 범위로 출력이 확률처럼 해석돼요. tanh는 (-1,1)로 zero-centered라 시그모이드보다 학습이 잘 돼요.

**Host A**: 근데 요즘은 ReLU를 많이 쓴다면서요?

**Host B**: 네! ReLU(z) = max(0, z). 단순하고, z>0일 때 그래디언트가 1이라 vanishing gradient 문제가 없어요. 하지만 z<0이면 그래디언트가 0이라 "죽는 ReLU" 문제가 있어요.

**Host A**: 그래서 Leaky ReLU나 ELU가 나온 거군요.

**Host B**: 맞아요! Leaky ReLU는 z<0일 때 작은 기울기(0.01 정도)를 줘요. 기본적으로는 ReLU를 쓰고, 문제가 생기면 변형을 시도해보세요.

---

**[섹션 4: 출력층과 손실 함수 - 7:30]**

**Host A**: 출력층은 문제에 따라 다르죠?

**Host B**: 네! 회귀면 활성화 없이(Linear), MSE 손실. 이진 분류면 시그모이드에 Binary Cross-Entropy. 다중 분류면 Softmax에 Categorical Cross-Entropy예요.

**Host A**: 꼭 맞춰야 하나요?

**Host B**: 수학적으로 연결되어 있어서요. Softmax + Cross-Entropy 조합이 그래디언트가 깔끔하게 나와요. 잘못 조합하면 학습이 안 되거나 불안정해져요.

**Host A**: Universal Approximation Theorem도 들어봤어요.

**Host B**: 은닉층 하나만 있어도 충분히 뉴런이 많으면 어떤 연속 함수든 근사할 수 있다는 정리예요! 근데 "충분히 많은"이 기하급수적으로 많을 수 있어서, 깊은 네트워크가 더 효율적이에요.

---

**[섹션 5: 순전파 알고리즘 - 9:00]**

**Host A**: Forward Propagation이 어떻게 동작해요?

**Host B**: 단계별로 설명할게요. 먼저 입력 a⁽⁰⁾ = x. 각 층 l에서 z⁽ˡ⁾ = W⁽ˡ⁾a⁽ˡ⁻¹⁾ + b⁽ˡ⁾로 선형 조합하고, a⁽ˡ⁾ = σ(z⁽ˡ⁾)로 활성화해요. 마지막에 ŷ = a⁽ᴸ⁾가 예측이에요.

**Host A**: 간단한 숫자 예시를 들어주실 수 있어요?

**Host B**: 입력 [1.0, 0.5]가 있고, 첫 번째 층 후 z⁽¹⁾ = [0.8, 1.2]가 나왔다면, 시그모이드 적용해서 a⁽¹⁾ = [0.69, 0.77]. 다음 층에서 z⁽²⁾ = 1.1이면, 최종 출력 ŷ = σ(1.1) = 0.75예요.

**Host A**: 손실은 이 예측과 실제 레이블로 계산하는 거죠?

**Host B**: 정확해요! 손실이 작아지도록 가중치를 업데이트하는 게 학습이에요. 그게 역전파(Backpropagation)예요.

---

**[섹션 6: 역전파 알고리즘 - 10:30]**

**Host A**: 역전파가 딥러닝의 핵심이죠!

**Host B**: 네! 핵심 아이디어는 Chain Rule이에요. 합성 함수의 미분은 각 단계 미분의 곱이죠. d/dx[f(g(x))] = f'(g(x)) · g'(x).

**Host A**: 신경망에 어떻게 적용돼요?

**Host B**: 출력층에서 에러 δ⁽ᴸ⁾ = ∂L/∂z⁽ᴸ⁾를 계산하고, 이걸 뒤로 전파해요. δ⁽ˡ⁾ = (W⁽ˡ⁺¹⁾)ᵀδ⁽ˡ⁺¹⁾ ⊙ σ'(z⁽ˡ⁾). 각 층의 그래디언트는 ∂L/∂W⁽ˡ⁾ = δ⁽ˡ⁾(a⁽ˡ⁻¹⁾)ᵀ예요.

**Host A**: 직접 계산하면 복잡하겠네요.

**Host B**: 그래서 PyTorch나 TensorFlow가 있죠! Autograd가 계산 그래프를 만들어서 자동으로 그래디언트를 계산해줘요. loss.backward() 한 줄이면 끝!

---

**[섹션 7: 미니배치 경사하강법 - 12:00]**

**Host A**: 실제로는 어떻게 학습해요?

**Host B**: 세 가지 방법이 있어요. Batch GD는 전체 데이터를 쓰는데 느려요. SGD는 샘플 하나씩이라 빠르지만 불안정해요. Mini-batch가 절충안이에요!

**Host A**: Mini-batch 크기는 어떻게 정해요?

**Host B**: 보통 32, 64, 128, 256... 2의 거듭제곱을 많이 써요. GPU 메모리에 맞추면서 충분히 큰 게 좋아요. 너무 작으면 노이즈가 심하고, 너무 크면 수렴이 느려요.

**Host A**: 학습률(Learning Rate)도 중요하죠?

**Host B**: 가장 중요한 하이퍼파라미터예요! 너무 크면 발산하고, 너무 작으면 수렴이 안 돼요. 보통 0.001 정도로 시작해서 조절해요. Adam 같은 적응형 옵티마이저가 학습률을 자동 조절해줘요.

---

**[섹션 8: 실습 팁과 디버깅 - 13:30]**

**Host A**: 실제로 학습할 때 팁 좀 주세요!

**Host B**: 첫째, 입력 정규화! 평균 0, 표준편차 1로 맞추면 학습이 훨씬 안정돼요. 둘째, 작은 배치로 과적합시켜보기! 모델이 학습 가능한지 먼저 확인해요.

**Host A**: 문제가 생기면요?

**Host B**: Loss가 안 줄면 학습률을 확인하세요. NaN이 나오면 학습률이 너무 크거나 데이터에 문제가 있어요. Gradient Checking으로 역전파 구현이 맞는지 확인할 수도 있어요.

**Host A**: PyTorch 코드 구조는요?

**Host B**: model = nn.Sequential(...)로 네트워크 정의, criterion으로 손실 함수, optimizer로 옵티마이저 설정. 루프에서 output = model(x), loss = criterion(output, y), optimizer.zero_grad(), loss.backward(), optimizer.step() 순서예요!

---

**[아웃트로 - 14:30]**

**Host A**: 오늘 핵심을 정리해볼까요?

**Host B**: 첫째, XOR 문제가 보여주듯이, 선형 모델의 한계를 극복하려면 다층 구조가 필요해요!

**Host A**: 둘째, 활성화 함수가 비선형성을 제공해요. ReLU가 기본 선택이에요.

**Host B**: 셋째, 순전파로 예측하고, 역전파로 그래디언트를 계산해서 가중치를 업데이트해요.

**Host A**: 넷째, Mini-batch로 효율과 안정성을 균형 잡고, 현대 프레임워크가 자동으로 그래디언트를 계산해줘요!

**Host B**: 다음 시간에는 CNN으로 이미지를 다루는 방법을 배울 거예요. 기대해주세요!

**Host A**: 감사합니다!

---

## 핵심 키워드
- XOR Problem, Linear Separability
- Representation Learning, Feature Transformation
- MLP (Multi-Layer Perceptron)
- Activation Functions: Sigmoid, Tanh, ReLU, Leaky ReLU
- Universal Approximation Theorem
- Forward Propagation, Backward Propagation
- Chain Rule, Computational Graph, Autograd
- Mini-batch Gradient Descent, Learning Rate
