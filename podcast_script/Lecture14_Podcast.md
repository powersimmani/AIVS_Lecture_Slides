# Lecture 14 Podcast: 사전학습 언어 모델 - BERT, GPT, 그리고 그 이후

## 에피소드 정보
- **주제**: 사전학습 패러다임, BERT, GPT, 파인튜닝, 프롬프팅, RLHF
- **예상 시간**: 15분
- **대상**: ML/DL을 공부하는 학생 및 실무자

---

## 스크립트

**[인트로 - 0:00]**

**Host A**: 안녕하세요! AI 비전 시스템 팟캐스트에 오신 것을 환영합니다. 저는 호스트 A이고요.

**Host B**: 안녕하세요, 호스트 B입니다! 오늘은 현대 AI의 핵심인 사전학습 언어 모델에 대해 다뤄볼 거예요. ChatGPT, Claude, Gemini... 이 모든 것의 기반이 되는 개념이죠!

**Host A**: 맞아요. 2018년 BERT와 GPT가 나온 이후 AI 분야가 완전히 바뀌었어요. "사전학습 후 파인튜닝" 패러다임이 어떻게 혁명을 일으켰는지 알아봅시다!

---

**[섹션 1: 패러다임의 변화 - 1:00]**

**Host B**: 예전에는 어떻게 했나요?

**Host A**: 2018년 이전에는 각 태스크마다 처음부터 모델을 학습했어요. 감성 분석 모델, 개체명 인식 모델, 번역 모델... 각각 따로 만들고 태스크별 아키텍처도 달랐죠.

**Host B**: 문제점이 뭐였나요?

**Host A**: 첫째, 레이블된 데이터가 많이 필요해요. 둘째, 각 태스크마다 새로 학습하니까 비효율적이에요. 셋째, 지식이 태스크 간에 전이되지 않아요.

**Host B**: 새 패러다임은 어떤 건가요?

**Host A**: "사전학습 후 파인튜닝"이에요. 먼저 거대한 텍스트 데이터로 일반적인 언어 이해를 학습하고, 그다음 작은 레이블 데이터로 특정 태스크에 적응시키는 거예요.

**Host B**: 비유하자면요?

**Host A**: 사람이 먼저 읽는 법을 배우고, 그 다음에 특정 업무를 배우는 것과 비슷해요. 읽기라는 기초 능력이 있으면 새로운 태스크 학습이 훨씬 쉽죠!

---

**[섹션 2: 사전학습의 핵심 - 2:30]**

**Host B**: 사전학습이 정확히 뭔가요?

**Host A**: 대규모 텍스트 코퍼스에서 일반적인 표현을 학습하는 거예요. 핵심은 자기지도학습이에요. 레이블이 데이터 자체에서 나오니까 수십억 단어를 사용할 수 있어요.

**Host B**: 어떤 목적 함수를 사용하나요?

**Host A**: 크게 두 가지예요. 첫째, 자기회귀 언어 모델링. GPT 스타일이죠. 이전 토큰들이 주어지면 다음 토큰을 예측해요. P(x_t | x_1, ..., x_{t-1})를 최대화하는 거예요.

**Host B**: 둘째는요?

**Host A**: 마스크드 언어 모델링, BERT 스타일이에요. 일부 토큰을 마스크하고 문맥에서 그걸 예측해요. 양방향 문맥을 사용할 수 있어서 이해 태스크에 강해요.

**Host B**: 데이터는 어디서 가져오나요?

**Host A**: 책, 웹 텍스트, 위키피디아, 코드, 논문 등이에요. GPT-3는 3000억 토큰, LLaMA는 1.4조 토큰으로 학습했어요. 규모가 정말 엄청나죠!

---

**[섹션 3: BERT 깊이 보기 - 4:30]**

**Host B**: BERT에 대해 자세히 알아볼까요?

**Host A**: BERT는 Bidirectional Encoder Representations from Transformers의 약자예요. 2018년 구글에서 발표했고, 당시 11개 NLP 벤치마크에서 최고 성능을 달성했어요.

**Host B**: BERT의 핵심 혁신은 뭔가요?

**Host A**: 양방향 문맥이에요. GPT는 왼쪽에서 오른쪽으로만 보는데, BERT는 양쪽을 다 봐요. "The [MASK] sat on the mat"에서 [MASK]를 예측할 때 "sat on the mat"도 참고할 수 있어요.

**Host B**: 마스크드 언어 모델링이 어떻게 동작하나요?

**Host A**: 입력 토큰의 15%를 랜덤하게 선택해요. 그중 80%는 [MASK]로 바꾸고, 10%는 랜덤 토큰으로 바꾸고, 10%는 그대로 둬요. 이 혼합이 학습을 더 강건하게 만들어요.

**Host B**: 다음 문장 예측도 있다고 했죠?

**Host A**: NSP라고 하는데요, 두 문장이 연속인지 예측해요. 근데 나중에 RoBERTa 연구에서 별로 도움 안 된다는 게 밝혀졌어요. 요즘 모델들은 안 써요.

---

**[섹션 4: BERT 파인튜닝 - 6:00]**

**Host B**: BERT를 어떻게 특정 태스크에 사용하나요?

**Host A**: 파인튜닝이에요! 사전학습된 BERT 위에 태스크별 레이어를 추가하고, 전체를 작은 레이블 데이터로 학습해요.

**Host B**: 예를 들어볼까요?

**Host A**: 분류면 [CLS] 토큰 임베딩에 선형 레이어를 붙여요. 개체명 인식이면 각 토큰 임베딩에 선형 레이어를 붙여서 레이블을 예측해요. 질문 응답이면 답변 시작/끝 위치를 예측해요.

**Host B**: BERT 변형들도 있죠?

**Host A**: 네! RoBERTa는 NSP를 제거하고 더 많은 데이터로 더 오래 학습해서 성능을 높였어요. ALBERT는 파라미터 공유로 크기를 줄이고, DistilBERT는 지식 증류로 40% 작으면서 97% 성능을 유지해요.

---

**[섹션 5: GPT 시리즈 - 7:30]**

**Host B**: 이제 GPT 이야기를 해볼까요?

**Host A**: GPT는 Generative Pre-trained Transformer예요. 2018년 OpenAI에서 발표했고, 디코더만 사용하는 자기회귀 모델이에요.

**Host B**: BERT와 뭐가 다른가요?

**Host A**: BERT는 인코더로 양방향 이해에 좋고, GPT는 디코더로 텍스트 생성에 좋아요. BERT는 분류에, GPT는 생성에 적합하죠.

**Host B**: GPT-3가 특별한 이유는 뭔가요?

**Host A**: 규모예요! 1750억 파라미터, 3000억 토큰 학습. 그리고 Few-shot 학습이 등장했어요. 파인튜닝 없이 프롬프트에 몇 개 예시만 주면 태스크를 수행해요.

**Host B**: 예를 들어볼까요?

**Host A**: "영어를 불어로 번역: sea otter → loutre de mer, cheese →"라고 주면 모델이 "fromage"를 생성해요. 그래디언트 업데이트 없이, 프롬프트만으로요!

---

**[섹션 6: 인코더-디코더 모델 - 9:00]**

**Host B**: T5 같은 인코더-디코더 모델도 있죠?

**Host A**: 네! T5는 Text-to-Text Transfer Transformer예요. 모든 태스크를 텍스트-투-텍스트 형식으로 통일했어요.

**Host B**: 어떤 식으로요?

**Host A**: 분류는 "classify: I love this!" → "positive", 번역은 "translate English to French: Hello" → "Bonjour", 요약은 "summarize: [긴 텍스트]" → "[요약]" 이런 식이에요.

**Host B**: 장점이 뭔가요?

**Host A**: 모든 태스크에 같은 아키텍처, 같은 손실 함수를 쓸 수 있어요. 새 태스크 추가가 쉽고요. BART도 비슷한데, 노이즈 제거 사전학습을 사용해서 요약에 특히 강해요.

---

**[섹션 7: 파라미터 효율적 파인튜닝 - 10:30]**

**Host B**: 175B 파라미터 모델을 태스크마다 전체 파인튜닝하면 비용이 엄청나겠네요?

**Host A**: 맞아요! 그래서 PEFT, 파라미터 효율적 파인튜닝 방법들이 나왔어요. 대표적으로 LoRA가 있어요.

**Host B**: LoRA가 뭔가요?

**Host A**: Low-Rank Adaptation이에요. 전체 가중치를 업데이트하는 대신, 가중치 변화를 저랭크 행렬로 분해해요. W' = W + BA에서 B, A만 학습하는 거죠. r=8 정도면 전체 파라미터의 0.1%만 학습해도 돼요!

**Host B**: 다른 방법들은요?

**Host A**: 어댑터는 레이어 사이에 작은 모듈을 끼워넣어요. 프롬프트 튜닝은 입력 앞에 학습 가능한 소프트 프롬프트를 붙여요. Prefix Tuning은 어텐션 레이어에 학습 가능한 벡터를 추가해요.

---

**[섹션 8: 프롬프팅과 인컨텍스트 학습 - 12:00]**

**Host B**: 프롬프트 엔지니어링에 대해 알아볼까요?

**Host A**: 프롬프팅은 모델에 텍스트로 조건을 주는 거예요. 파라미터 업데이트 없이 태스크를 수행할 수 있어요. 제로샷, 원샷, 퓨샷이 있죠.

**Host B**: 고급 프롬프팅 기법도 있나요?

**Host A**: Chain-of-Thought가 대표적이에요! "단계별로 생각해봅시다"를 추가하면 추론 성능이 크게 올라가요. 수학이나 논리 문제에서 특히 효과적이에요.

**Host B**: 예를 들어볼까요?

**Host A**: "15의 80%는?" 바로 물으면 틀릴 수 있는데, "단계별로 생각해보면, 15%는 0.15이고, 0.15 곱하기 80은 12입니다"처럼 유도하면 정확해져요.

**Host B**: 파인튜닝과 프롬프팅 중 뭘 써야 하나요?

**Host A**: 빠른 프로토타이핑이나 레이블 데이터가 없으면 프롬프팅, 최대 성능이 필요하고 데이터가 있으면 파인튜닝이에요. 요즘은 둘을 조합해서 쓰기도 해요.

---

**[섹션 9: RLHF와 최신 트렌드 - 13:30]**

**Host B**: ChatGPT는 어떻게 만들어졌나요?

**Host A**: RLHF, 인간 피드백 강화학습이에요! 세 단계로 이루어져요. 첫째, 지도 파인튜닝으로 시연 데이터를 학습해요. 둘째, 인간 선호도로 리워드 모델을 학습해요. 셋째, PPO로 리워드를 최대화하도록 정책을 학습해요.

**Host B**: 왜 이게 필요한가요?

**Host A**: 사전학습된 모델은 다음 토큰 예측만 잘하지, 지시를 따르거나 도움이 되는 답변을 하는 건 별개예요. RLHF가 모델을 인간 선호에 맞게 정렬시켜요.

**Host B**: Constitutional AI는 뭔가요?

**Host A**: 원칙 기반 학습이에요. "도움이 되고, 해롭지 않고, 정직하라"같은 원칙을 정하고, 모델이 스스로 응답을 비평하고 개선하게 해요. Claude가 이 방식을 사용해요.

---

**[아웃트로 - 14:30]**

**Host A**: 오늘 정말 방대한 내용을 다뤘네요! 정리해볼까요?

**Host B**: 첫째, 사전학습 후 파인튜닝 패러다임이 AI를 혁신했어요. 대규모 비레이블 데이터로 일반 지식을 학습하고 전이해요.

**Host A**: 둘째, BERT는 양방향 이해에, GPT는 생성에 강해요. T5 같은 인코더-디코더는 둘 다 잘해요.

**Host B**: 셋째, LoRA 같은 PEFT 방법으로 효율적 적응이 가능하고, 프롬프팅으로 파인튜닝 없이도 많은 태스크를 수행할 수 있어요.

**Host A**: 넷째, RLHF가 모델을 인간 선호에 맞게 정렬시켜서 ChatGPT 같은 유용한 어시스턴트가 탄생했어요!

**Host B**: AI는 정말 빠르게 발전하고 있어요. 윤리적 고려, 편향 문제, 책임있는 사용도 중요하다는 걸 잊지 마세요!

**Host A**: 감사합니다! 다음 시간에 만나요!

---

## 핵심 키워드
- Pre-training, Fine-tuning, Transfer Learning
- Self-supervised Learning, Language Modeling
- BERT (Bidirectional Encoder Representations from Transformers)
- Masked Language Modeling (MLM), Next Sentence Prediction (NSP)
- GPT (Generative Pre-trained Transformer), Autoregressive
- Zero-shot, One-shot, Few-shot, In-context Learning
- T5 (Text-to-Text Transfer Transformer), BART
- PEFT (Parameter-Efficient Fine-Tuning), LoRA, Adapter, Prompt Tuning
- Chain-of-Thought (CoT), Prompt Engineering
- RLHF (Reinforcement Learning from Human Feedback), Constitutional AI
- Instruction Tuning, Alignment, Safety
