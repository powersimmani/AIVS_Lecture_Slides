<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Overall Transformer Structure</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: Aptos, 'Segoe UI', sans-serif;
            background: white;
            padding: 40px 0;
        }
        
        .container {
            width: 960px;
            height: 540px;
            padding: 30px;
            background: white;
            display: flex;
            flex-direction: column;
            gap: 14px;
            margin: 0 auto;
        }
        
        .title {
            text-align: center;
            font-size: 24px;
            font-weight: 600;
            color: #1E64C8;
            margin-bottom: 2px;
        }
        
        .subtitle {
            text-align: center;
            font-size: 16px;
            color: #666;
            margin-bottom: 6px;
        }
        
        .architecture-flow {
            display: flex;
            gap: 14px;
            flex: 1;
        }
        
        .flow-column {
            flex: 1;
            display: flex;
            flex-direction: column;
            gap: 10px;
        }
        
        .component-box {
            border-radius: 8px;
            padding: 14px 16px;
            text-align: center;
            border: 2px solid;
        }
        
        .input-box {
            background: #f0f7ff;
            border-color: #1E64C8;
        }
        
        .input-title {
            font-size: 17px;
            font-weight: 600;
            color: #1E64C8;
            margin-bottom: 4px;
        }
        
        .input-description {
            font-size: 14px;
            color: #666;
        }
        
        .encoder-box {
            background: #f0f7ff;
            border-color: #1E64C8;
            flex: 1;
            display: flex;
            flex-direction: column;
            justify-content: center;
            gap: 8px;
        }
        
        .decoder-box {
            background: #f0fdf4;
            border-color: #16a34a;
            flex: 1;
            display: flex;
            flex-direction: column;
            justify-content: center;
            gap: 8px;
        }
        
        .component-icon {
            font-size: 32px;
            margin-bottom: 4px;
        }
        
        .component-title {
            font-size: 19px;
            font-weight: 600;
            margin-bottom: 6px;
        }
        
        .encoder-box .component-title {
            color: #1E64C8;
        }
        
        .decoder-box .component-title {
            color: #16a34a;
        }
        
        .component-info {
            font-size: 15px;
            color: #333;
            line-height: 1.4;
        }
        
        .layer-count {
            background: white;
            border-radius: 4px;
            padding: 6px 10px;
            font-size: 14px;
            font-weight: 600;
            margin-top: 4px;
        }
        
        .encoder-box .layer-count {
            color: #1E64C8;
        }
        
        .decoder-box .layer-count {
            color: #16a34a;
        }
        
        .output-box {
            background: #fff9f0;
            border-color: #f59e0b;
        }
        
        .output-title {
            font-size: 17px;
            font-weight: 600;
            color: #d97700;
            margin-bottom: 4px;
        }
        
        .arrow-down {
            text-align: center;
            color: #1E64C8;
            font-size: 20px;
            height: 6px;
        }
        
        .arrow-right {
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 28px;
            color: #1E64C8;
            font-weight: 700;
        }
        
        .connection-box {
            background: #fef3c7;
            border: 2px dashed #f59e0b;
            border-radius: 8px;
            padding: 10px 16px;
            text-align: center;
            font-size: 15px;
            font-weight: 600;
            color: #d97700;
        }
        
        .key-points {
            display: flex;
            gap: 10px;
        }
        
        .key-point {
            flex: 1;
            background: #f0f7ff;
            border: 2px solid #1E64C8;
            border-radius: 8px;
            padding: 10px 14px;
            text-align: center;
        }
        
        .key-point-icon {
            font-size: 20px;
            margin-bottom: 4px;
        }
        
        .key-point-text {
            font-size: 14px;
            color: #333;
            line-height: 1.3;
        }
        
        .highlight {
            font-weight: 600;
            color: #1E64C8;
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="title">Overall Transformer Structure</div>
        <div class="subtitle">Encoder-Decoder Architecture for Sequence-to-Sequence Tasks</div>
        
        <div class="architecture-flow">
            <div class="flow-column">
                <div class="component-box input-box">
                    <div class="input-title">Input</div>
                    <div class="input-description">Source Sequence<br>(e.g., English)</div>
                </div>
                
                <div class="arrow-down">‚Üì</div>
                
                <div class="component-box input-box">
                    <div class="input-title">Positional Encoding</div>
                    <div class="input-description">Add position info</div>
                </div>
                
                <div class="arrow-down">‚Üì</div>
                
                <div class="component-box encoder-box">
                    <div class="component-icon">üî∑</div>
                    <div class="component-title">Encoder</div>
                    <div class="component-info">
                        Processes source sequence
                    </div>
                    <div class="layer-count">6 Stacked Layers</div>
                </div>
            </div>
            
            <div class="arrow-right">‚Üí</div>
            
            <div class="flow-column">
                <div class="component-box input-box">
                    <div class="input-title">Output (shifted right)</div>
                    <div class="input-description">Target Sequence<br>(e.g., French)</div>
                </div>
                
                <div class="arrow-down">‚Üì</div>
                
                <div class="component-box input-box">
                    <div class="input-title">Positional Encoding</div>
                    <div class="input-description">Add position info</div>
                </div>
                
                <div class="arrow-down">‚Üì</div>
                
                <div class="component-box decoder-box">
                    <div class="component-icon">üî∂</div>
                    <div class="component-title">Decoder</div>
                    <div class="component-info">
                        Generates target sequence<br>with masking
                    </div>
                    <div class="layer-count">6 Stacked Layers</div>
                </div>
                
                <div class="arrow-down">‚Üì</div>
                
                <div class="component-box output-box">
                    <div class="output-title">Output</div>
                    <div class="input-description">Generated Sequence</div>
                </div>
            </div>
        </div>
        
        <div class="connection-box">
            üîó Encoder-Decoder Attention connects both components
        </div>
        
        <div class="key-points">
            <div class="key-point">
                <div class="key-point-icon">üèóÔ∏è</div>
                <div class="key-point-text">
                    <span class="highlight">Identical layers</span><br>with different parameters
                </div>
            </div>
            <div class="key-point">
                <div class="key-point-icon">üéØ</div>
                <div class="key-point-text">
                    <span class="highlight">Seq2Seq tasks</span><br>Translation, Summarization
                </div>
            </div>
            <div class="key-point">
                <div class="key-point-icon">üîí</div>
                <div class="key-point-text">
                    Decoder uses<br><span class="highlight">masking</span> for causality
                </div>
            </div>
        </div>
    </div>
    
    <div class="container" style="margin-top: 400px; height: auto; padding-bottom: 40px;">
        <div class="title" style="color: #16a34a;">Detailed Computation Example</div>
        <div class="subtitle">Step-by-step process with actual dimensions (d_model = 4, sequence length = 8)</div>
        
        <div style="margin-top: 20px;">
            <div style="background: #f0f7ff; border: 2px solid #1E64C8; border-radius: 8px; padding: 20px; margin-bottom: 20px;">
                <div style="font-size: 18px; font-weight: 600; color: #1E64C8; margin-bottom: 12px;">
                    üì• Step 1: Input Embedding
                </div>
                <div style="font-size: 15px; color: #333; line-height: 1.6;">
                    <strong>Source (English):</strong> "I love deep learning very much !"<br>
                    <strong>Tokenization:</strong> [I, love, deep, learning, very, much, !, &lt;PAD&gt;] ‚Üí 8 tokens<br><br>
                    
                    <strong>Embedding Matrix:</strong> Vocabulary √ó d_model ‚Üí Each token ‚Üí 4D vector<br>
                    <div style="background: white; padding: 12px; border-radius: 4px; margin-top: 8px; font-family: monospace; font-size: 13px;">
                        X_embedded (8 √ó 4) = <br>
                        [[0.2, 0.5, -0.3, 0.8],&nbsp;&nbsp;# I<br>
                        &nbsp;[0.1, -0.4, 0.6, 0.3],&nbsp;&nbsp;# love<br>
                        &nbsp;[-0.5, 0.2, 0.4, -0.1],&nbsp;# deep<br>
                        &nbsp;[0.3, 0.7, -0.2, 0.5],&nbsp;&nbsp;# learning<br>
                        &nbsp;[0.4, -0.3, 0.1, 0.6],&nbsp;&nbsp;# very<br>
                        &nbsp;[-0.2, 0.5, 0.3, -0.4],&nbsp;# much<br>
                        &nbsp;[0.6, 0.1, -0.5, 0.2],&nbsp;&nbsp;# !<br>
                        &nbsp;[0.0, 0.0, 0.0, 0.0]]&nbsp;&nbsp;&nbsp;# &lt;PAD&gt;
                    </div>
                </div>
            </div>
            
            <div style="background: #f0f7ff; border: 2px solid #1E64C8; border-radius: 8px; padding: 20px; margin-bottom: 20px;">
                <div style="font-size: 18px; font-weight: 600; color: #1E64C8; margin-bottom: 12px;">
                    üìç Step 2: Positional Encoding
                </div>
                <div style="font-size: 15px; color: #333; line-height: 1.6;">
                    <strong>Add position information using sin/cos functions:</strong><br>
                    PE(pos, 2i) = sin(pos / 10000^(2i/d_model))<br>
                    PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))<br><br>
                    
                    <div style="background: white; padding: 12px; border-radius: 4px; margin-top: 8px; font-family: monospace; font-size: 13px;">
                        PE (8 √ó 4) = <br>
                        [[0.00, 1.00, 0.00, 1.00],&nbsp;&nbsp;# pos 0<br>
                        &nbsp;[0.84, 0.54, 0.01, 1.00],&nbsp;&nbsp;# pos 1<br>
                        &nbsp;[0.91, -0.42, 0.02, 1.00],&nbsp;# pos 2<br>
                        &nbsp;[0.14, -0.99, 0.03, 1.00],&nbsp;# pos 3<br>
                        &nbsp;[-0.76, -0.65, 0.04, 1.00],&nbsp;# pos 4<br>
                        &nbsp;[-0.96, 0.28, 0.05, 1.00],&nbsp;&nbsp;# pos 5<br>
                        &nbsp;[-0.28, 0.96, 0.06, 1.00],&nbsp;&nbsp;# pos 6<br>
                        &nbsp;[0.66, 0.75, 0.07, 0.99]]&nbsp;&nbsp;&nbsp;# pos 7<br><br>
                        
                        <strong>X_input (8 √ó 4) = X_embedded + PE</strong><br>
                        [[0.20, 1.50, -0.30, 1.80],<br>
                        &nbsp;[0.94, 0.14, 0.61, 1.30],<br>
                        &nbsp;[0.41, -0.22, 0.42, 0.90],<br>
                        &nbsp;[0.44, -0.29, -0.17, 1.50],<br>
                        &nbsp;[-0.36, -0.95, 0.14, 1.60],<br>
                        &nbsp;[-1.16, 0.78, 0.35, 0.60],<br>
                        &nbsp;[0.32, 1.06, -0.44, 1.20],<br>
                        &nbsp;[0.66, 0.75, 0.07, 0.99]]
                    </div>
                </div>
            </div>
            
            <div style="background: #f0f7ff; border: 2px solid #1E64C8; border-radius: 8px; padding: 20px; margin-bottom: 20px;">
                <div style="font-size: 18px; font-weight: 600; color: #1E64C8; margin-bottom: 12px;">
                    üî∑ Step 3: Encoder Self-Attention (Layer 1)
                </div>
                <div style="font-size: 15px; color: #333; line-height: 1.6;">
                    <strong>Multi-Head Attention computation:</strong><br><br>
                    
                    <strong>1) Linear projections (d_k = d_model / num_heads = 4 / 2 = 2 per head):</strong><br>
                    <div style="background: white; padding: 12px; border-radius: 4px; margin-top: 8px; font-family: monospace; font-size: 13px;">
                        W_Q (4 √ó 4), W_K (4 √ó 4), W_V (4 √ó 4)<br>
                        Q = X_input @ W_Q ‚Üí (8 √ó 4)<br>
                        K = X_input @ W_K ‚Üí (8 √ó 4)<br>
                        V = X_input @ W_V ‚Üí (8 √ó 4)
                    </div><br>
                    
                    <strong>2) Attention scores:</strong><br>
                    <div style="background: white; padding: 12px; border-radius: 4px; margin-top: 8px; font-family: monospace; font-size: 13px;">
                        Scores = Q @ K^T / ‚àöd_k ‚Üí (8 √ó 8)<br>
                        <span style="color: #666;">// Example scores (showing first 4√ó4 block):</span><br>
                        [[2.3, 0.8, 1.2, 1.5, ...],<br>
                        &nbsp;[0.8, 2.1, 0.9, 1.3, ...],<br>
                        &nbsp;[1.2, 0.9, 2.5, 1.1, ...],<br>
                        &nbsp;[1.5, 1.3, 1.1, 2.2, ...]]<br><br>
                        
                        Attention = softmax(Scores) ‚Üí (8 √ó 8)<br>
                        <span style="color: #666;">// Softmax normalizes each row to sum to 1</span><br>
                        [[0.35, 0.08, 0.12, 0.15, ...],&nbsp;# I attends to all<br>
                        &nbsp;[0.10, 0.31, 0.11, 0.16, ...],&nbsp;# love attends to all<br>
                        &nbsp;[0.13, 0.09, 0.41, 0.12, ...],&nbsp;# deep attends to all<br>
                        &nbsp;...]
                    </div><br>
                    
                    <strong>3) Apply attention to values:</strong><br>
                    <div style="background: white; padding: 12px; border-radius: 4px; margin-top: 8px; font-family: monospace; font-size: 13px;">
                        Output = Attention @ V ‚Üí (8 √ó 4)<br>
                        <span style="color: #666;">// Each token representation is now context-aware</span>
                    </div>
                </div>
            </div>
            
            <div style="background: #f0f7ff; border: 2px solid #1E64C8; border-radius: 8px; padding: 20px; margin-bottom: 20px;">
                <div style="font-size: 18px; font-weight: 600; color: #1E64C8; margin-bottom: 12px;">
                    üîÑ Step 4: Add & Norm + Feed-Forward
                </div>
                <div style="font-size: 15px; color: #333; line-height: 1.6;">
                    <strong>1) Residual connection and Layer Normalization:</strong><br>
                    <div style="background: white; padding: 12px; border-radius: 4px; margin-top: 8px; font-family: monospace; font-size: 13px;">
                        X1 = LayerNorm(X_input + Attention_output) ‚Üí (8 √ó 4)
                    </div><br>
                    
                    <strong>2) Feed-Forward Network (expand to d_ff = 16, then back to 4):</strong><br>
                    <div style="background: white; padding: 12px; border-radius: 4px; margin-top: 8px; font-family: monospace; font-size: 13px;">
                        FFN(x) = W_2 @ ReLU(W_1 @ x + b_1) + b_2<br>
                        W_1: (4 √ó 16), W_2: (16 √ó 4)<br><br>
                        
                        FFN_output = FFN(X1) ‚Üí (8 √ó 4)<br>
                        X_encoder1 = LayerNorm(X1 + FFN_output) ‚Üí (8 √ó 4)
                    </div><br>
                    
                    <strong>‚Üí Repeat this process for 6 encoder layers</strong><br>
                    <strong>‚Üí Final encoder output: (8 √ó 4)</strong>
                </div>
            </div>
            
            <div style="background: #f0fdf4; border: 2px solid #16a34a; border-radius: 8px; padding: 20px; margin-bottom: 20px;">
                <div style="font-size: 18px; font-weight: 600; color: #16a34a; margin-bottom: 12px;">
                    üî∂ Step 5: Decoder Input & Masked Self-Attention
                </div>
                <div style="font-size: 15px; color: #333; line-height: 1.6;">
                    <strong>Target (French):</strong> "&lt;BOS&gt; J' aime l' apprentissage profond &lt;PAD&gt;"<br>
                    <strong>Shifted right:</strong> [&lt;BOS&gt;, J', aime, l', apprentissage, profond, &lt;PAD&gt;, &lt;PAD&gt;]<br><br>
                    
                    <strong>After embedding + positional encoding:</strong> Y_input (8 √ó 4)<br><br>
                    
                    <strong>Masked Self-Attention:</strong><br>
                    <div style="background: white; padding: 12px; border-radius: 4px; margin-top: 8px; font-family: monospace; font-size: 13px;">
                        Mask prevents attending to future tokens:<br>
                        [[1, 0, 0, 0, 0, 0, 0, 0],&nbsp;&nbsp;# &lt;BOS&gt; only sees itself<br>
                        &nbsp;[1, 1, 0, 0, 0, 0, 0, 0],&nbsp;&nbsp;# J' sees &lt;BOS&gt;, J'<br>
                        &nbsp;[1, 1, 1, 0, 0, 0, 0, 0],&nbsp;&nbsp;# aime sees up to aime<br>
                        &nbsp;[1, 1, 1, 1, 0, 0, 0, 0],&nbsp;&nbsp;# l' sees up to l'<br>
                        &nbsp;[1, 1, 1, 1, 1, 0, 0, 0],&nbsp;&nbsp;# apprentissage<br>
                        &nbsp;[1, 1, 1, 1, 1, 1, 0, 0],&nbsp;&nbsp;# profond<br>
                        &nbsp;[1, 1, 1, 1, 1, 1, 1, 0],&nbsp;&nbsp;# &lt;PAD&gt;<br>
                        &nbsp;[1, 1, 1, 1, 1, 1, 1, 1]]&nbsp;&nbsp;# &lt;PAD&gt;<br><br>
                        
                        Masked_Attention_output ‚Üí (8 √ó 4)
                    </div>
                </div>
            </div>
            
            <div style="background: #fef3c7; border: 2px solid #f59e0b; border-radius: 8px; padding: 20px; margin-bottom: 20px;">
                <div style="font-size: 18px; font-weight: 600; color: #d97700; margin-bottom: 12px;">
                    üîó Step 6: Encoder-Decoder Cross-Attention
                </div>
                <div style="font-size: 15px; color: #333; line-height: 1.6;">
                    <strong>Query from decoder, Key & Value from encoder:</strong><br>
                    <div style="background: white; padding: 12px; border-radius: 4px; margin-top: 8px; font-family: monospace; font-size: 13px;">
                        Q = Decoder_output @ W_Q ‚Üí (8 √ó 4)&nbsp;&nbsp;<span style="color: #16a34a;"># from decoder</span><br>
                        K = Encoder_output @ W_K ‚Üí (8 √ó 4)&nbsp;&nbsp;<span style="color: #1E64C8;"># from encoder</span><br>
                        V = Encoder_output @ W_V ‚Üí (8 √ó 4)&nbsp;&nbsp;<span style="color: #1E64C8;"># from encoder</span><br><br>
                        
                        Cross_Attention = softmax(Q @ K^T / ‚àöd_k) @ V<br>
                        <span style="color: #666;">// Decoder attends to ALL encoder positions</span><br>
                        <span style="color: #666;">// This allows decoder to "look at" the source sentence</span><br><br>
                        
                        Cross_Attention_output ‚Üí (8 √ó 4)
                    </div>
                </div>
            </div>
            
            <div style="background: #fff9f0; border: 2px solid #f59e0b; border-radius: 8px; padding: 20px;">
                <div style="font-size: 18px; font-weight: 600; color: #d97700; margin-bottom: 12px;">
                    üì§ Step 7: Final Output & Prediction
                </div>
                <div style="font-size: 15px; color: #333; line-height: 1.6;">
                    <strong>After 6 decoder layers:</strong><br>
                    <div style="background: white; padding: 12px; border-radius: 4px; margin-top: 8px; font-family: monospace; font-size: 13px;">
                        Decoder_final_output ‚Üí (8 √ó 4)
                    </div><br>
                    
                    <strong>Linear projection to vocabulary size (e.g., 10,000):</strong><br>
                    <div style="background: white; padding: 12px; border-radius: 4px; margin-top: 8px; font-family: monospace; font-size: 13px;">
                        Logits = Decoder_output @ W_output ‚Üí (8 √ó 10000)<br>
                        Probabilities = softmax(Logits) ‚Üí (8 √ó 10000)
                    </div><br>
                    
                    <strong>For each position, pick highest probability token:</strong><br>
                    <div style="background: white; padding: 12px; border-radius: 4px; margin-top: 8px; font-family: monospace; font-size: 13px;">
                        Position 0: P("J'") = 0.92 ‚Üí output "J'"<br>
                        Position 1: P("aime") = 0.87 ‚Üí output "aime"<br>
                        Position 2: P("l'") = 0.91 ‚Üí output "l'"<br>
                        Position 3: P("apprentissage") = 0.84 ‚Üí output "apprentissage"<br>
                        Position 4: P("profond") = 0.89 ‚Üí output "profond"<br>
                        Position 5: P("&lt;EOS&gt;") = 0.95 ‚Üí output "&lt;EOS&gt;" (stop)<br><br>
                        
                        <strong style="color: #16a34a;">Final Translation: "J' aime l' apprentissage profond"</strong>
                    </div>
                </div>
            </div>
        </div>
        
        <div style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius: 8px; padding: 20px; margin-top: 20px; color: white;">
            <div style="font-size: 18px; font-weight: 600; margin-bottom: 10px; text-align: center;">
                üéØ Key Dimensional Flow Summary
            </div>
            <div style="font-size: 14px; line-height: 1.8; font-family: monospace;">
                Input tokens (8) ‚Üí Embedding (8√ó4) ‚Üí +PE (8√ó4) ‚Üí<br>
                Encoder layers (8√ó4 maintained) ‚Üí Encoder output (8√ó4) ‚Üí<br>
                Decoder input (8√ó4) ‚Üí Decoder layers (8√ó4 maintained) ‚Üí<br>
                Linear projection (8√ó4 ‚Üí 8√ó10000) ‚Üí Softmax ‚Üí Predictions (8√ó10000)
            </div>
        </div>
    </div>
</body>
</html>