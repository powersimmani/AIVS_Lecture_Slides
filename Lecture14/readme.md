# Lecture 14: Pre-trained Language Models & LLM Era

## ğŸ“‹ Overview

**Instructor:** Ho-min Park  
**Email:** homin.park@ghent.ac.kr | powersimmani@gmail.com

This lecture covers pre-trained models like BERT and GPT, and the era of Large Language Models.

---

## ğŸ¯ Learning Objectives

1. Understand Transfer Learning paradigm
2. Compare BERT (Encoder) vs GPT (Decoder)
3. Apply Fine-tuning strategies
4. Prompt Engineering techniques
5. Understand LLM characteristics and applications

---

## ğŸ“š Key Topics

**BERT**: Bidirectional, MLM, NSP, bidirectional context
**GPT**: Unidirectional, Autoregressive, text generation
**T5/BART**: Encoder-Decoder, Text-to-Text
**Transfer Learning**: Pre-training + Fine-tuning
**LLM**: GPT-3/4, Few-shot learning, In-context learning

---

## ğŸ’¡ Key Concepts

- Pre-training learns general language understanding
- BERT strong at understanding, GPT at generation
- Fine-tuning improves task-specific performance
- Prompts control LLM behavior
- Scale enables emergent abilities

---

## ğŸ› ï¸ Prerequisites

- Basic Python programming
- Understanding of previous lecture content
- Basic machine learning concepts

---

## ğŸ“– Additional Resources

For detailed code examples, practice materials, and slides, please refer to the original lecture files.
Lecture materials: HTML-based interactive slides provided
