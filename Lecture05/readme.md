# Lecture 5: From Logistic Regression to Multi-layer Perceptrons

## ğŸ“‹ Overview

**Instructor:** Ho-min Park  
**Email:** homin.park@ghent.ac.kr | powersimmani@gmail.com

This lecture overcomes limitations of linear models and solves nonlinear problems with Multi-layer Perceptrons (MLP).

---

## ğŸ¯ Learning Objectives

1. Analyze limitations of linear models (XOR problem)
2. Design and implement MLP architectures
3. Understand and apply Backpropagation algorithm
4. Understand role of activation functions
5. Understand Universal Approximation Theorem

---

## ğŸ“š Key Topics

**Linear model limitations**: XOR problem, nonlinear inseparability
**MLP structure**: Input-Hidden-Output layers, fully connected
**Activation functions**: Sigmoid, ReLU, Tanh, Leaky ReLU
**Backpropagation**: Chain rule, gradient propagation
**Universal Approximation**: Approximate any function with sufficient hidden units

---

## ğŸ’¡ Key Concepts

- Single layer cannot solve XOR
- Hidden layers learn nonlinear features
- Activation functions provide nonlinearity
- Backpropagation enables efficient gradient computation
- Deep networks learn hierarchical representations

---

## ğŸ› ï¸ Prerequisites

- Basic Python programming
- Understanding of previous lecture content
- Basic machine learning concepts

---

## ğŸ“– Additional Resources

For detailed code examples, practice materials, and slides, please refer to the original lecture files.
Lecture materials: HTML-based interactive slides provided
