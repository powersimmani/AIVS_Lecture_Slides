# Lecture 3: From Set Theory to Linear Regression

## ğŸ“‹ Overview

**Instructor:** Ho-min Park  
**Email:** homin.park@ghent.ac.kr | powersimmani@gmail.com

This lecture systematically covers mathematical foundations from set theory, linear algebra, calculus, and probability to linear regression.

---

## ğŸ¯ Learning Objectives

1. Apply set theory, linear algebra, and calculus to ML problems
2. Utilize probability and statistics concepts in regression models
3. Derive linear regression using Normal Equation and geometric interpretation
4. Validate models through residual analysis and diagnostic techniques
5. Implement regression models with NumPy and scikit-learn

---

## ğŸ“š Key Topics

**Mathematical foundations**: Set theory, functions, vector spaces, inner product, orthogonality
**Linear algebra**: Matrix operations, transpose, inverse, eigenvalues/eigenvectors
**Calculus**: Partial derivatives, gradient, optimization
**Probability and statistics**: Probability distributions, expectation, variance, Maximum Likelihood Estimation (MLE)
**Linear regression**: OLS, Normal Equation, residual analysis, RÂ², assumption validation

---

## ğŸ’¡ Key Concepts

- Vector spaces and basis are fundamental to data representation
- Inner product and orthogonality are key to understanding projection and residuals
- Gradient indicates optimization direction
- MLE provides theoretical foundation for parameter estimation
- Linear regression assumptions: linearity, independence, normality, homoscedasticity

---

## ğŸ› ï¸ Prerequisites

- Basic Python programming
- Understanding of previous lecture content
- Basic machine learning concepts

---

## ğŸ“– Additional Resources

For detailed code examples, practice materials, and slides, please refer to the original lecture files.
Lecture materials: HTML-based interactive slides provided
